[
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html",
    "title": "Advent of Code 2021",
    "section": "",
    "text": "See my solution for Day 1 here."
  },
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html#the-data",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here.\nFor day 2, our data are a set of directions that we can use to explore the ocean floor that we mapped out on Day 1. Each row of data contains two pieces of information: the direction to move (e.g. ‘forward’, ‘up’) and the distance (an integer). We’ll load the data in and then separate the direction and distance information.\n\nlibrary(readr)\n\n#Load data\nday2_data &lt;- readr::read_delim(file = \"./data/Day2.txt\", delim = \" \",\n                               col_names = c(\"direction\", \"distance\"), show_col_types = FALSE)\n\nhead(day2_data)\n\n# A tibble: 6 × 2\n  direction distance\n  &lt;chr&gt;        &lt;dbl&gt;\n1 forward          8\n2 forward          3\n3 forward          8\n4 down             6\n5 forward          3\n6 up               6"
  },
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html#the-challenges",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\n\nChallenge 1\n\nStarting from (0,0) we need to follow each of the directions included in the data and determine our final location. We could simply do this using a for loop (probably the simpler solution), but I wanted to take this opportunity to practice writing recursive functions.\nLet’s build a function that runs through the data row by row and recalls itself each time until there is not data left. Unlike in a for loop, we don’t need to define the number of iterations at the beginning, we simply define the conditions under which the function will continue. Of course…we also need to be careful not to create an infinite loop.\n\nOur function will take a data input, a vector of start coordinates c(0,0), and three functions that will be applied to the start coordinates (this will be important for challenge 2).\n\nmove &lt;- function(data, start,\n                 forward_func,\n                 up_func,\n                 down_func){\n\n  #Separate the first line of data and the remaining data\n  input  &lt;- data[1, ]\n  remain &lt;- data[-1, ]\n\n  #Depending on the directions, apply a difference function\n  new_coord &lt;- switch(input$direction,\n                      forward = forward_func(start = start, distance = input$distance),\n                      down = down_func(start = start, distance = input$distance),\n                      up = up_func(start = start, distance = input$distance))\n\n  #If there are still rows of data remaining, recall the function\n  if (nrow(remain) &gt; 0) {\n\n    Recall(data = remain, start = new_coord,\n           forward_func = forward_func,\n           up_func = up_func,\n           down_func = down_func)\n\n  } else {\n\n    #If we've gone through all rows of data, return the coordinate\n    return(new_coord)\n\n  }\n\n}\n\nWe’ll create some very simply input functions that we can feed into our move() function.\n\n#Write funcs to do each process\n#Forward moves on the x axis\nforward &lt;- function(start, distance){\n  start[1] &lt;- start[1] + distance\n  return(start)\n}\n\n#Up will *decrease* depth\nup &lt;- function(start, distance){\n  start[2] &lt;- start[2] - distance\n  return(start)\n}\n\n#Down will *increase* depth\ndown &lt;- function(start, distance){\n  start[2] &lt;- start[2] + distance\n  return(start)\n}\n\nNow we can run our recursive function.\n\nfinal_position &lt;- move(data = day2_data, start = c(0, 0),\n                       forward_func = forward,\n                       up_func = up,\n                       down_func = down)\n\n#Return the product, which is our answer\nprod(final_position[1], final_position[2])\n\n[1] 1383564\n\n\n\n\n\nChallenge 2\n\nFor the second challenge, the function used for each direction type has changed. We now also need to deal with an ‘aim’ value that affects our forward movement. This aim value can be the third value in our start vector. Luckily, we’ve already written our recursive function, so this is as simple as substituting in new functions for forward, up, and down.\n\nforward_new &lt;- function(start, distance){\n  start[1] &lt;- start[1] + distance\n  start[2] &lt;- start[2] + distance*start[3]\n  return(start)\n}\n\nup_new &lt;- function(start, distance){\n  start[3] &lt;- start[3] - distance\n  return(start)\n}\n\ndown_new &lt;- function(start, distance){\n  start[3] &lt;- start[3] + distance\n  return(start)\n}\n\n\nfinal_position &lt;- move(data = day2_data, start = c(0, 0, 0),\n                       forward_func = forward_new, up_func = up_new, down_func = down_new)\n\n#Return the product, which is our answer\nprod(final_position[1], final_position[2])\n\n[1] 1488311643\n\n\nBy putting in a bit of effort to make a robust function at the beginning we were able to solve both challenges. It’s a good example of how writing good functions can save you a lot of time and effort later on."
  },
  {
    "objectID": "posts/2021-12-09-advent-of-code-day-6/index.html#the-data",
    "href": "posts/2021-12-09-advent-of-code-day-6/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nFor Day 6 we have to model the growth of a population of fish…this is much closer to my comfort zone. See the explanation for today’s challenge here. We are given a vector of integers that represents the time since last reproduction for every fish. When a fish reaches a value of 0 it will reproduce with a clutch size of 1 (which seems a tad too few for a fish…but this is a coding challenge not a biology challenge). The inter-birth interval of an adult is 6 days and the age at first reproduction for a newly born fish is 8 days. Now we have all the information, let’s get started.\n\n\n#Load data\nday6_data &lt;- as.integer(scan(file = \"./data/Day6.txt\", sep = \",\"))\n\nday6_data\n\n  [1] 5 1 1 5 4 2 1 2 1 2 2 1 1 1 4 2 2 4 1 1 1 1 1 4 1 1 1 1 1 5 3 1 4 1 1 1 1\n [38] 1 4 1 5 1 1 1 4 1 2 2 3 1 5 1 1 5 1 1 5 4 1 1 1 4 3 1 1 1 3 1 5 5 1 1 1 1\n [75] 5 3 2 1 2 3 1 5 1 1 4 1 1 2 1 5 1 1 1 1 5 4 5 1 3 1 3 3 5 5 1 3 1 5 3 1 1\n[112] 4 2 3 3 1 2 4 1 1 1 1 1 1 1 2 1 1 4 1 3 2 5 2 1 1 1 4 2 1 1 1 4 2 4 1 1 1\n[149] 1 4 1 3 5 5 1 2 1 3 1 1 4 1 1 1 1 2 1 1 4 2 3 1 1 1 1 1 1 1 4 5 1 1 3 1 1\n[186] 2 1 1 1 5 1 1 1 1 1 3 2 1 2 4 5 1 5 4 1 1 3 1 1 5 5 1 3 1 1 1 1 4 4 2 1 2\n[223] 1 1 5 1 1 4 5 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 4 2 1 1 1 2 5 1 4 1 1 1 4 1\n[260] 1 5 4 4 3 1 1 4 5 1 1 3 5 3 1 2 5 3 4 1 3 5 4 1 3 1 5 1 4 1 1 4 2 1 1 1 3\n[297] 2 1 1 4"
  },
  {
    "objectID": "posts/2021-12-09-advent-of-code-day-6/index.html#the-challenges",
    "href": "posts/2021-12-09-advent-of-code-day-6/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFirst off, we need to determine how many individuals would be in the population after 80 days (assuming no mortality in adults or offspring). The first way we might think to do this is just brute force. Keep the data in its original format and count the number of 0s in every time step to identify reproducers.\n\nfinal_pop80 &lt;- day6_data\n\nsystem.time({for (i in 1:80){\n  \n  #At the start of the time step, work out how many individuals reproduce\n  reproducer_location &lt;- final_pop80 == 0\n  \n  #Reduce the time of all individuals by 1\n  final_pop80 &lt;- final_pop80 - 1\n  \n  #Reset the age of reproducers to 6\n  final_pop80[reproducer_location] &lt;- 6\n  \n  #For every reproducer, add a new individual with a value of 8.\n  final_pop80 &lt;- append(final_pop80, rep(8, times = sum(reproducer_location)))\n  \n}})\n\n   user  system elapsed \n  0.026   0.009   0.036 \n\n\nThis ran in no time and we can quickly get our answer! It seems that the brute force approach works fine.\n\nlength(final_pop80)\n\n[1] 377263\n\n\n\n\nChallenge 2\n\nFor challenge 2 we need to push things a bit further and predict the population after 256 days. Could our brute force approach still work for this? The worrying detail is that without any mortality and a constant rate of reproduction this population of fish is going to be growing exponentially so the size of our integer is going to get very unmanageable very quickly. This is exactly what happens if we try our brute force approach, eventually maxing out our memory and crashing R. I’ve run the code below to show you how much trouble it has but I’ve capped it at 1min run time.\n\nfinal_pop &lt;- day6_data\n\nR.utils::withTimeout({for (i in 1:256){\n    \n    #At the start of the time step, work out how \n    reproducer_location &lt;- final_pop == 0\n    \n    #Reduce the time of all individuals by 1\n    final_pop &lt;- final_pop - 1\n    \n    #Reset the age of reproducers to 6\n    final_pop[reproducer_location] &lt;- 6\n    \n    #For every reproducer, add a new individual with a value of 8.\n    final_pop &lt;- append(final_pop, rep(8, times = sum(reproducer_location)))\n  }\n    #Timeout after 60 seconds\n  }, timeout = 60)\n\n[2023-05-12 15:43:13] TimeoutException: reached elapsed time limit [cpu=60s,\nelapsed=60s]\n\n\nIt seems we need another method. The key here is that (at least in this simplified fish population) every individual with the same number can be grouped together. They have no other distinguishing characteristics. So, instead of starting with a vector of 300 integers, we can group all individuals into age classes and instead just work with a vector of length 9 (values 0 - 8).\n\n#Turn our vector into a table\ntable_of_ages &lt;- table(day6_data)\n\n#Extend this to include all possible ages.\n#Make a named vector so we can easily index it\nages_vector &lt;- tidyr::replace_na(as.vector(table_of_ages[as.character(0:8)]), 0)\nnames(ages_vector) &lt;- 0:8\n\nages_vector\n\n  0   1   2   3   4   5   6   7   8 \n  0 168  31  29  37  35   0   0   0 \n\n\nThe values in our vector might change, but the length of the vector will never grow because individuals cannot have a value outside of this range. Now we can run very similar code to above but using our new age classes rather than individuals.\n\npop_size &lt;- as.numeric(NULL)\nsystem.time({for (i in 1:256){\n\n  reproducers &lt;- as.numeric(ages_vector[\"0\"])\n\n  ages_vector[as.character(0:7)] &lt;- ages_vector[as.character(1:8)]\n\n  ages_vector[\"6\"] &lt;- as.numeric(ages_vector[\"6\"]) + reproducers\n  ages_vector[\"8\"] &lt;- reproducers\n\n  pop_size &lt;- append(pop_size, sum(ages_vector))\n\n}})\n\n   user  system elapsed \n  0.005   0.000   0.004 \n\n\nAs you can see, our final population size is very large…no wonder we couldn’t use the brute force approach.\n\npop_size[length(pop_size)]\n\n[1] 1695929023803\n\n\n\n\nBONUS ROUND\n\nUnlike the data we used for this challenge, in reality when studying a population of animals we probably don’t know the exact age of every individual or the precise rules that might govern their reproduction. In many cases, our data might just be population counts. What could we do then?\nIn this example, we are given information that the population is growing exponentially and this can be modelled using the exponential growth model:\n\\[N_{t+1} = RN_{t}\\]\nWhere the population at time t (\\(N_{t}\\)) will grow by a constant reproduction factor (\\(R\\)). This parameter \\(R\\) will be different for every animal and population, but we can estimate it by looking at the relationship between \\(N_{t}\\) and \\(N_{t+1}\\) in the data we have already. Let’s imagine we only have population size data from the first 100 days. We can use a linear model (lm()) to estimate the relationship between \\(N_{t}\\) and \\(N_{t+1}\\) in this subset of data.\n\n\n\n\n\n\nNote\n\n\n\nWe fix the intercept of the model below to be 0 by including ~ 0 + ... in the model formula. This normally wouldn’t be advised, but in this case we know a priori that when \\(N_{t}\\) is 0, \\(N_{t+1}\\) must also be 0. An extinct population can’t grow!!\n\n\n\ninput_data &lt;- data.frame(nt = pop_size[1:100]) %&gt;% \n  mutate(nt_1 = lead(nt))\n\nour_model &lt;- lm(nt_1 ~ 0 + nt, data = input_data)\n  \nsummary(our_model)\n\n\nCall:\nlm(formula = nt_1 ~ 0 + nt, data = input_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57015  -1033    -39    766  59762 \n\nCoefficients:\n   Estimate Std. Error t value            Pr(&gt;|t|)    \nnt 1.091493   0.002916   374.4 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14150 on 98 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.9993 \nF-statistic: 1.401e+05 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe parameter estimate for nt in our model summary above is an estimation of the reproduction factor (\\(R\\)) for this population. We can now determine the population size at any time using the below equation:\n\\[N(t) = N_{0}R^t\\]\nWhere \\(N_{0}\\) is our starting population size and \\(t\\) is time. How does our population estimate from this exponential model compare to our calculation with exact individual level data?\n\n#Extract R value\nR &lt;- our_model$coefficients[1]\n\n#Write out exponential func to estimate N at any time point\nNt_func &lt;- function(N0, R, t){\n  \n  N0*R^t\n  \n}\n\n#Run this function from 1 - 256 days\npredicted_N &lt;- Nt_func(N0 = length(day6_data), R, t = 1:256)\n\n\n#Percentage difference between our population estimates\n(diff(c(predicted_N[256], pop_size[256]))/pop_size[256])*100\n\n[1] 4.268097\n\n\nAlthough we didn’t have any information about the age of individuals or their exact reproductive behaviour in our exponential growth model we were able to estimate a population size that was only 4% different from the exact value we calculated where we had complete information about every individual. This is an example where estimating a value using a mathematical model can provide a plausible alternative in cases where we don’t know all the details about a particular system.\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "",
    "text": "👴 Old post\n\n\n\nThis post was created before the release of R v4.1.0. Some code might be outdated.\nAnother week, another TidyTuesday! As before, this figure was created during the CorrelAid TidyTuesday meetup where we brainstorm and troubleshoot ideas. You can see the plots from all the CorrelAid TidyTuesday meetups here."
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#introduction-the-bechdel-test-and-james-bond",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#introduction-the-bechdel-test-and-james-bond",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Introduction: The Bechdel test and James Bond",
    "text": "Introduction: The Bechdel test and James Bond\n\nIn 1985, cartoonist Alison Bechdel published a cartoon that described 3 ‘rules’ used to rate female representation in movies. The movie must:\n\nHave at least 2 named women in it.\nThese women talk with each other.\nThe women talk about something other than a man.\n\nSounds pretty simple, right? But very few films (fewer than 50%) pass the eponymous Bechdel test, including major blockbusters like “The Hobbit: An Unexpected Journey” and “The Avengers”.\nOne film series that we would not expect to do particularly well on the Bechdel test is that of James Bond. Since the release of Dr. No in 1962, 007 has never been known for his healthy relationship to women. But how do the Bond films actually compare to movies at large, and how have they changed over time?"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#setup-data-and-packages",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#setup-data-and-packages",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Setup: Data and packages",
    "text": "Setup: Data and packages\n\nThis week’s TidyTuesday included two datasets taken from FiveThirtyEight, one complete dataset including Bechdel test scores for 8,839 movies since 1888 and a smaller subset with additional movie data from IMDB. The packages used for this post were:\n\nlibrary(tidytuesdayR) #To download the data\nlibrary(dplyr) #For data wrangling\nlibrary(ggplot2) #For plotting\nlibrary(fuzzyjoin) #For string matching\nlibrary(showtext) #Use fonts stored on the Google Fonts database\n\n## For adding images to our plot\nlibrary(png) #For reading .png files\nlibrary(gridExtra) #For turning .png files into grobs\nlibrary(egg) #For adding grob objects to our plot\n\nTo start with we need to download the data and separate out the different datasets.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 11)\n\n\n    Downloading file 1 of 2: `raw_bechdel.csv`\n    Downloading file 2 of 2: `movies.csv`\n\nbechdel &lt;- tuesdata$raw_bechdel\nmovies  &lt;- tuesdata$movies\n\nWe’ll create a custom ggplot theme so we don’t need to make these aesthetic changes in every plot.\n\nmy_theme &lt;- function(){\n  \n  theme_classic() %+replace% \n    #Remove legend by default\n    theme(legend.position = \"none\",\n          #Make axes black\n          axis.text = element_text(colour = \"black\"),\n          #Make clear titles and caption\n          plot.title = element_text(size = 18, face = \"bold\", hjust = 0),\n          plot.caption = element_text(size = 8, hjust = 1))\n  \n}"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#exploration-how-does-the-data-look",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#exploration-how-does-the-data-look",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Exploration: How does the data look?",
    "text": "Exploration: How does the data look?\n\nFor this visualisation, we’re only really interested in the full Bechdel test dataset and don’t need the more detailed movie information. Rather than treating the Bechdel test as pass/fail, each movie will be given a score from 0 (less than 2 named women) to 3 (passes the full Bechdel test). The full Bechdel test dataframe is fairly simple and should be easy to use. The title, rating, and year columns will be important to help us identify Bond (and non-Bond movies) and there don’t appear to be any nasty surprises in the data structure!\n\ntail(bechdel)\n\n# A tibble: 6 × 5\n   year    id imdb_id  title                    rating\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                     &lt;dbl&gt;\n1  2021  9473 10332588 Finding ’Ohana                3\n2  2021  9498 5144174  The Dry                       3\n3  2021  9505 10919362 Sweetheart                    3\n4  2021  9501 10813940 Ginny and Georgia             2\n5  2021  9504 5109280  Raya and the Last Dragon      3\n6  2021  9500 9286908  High Ground                   2\n\n\nIf we want to focus on Bond movies, we need to include some external data to identify which films are in the Bond franchise. We’ll consider all Bond movies except the ‘spy-parody’ Casino Royale from 1967. We’ll use the fuzzyjoin package to join datasets using movie titles. Fuzzy joining will allow us to match similar titles even if they’re not exactly the same (e.g. missing punctuation or different capitalisation).\n\nbond_movies &lt;- read.csv(\"./data/james_bond_movies.csv\") %&gt;% \n  #We use fuzzyjoin incase there are slight differences in the titles (e.g. missing ')\n  fuzzyjoin::stringdist_left_join(bechdel, by = c(\"movie\" = \"title\")) %&gt;% \n  #Remove old Casino Royale and select relevant columns\n  dplyr::filter(!(movie == \"Casino Royale\" & year == \"1967\")) %&gt;% \n  dplyr::select(movie, year, actor, bond_rating = rating) %&gt;% \n  #Unfortunately one movie (The Living Daylights) doesn't have a Bechdel score.\n  #We still want to include this in our graph even if it doesn't have a score so we need to manually add the year\n  mutate(year = case_when(movie == \"The Living Daylights\" ~ 1987,\n                          TRUE ~ year))\n\nhead(bond_movies)\n\n                            movie year          actor bond_rating\n1                          Dr. No 1962   Sean Connery           1\n2           From Russia with Love 1963   Sean Connery           3\n3                      Goldfinger 1964   Sean Connery           1\n4                     Thunderball 1965   Sean Connery           2\n5             You Only Live Twice 1967   Sean Connery           1\n6 On Her Majesty's Secret Service 1969 George Lazenby           2"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Initial plots: What can we see when we start plotting?",
    "text": "Initial plots: What can we see when we start plotting?\n\nWe now have Bechdel scores of all (but one) of the Bond movies. To start exploring we can create a very rough plot to look at the average Bechdel score of each James Bond actor.\n\nggplot(data = bond_movies) +\n  geom_boxplot(aes(x = actor, y = bond_rating, fill = actor),\n               colour = \"black\", width = 0.15, alpha = 0.25) +\n  #Just use viridis fill for now\n  scale_fill_viridis_d() +\n  scale_y_continuous(breaks = c(1, 2, 3)) +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"Bechdel score of each Bond\",\n       subtitle = \"\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  my_theme() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nWe can see there are differences in the median Bechdel score of each James Bond actor, but this initial plot is a bit lacking. Are the differences we see related to the number of movies each actor starred in? Or are we seeing a cultural shift over time as more movies pass the Bechdel test? We need to disentangle these possibilities.\nFirstly, we want to look at the Bechdel score of all other movies during the time James Bond movies were being made.\n\nother_movies &lt;- bechdel %&gt;% \n  #Remove all Bond movies\n  fuzzyjoin::stringdist_anti_join(bond_movies, by = c(\"title\" = \"movie\")) %&gt;%\n  #Only consider movies in the years since Bond movies started\n  dplyr::filter(year &gt;= min(bond_movies$year) & year &lt;= max(bond_movies$year))\n\n\nrollingmean &lt;- other_movies %&gt;% \n  #Take a mean of Bechdel score for each year\n  dplyr::group_by(year) %&gt;% \n  dplyr::summarise(all_movie_rating = mean(rating),\n                   n = n()) %&gt;% \n  #Create a 3 year rolling mean\n  dplyr::mutate(lag1 = lag(all_movie_rating),\n                lead1 = lead(all_movie_rating)) %&gt;% \n  dplyr::rowwise() %&gt;% \n  dplyr::mutate(rollingmean_allmovies = mean(c(all_movie_rating, lag1, lead1))) %&gt;% \n  dplyr::select(-lag1, -lead1) %&gt;% \n  #Remove years where a rolling mean is not possible\n  dplyr::filter(!is.na(rollingmean_allmovies))\n\nWe next need to define the Bond ‘eras’ (the periods during which each actor played James Bond).\n\nbond_eras &lt;- bond_movies %&gt;% \n  group_by(actor) %&gt;% \n  summarise(start = min(year),\n            end = max(year),\n            midyear = mean(c(start, end)),\n            mean_score = mean(bond_rating, na.rm = TRUE))\n\nbond_eras\n\n# A tibble: 6 × 5\n  actor          start   end midyear mean_score\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Daniel Craig    2006  2015   2010.       1.5 \n2 George Lazenby  1969  1969   1969        2   \n3 Pierce Brosnan  1995  2002   1998.       2   \n4 Roger Moore     1973  1985   1979        1.43\n5 Sean Connery    1962  1971   1966.       1.5 \n6 Timothy Dalton  1987  1989   1988        2   \n\n\nNow we can plot the mean Bechdel scores over time and compare it to the mean Bechdel score of each Bond actor!\n\nggplot() +\n  #Adjust the location so that movies fall in the middle of a year rather than at the start\n  geom_rect(data = bond_eras, aes(xmin = start, xmax = end + 1, ymin = -Inf, ymax = Inf, fill =  actor),\n            colour = NA) +\n  geom_path(data = rollingmean, aes(x = year + 0.5, y = rollingmean_allmovies), size = 1, colour = \"blue\") +\n  geom_point(data = bond_eras, aes(x = midyear, y = mean_score), size = 3, shape = 21, fill = \"grey50\") +\n  scale_fill_viridis_d(name = \"\") +\n  scale_x_continuous(breaks = seq(1960, 2020, 5), name = \"\") +\n  scale_y_continuous(breaks = c(1, 2, 3)) +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"Bechdel score of each Bond\",\n       subtitle = \"\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  my_theme() +\n  theme(legend.position = \"right\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#digging-deeper-adding-images",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#digging-deeper-adding-images",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Digging deeper: Adding images",
    "text": "Digging deeper: Adding images\n\nWhile this graph serves our purposes, I don’t think it’s the ideal format to present the data. For one, by using year on the x-axis, we include many years where James Bond movies were not being made. Another problem we encounter is that the Bond ‘eras’ are not sequential, because one actor (George Lazenby) made a movie during the Sean Connery era. As an alternative, we can use the sequence of Bond movies as the x-axis. We can then compare the mean Bechdel score of each Bond to the mean Bechdel score of contemporary movies during their time in the role.\n\n#Specify the era 'order' of the actors\nbond_order &lt;- data.frame(actor = c(\"Sean Connery\",\n                                   \"George Lazenby\",\n                                   \"Roger Moore\",\n                                   \"Timothy Dalton\",\n                                   \"Pierce Brosnan\",\n                                   \"Daniel Craig\")) %&gt;%\n  mutate(era_nr = 1:n())\n\nbond_era_ratings &lt;- bond_movies %&gt;%\n  left_join(bond_order, by = \"actor\") %&gt;% \n  arrange(era_nr, year) %&gt;% \n  #Have a movie number that can be used as our x axis\n  tibble::rownames_to_column(var = \"movie_nr\") %&gt;%\n  #Separate first name and surname\n  tidyr::separate(actor, into = c(\"given_name\", \"surname\"), sep = \" \") %&gt;% \n  group_by(surname) %&gt;% \n  summarise(mean_bond_rating = mean(bond_rating, na.rm = TRUE),\n            first_yr = first(year),\n            #The first and last number are buffered by 0.5 on either side\n            #This allows George Lazanby (with 1 movie) to still be visible\n            first_nr = as.integer(first(movie_nr)) - 0.5,\n            last_yr = last(year),\n            last_nr = as.integer(last(movie_nr)) + 0.5,\n            middle_point = mean(c(first_nr, last_nr))) %&gt;% \n  #For each actor, determine the mean Bechdel score of all other movies during their era\n  mutate(mean_movie_rating = purrr::map2_dbl(.x = first_yr, .y = last_yr,\n                                             .f = function(first, last, movie_db){\n                                               \n                                               movie_db_subset &lt;- other_movies %&gt;% \n                                                 filter(year &gt;= first & year &lt;= last)\n                                               \n                                               mean(movie_db_subset$rating)\n                                               \n                                             }, movie_db = other_movies),\n         year_text = case_when(first_yr == last_yr ~ as.character(first_yr),\n                               TRUE ~ paste(first_yr, last_yr, sep = \"-\")))\n\n#Although we use movie number as our x axis, we want the labels to be the movie names\n#and the movie's Bechdel score\nx_labels &lt;- bond_movies %&gt;%\n  left_join(bond_order, by = \"actor\") %&gt;% \n  arrange(era_nr, year) %&gt;% \n  tibble::rownames_to_column(var = \"movie_nr\") %&gt;% \n  mutate(movie_nr = as.integer(movie_nr),\n         movie = paste0(movie, ' (', bond_rating, ')'))\n\n\nggplot(data = bond_era_ratings) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = \"black\", alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 1.5) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            size = 5) +\n  geom_point(aes(x = middle_point, y = mean_bond_rating),\n             size = 6, shape = 21, stroke = 2, fill = \"grey50\") + \n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.1,\n                label = surname), size = 7) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.2,\n                label = round(mean_bond_rating, 2)), size = 7) +\n  scale_x_continuous(breaks = x_labels$movie_nr, labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  coord_cartesian(expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, colour = \"black\", size = 15),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.text.y = element_text(size = 15, colour = \"black\"),\n        axis.title = element_text(size = 20),\n        plot.title = element_text(size = 25, face = \"bold\"),\n        plot.subtitle = element_text(size = 17))\n\n\n\n\nUsing points to show the ratings of different actors works well enough, but perhaps headshots of the different actors would be more effective. For this, we use the png and grid packages to create a grob, or ‘graphical object’. grobs come from the grid package and also underlie ggplot2 and grobs can be inserted directly into our plot using the geom_custom function from the egg package.\nFirst, we need to convert the .png files into grobs and store this in a dataframe.\n\n#List all .png files\nall_png &lt;- data.frame(file = list.files(\"./data\", pattern = \".png\", full.names = TRUE)) %&gt;%\n  #Extract surname of each actor from the png file path\n  mutate(surname = stringr::str_to_title(stringr::str_sub(basename(file), end = -5))) %&gt;% \n  #For each file, read it in as a .png and then convert to a grob\n  rowwise() %&gt;% \n  mutate(grob = list(grid::rasterGrob(png::readPNG(source = file),\n                                      width = unit(1, \"cm\"), height = unit(1, \"cm\"))))\n\n#Join data frame with grobs into original data\nbond_era_ratings_png &lt;- bond_era_ratings %&gt;% \n  left_join(all_png, by = \"surname\")\n\nThen we can add these png derived grobs into the plot in place of points.\n\nggplot(data = bond_era_ratings_png) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = \"black\", alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 1.5) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            size = 5) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.2,\n                label = round(mean_bond_rating, 2)), size = 7) +\n  egg::geom_custom(aes(x = middle_point, y = mean_bond_rating, data = grob), grob_fun = \"identity\") +\n  scale_x_continuous(breaks = seq(1, 24, 1), labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  coord_cartesian(expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, colour = \"black\", size = 15),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.text.y = element_text(size = 15, colour = \"black\"),\n        axis.title = element_text(size = 20),\n        plot.title = element_text(size = 25, face = \"bold\"),\n        plot.subtitle = element_text(size = 17),\n        plot.caption = element_text(size = 20))"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#the-finishing-touches-making-things-look-pretty",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#the-finishing-touches-making-things-look-pretty",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "The finishing touches: Making things look pretty",
    "text": "The finishing touches: Making things look pretty\n\nWe’ll now add some custom fonts and additional explanation to the plot. To add additional annotation to a plot I like to again take advantage of grobs. We can make our whole plot into a grob and place it inside a new (empty) plotting environment where the annotation can be included, using the geom_annotate function. With this method you don’t need to mess around with your plot margins, and it’s particularly effective if you want to include multiple plots together or create custom legends.\n\n#Load a font from Google Fonts\nsysfonts::font_add_google(name = \"Mouse Memoirs\", family = \"Mouse\")\nsysfonts::font_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\n#Specify that the showtext package should be used\n#for rendering text\nshowtext::showtext_auto()\n\n\nmain_plot &lt;- ggplot(data = bond_era_ratings_png) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = NA, alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 0.75) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            family = \"Mouse\", size = 7.5/.pt) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.155,\n                label = surname), family = \"Mouse\", colour = \"#D53129\", size = 7.5/.pt) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.23,\n                label = format(round(mean_bond_rating, 2), nsmall = 2)), family = \"Mouse\",\n            size = 7.5/.pt) +\n  geom_text(aes(x = middle_point, y = mean_movie_rating + 0.05,\n                label = format(round(mean_movie_rating, 2), nsmall = 2)), family = \"Mouse\",\n            size = 7.5/.pt) +\n  egg::geom_custom(aes(x = middle_point, y = mean_bond_rating, data = grob), grob_fun = \"identity\") +\n  scale_x_continuous(limits = c(0, NA), breaks = x_labels$movie_nr, labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"&lt;- Worse                       Bechdel score                       Better -&gt;\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\") +\n  coord_fixed(ratio = 7, expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 7.5,\n                                   family = \"Mouse\", colour = \"black\"),\n        axis.text.y = element_text(family = \"Mouse\", colour = \"black\", size = 7.5),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.title = element_text(family = \"Mouse\", size = 20),\n        plot.title = element_text(family = \"Mouse\", size = 25, margin = margin(b = 3.75)),\n        plot.subtitle = element_text(size = 7.5, margin = margin(b = 7.5)))\n\nmain &lt;- ggplot2::ggplotGrob(main_plot)\n\nlabels &lt;- data.frame(x = 0.87, y = c(0.6, 0.375),\n                     text = c(\"Movies from the same period\",\n                              \"Score of Bond movies\"))\n\nggplot()+\n  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0.35, 0.65), expand = c(0, 0)) +\n  annotation_custom(grob = main, xmin = 0, xmax = 0.75, ymin = 0, ymax = 1) +\n  geom_curve(aes(x = 0.825, xend = 0.7, y = 0.6, yend = 0.55), curvature = 0.2,\n             arrow = arrow(length = unit(0.1, \"inches\")), size = 1) +\n  geom_curve(aes(x = 0.825, xend = 0.7, y = 0.375, yend = 0.45), curvature = -0.2,\n             arrow = arrow(length = unit(0.1, \"inches\")), size = 1) +\n  geom_label(data = labels, aes(x = x, y = y, label = text),\n                        family = \"Mouse\", colour = '#D53129', size = 7.5/.pt) +\n  labs(caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  theme_void() +\n  theme(plot.caption = element_text(family = \"Ubuntu Mono\", size = 5))"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#conclusion",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#conclusion",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThe mean Bechdel score of movies has increased steadily since the first Bond movie was released in 1962, but Bond movies haven’t kept up! James Bond films have never been known for their realistic portrayal of female characters, but it seems that the newest Bond movies with Daniel Craig are particularly bad at passing the Bechdel test when compared to their contemporaries.\nOf course, the Bechdel test has been rightly criticised as a very simple method for assessing female representation and ignores other issues, such as the role of the female characters in the film, the amount of dialogue given to women, and the representation of women of colour. There are now a plethora of new tests that seek to provide a better assessment of the role of women both in front of and behind the camera. Even so, it’s striking that all but one of Daniel Craig’s films as James Bond fails to even include dialogue between two female characters! Will the upcoming No Time To Die be any better? Will a new James Bond actor herald a change to the franchise’s Bechdel scores? Or is 007 just a bit out of date?"
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html",
    "title": "Advent of Code 2021",
    "section": "",
    "text": "It’s that time of year again. Houses are lit up, presents are being bought, and Michael Bublé has emerged from his hibernation to create another album of Christmas songs. This year, in the spirit of giving, I will share my code for the 2021 Advent of Code challenges. Each day is a new small coding challenge and I’ll attempt to upload and explain all my solutions! If I miss posting on any days (or you just want to look at the code in more detail) check out my GitHub repo."
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html#day-1-let-the-advent-of-code-challenge-begin",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html#day-1-let-the-advent-of-code-challenge-begin",
    "title": "Advent of Code 2021",
    "section": "Day 1: Let the (Advent of Code) challenge begin!",
    "text": "Day 1: Let the (Advent of Code) challenge begin!"
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html#the-data",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nWe are given a numeric vector containing values of ocean depth. I’ll read it in as a data frame here.\n\nlibrary(readr)\n\nday1_data &lt;- read_delim(file = \"./data/Day1.txt\", delim = \"/t\",\n                        col_names = \"depth\", show_col_types = FALSE)\n\nhead(day1_data)\n\n# A tibble: 6 × 1\n  depth\n  &lt;dbl&gt;\n1   191\n2   185\n3   188\n4   189\n5   204\n6   213"
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html#the-challenges",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\n\nChallenge 1\n\nFor the first challenge we need to compare values along a vector and determine whether they increase or decrease. We then need to determine the number of times the depth value has increased (i.e. ocean floor became deeper). This one’s pretty straight forward, as we can just use the lead() function from dplyr to compare our vector of depths to its neighbour.\n\n#Each value in the vector is compared to its neighbour. Return logical to check whether depth has increased\nlogical_output &lt;- day1_data$depth &lt; lead(day1_data$depth)\n\n#Example output\nlogical_output[1:10]\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n\n#Final result\nsum(logical_output, na.rm = TRUE)\n\n[1] 1709\n\n\n\n\n\nChallenge 2\n\nFor the second challenge we need to first create a new vector using a sliding window of size 3 (the sum of each value and its next two neighbours). Then we make the same check to see how often depth has increased using this newly created vector. This isn’t too much harder as we can use the n argument in lead() to return the 1st and 2nd neighbours.\n\n#Create sliding window vector with window size 3\nnew_vector &lt;- day1_data$depth + lead(day1_data$depth, n = 1) + lead(day1_data$depth, n = 2)\nsum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n\n[1] 1761\n\n\n\n\n\nBONUS ROUND!\n\nThis works, but this type of code always feels a bit inflexible to me. What if needed to change the size of the window? We would need to re-write our definition of new_vector every time. What if the size of the sliding window becomes very large? The definition for new_vector would also become very long and difficult to follow. Is there a functional way we can deal with this?\nFirstly, I’ll consider an approach using the tidyverse where I work in a data frame. I create a loop with purrr to add any number of new columns each with a different lead value. You might notice the walrus operator (:=) which is used in the tidyverse to create dynamic column names. I then use the package lay and its eponymous function to efficiently apply a function (sum()) across rows of a data frame. You’ll need to install the lay package from GitHub.\n\n#remotes::install_github(\"courtiol/lay\")\nlibrary(lay)\nlibrary(purrr)\n\n#Create a function that can take any window size\ntidy_windowfunc &lt;- function(data, window_size = 3){\n  \n  new_vector &lt;- data %&gt;%\n    #Window includes the existing depth col, so we add window_size - 1 new columns\n    mutate(map_dfc(.x = (1:window_size) - 1,\n                   .f = function(i, depth){\n                     \n                     #Create a new column with lag i\n                     #Allow for dynamic col names\n                     tibble(\"depth_lag{i}\" := lead(depth, n = i))\n                     \n                   }, depth = .data$depth)) %&gt;%\n    #Sum all the newly created lag columns using lay\n    mutate(depth_sum = lay(across(.cols = contains(\"lag\")), .fn = sum)) %&gt;% \n    pull(depth_sum)\n  \n  sum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n  \n}\n\nNow we can solve both puzzles using just one function!\n\n#Recreate Challenge 1\ntidy_windowfunc(data = day1_data, window_size = 1) == sum(day1_data$depth &lt; lead(day1_data$depth), na.rm = TRUE)\n\n[1] TRUE\n\n#Recreate Challenge 2\nnew_vector &lt;- day1_data$depth + lead(day1_data$depth, n = 1) + lead(day1_data$depth, n = 2)\ntidy_windowfunc(data = day1_data, window_size = 3) == sum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n\n[1] TRUE\n\n\nThis was the tidyverse way, but can we do it any better using base R? My first thought here would be to try a while() loop, that continues adding new neighbours until window size is reached.\n\n#Create similar function with base R\nbase_windowfunc &lt;- function(data, window_size = 3){\n  \n  i &lt;- 1\n  depth      &lt;- data$depth\n  new_vector &lt;- data$depth\n  \n  #As long as we haven't reached window size, keep going!\n  while (i &lt; window_size) {\n    \n    #Add depth data shifted by i to our existing vector.\n    #Will ensure vectors are same length (adds NAs at the end)\n    new_vector &lt;- new_vector + depth[1:length(depth) + i]\n    i &lt;- i + 1\n    \n  }\n  \n  sum(new_vector[1:(length(new_vector) - 1)] &lt; new_vector[2:length(new_vector)], na.rm = TRUE)\n  \n}\n\nThis works too and, as with most base R code, it’s 100% dependency free!\n\n#Recreate Challenge 1\ntidy_windowfunc(data = day1_data, window_size = 1) == base_windowfunc(data = day1_data, window_size = 1)\n\n[1] TRUE\n\n#Recreate Challenge 2\ntidy_windowfunc(data = day1_data, window_size = 3) == base_windowfunc(data = day1_data, window_size = 3)\n\n[1] TRUE\n\n\nSo which one is faster? I would assume that by working exclusively with vectors our base R function will be faster than our tidy function that spends some time manipulating a data frame. Is this actually the case? And if so, how much faster do we get? Let’s look at the number of iterations/sec for each function and different window sizes using the mark() function in the bench package.\n\nlibrary(bench)\n\nbench_df &lt;- map_df(.x = 1:20,\n                   .f = function(i){\n                     \n                     times &lt;- mark(tidy_windowfunc(data = day1_data, window_size = i),\n                                   base_windowfunc(data = day1_data, window_size = i))\n                     \n                     data.frame(method = c(\"tidy\", \"base\"),\n                                size = i,\n                                number_per_sec = times$`itr/sec`,\n                                speed_sec = 1/times$`itr/sec`)\n                     \n                   })\n\nggplot(bench_df) +\n  geom_line(aes(x = size, y = number_per_sec, colour = method), size = 1) +\n  geom_point(aes(x = size, y = number_per_sec, colour = method), stroke = 1.5, shape = 21, fill = \"white\") +\n  scale_y_continuous(name = \"Iterations/sec\",\n                     breaks = seq(0, 15000, 5000), limits = c(0, 15000)) +\n  scale_x_continuous(name = \"Window size\") +\n  scale_colour_discrete(name = \"\") +\n  labs(title = \"Advent of Code Day 1\",\n       subtitle = \"base v. tidyverse approach\") +\n  theme_classic(base_family = \"Courier New\") +\n  theme(axis.text = element_text(colour = \"black\"),\n        legend.position = c(0.8, 0.8),\n        legend.background = element_rect(colour = \"black\"),\n        legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAs expected, base R is more efficient, especially if we only need to work with short windows; however, this advantage decreases rapidly as we start to work with larger windows. Although it is slower, I think the tidyverse method I’ve used here is also very versatile, particularly if you want to return a data frame and not just a vector. Either way…they both get the correct result!"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html",
    "title": "Building maps using OpenStreetMap",
    "section": "",
    "text": "👴 Old post\n\n\n\nThis post was created before the release of R v4.1.0. Some code might be outdated."
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#where-are-the-shapefiles",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#where-are-the-shapefiles",
    "title": "Building maps using OpenStreetMap",
    "section": "Where are the shapefiles?!",
    "text": "Where are the shapefiles?!\n\nWhen I first started making maps and doing spatial analyses in R I often encountered a problem finding the data I needed. Where could I find a line showing state borders or a polygon of a nearby river? There are of course a huge number of ways to source this type of information, but for a beginner (and even sometimes as somebody more experienced) it can be difficult to keep track of all the different data sources. That’s why I was so excited to find out about the osmdata package that allows you to query OpenStreetMap data directly in R. Access to an almost unlimited amount of spatial information from across the globe, what’s not to like?!\nTo show off the power of OpenStreetMap data I’ll build a train map of Melbourne (my home town). Here’s what we’ll need:\n\nosmdata: For querying OpenStreetMap\nggplot2: For plotting the information we extract\nsf: For working with spatial data in R\ndplyr: For data wrangling\n\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#creating-a-query",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#creating-a-query",
    "title": "Building maps using OpenStreetMap",
    "section": "Creating a query",
    "text": "Creating a query\n\nThe first step is to specify the area within which we want to search for OpenStreetMap features. We can specify the latitude and longitude limits manually, but you can also use the getbb() function to return the limits of a particular place (e.g. a city).\n\n#Example of a bounding box for Melbourne\n#It's often necessary to specify the greater city limit to ensure the\n#bounding box is large enough\nmelb_bb &lt;- getbb(place_name = \"Melbourne, Australia\")\n\nmelb_bb\n\n        min       max\nx 144.44405 146.19250\ny -38.49937 -37.40175\n\n\nThen we can use the opq() function to start querying the OpenStreetMap API. You can build the query using pipes.\nThe first thing we’ll want to add to the query is the type of feature we want to return with add_osm_feature(). There are a lot of possible objects we could extract from OpenStreetMap. In this case we want to find train lines and stations. The key-value pair to use is not always obvious (e.g. a cycle path is under the ‘highway’ key), so you may want to explore different key-value pairs here or search for specific features on Nominatim.\n\n\n\n\n\n\nCoding tip\n\n\n\nIf you’re querying over a large area it can be good to increase the timeout argument from the default 25.\n\n\n\n#Query railway lines\nmelb_query_line &lt;- opq(bbox = melb_bb, timeout = 120) %&gt;% \n  add_osm_feature(key = 'route', value = 'train')\n\nmelb_query_station &lt;- opq(bbox = melb_bb, timeout = 120) %&gt;% \n  add_osm_feature(key = 'railway', value = 'station')\n\nmelb_query_line\n\n$bbox\n[1] \"-38.49937,144.44405,-37.40175,146.1925\"\n\n$prefix\n[1] \"[out:xml][timeout:120];\\n(\\n\"\n\n$suffix\n[1] \");\\n(._;&gt;;);\\nout body;\"\n\n$features\n[1] \"[\\\"route\\\"=\\\"train\\\"]\"\n\n$osm_types\n[1] \"node\"     \"way\"      \"relation\"\n\nattr(,\"class\")\n[1] \"list\"           \"overpass_query\"\nattr(,\"nodes_only\")\n[1] FALSE"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#extracting-some-data",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#extracting-some-data",
    "title": "Building maps using OpenStreetMap",
    "section": "Extracting some data",
    "text": "Extracting some data\n\nWe now have a query for the OpenStreetMap Overpass API…but we still need to run the query and return the spatial data! For this we can use osmdata_sf() to return information as sf objects (or osmdata_sp() if you’re working with the sp package).\n\nmelbourne_trainline &lt;- melb_query_line %&gt;% \n  osmdata_sf()\n\nmelbourne_station &lt;- melb_query_station %&gt;% \n  osmdata_sf()\n\nmelbourne_trainline\n\nObject of class 'osmdata' with:\n                 $bbox : -38.49937,144.44405,-37.40175,146.1925\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 50232 points\n            $osm_lines : 'sf' Simple Features Collection with 4258 linestrings\n         $osm_polygons : 'sf' Simple Features Collection with 338 polygons\n       $osm_multilines : 'sf' Simple Features Collection with 158 multilinestrings\n    $osm_multipolygons : NULL\n\n\nWe now have a whole range of objects (lines, points, and polygons), that fit our specific key-value pairs. We can already use these data to create a basic map with train lines and stations!\n\nmelbourne_trainline_lines &lt;- melbourne_trainline$osm_lines\nmelbourne_station_points  &lt;- melbourne_station$osm_points\n\nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points, size = 1, shape = 21, colour = \"black\", fill = \"dark grey\") +\n  theme_void()"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#tidying-up-osm-data",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#tidying-up-osm-data",
    "title": "Building maps using OpenStreetMap",
    "section": "Tidying up OSM data",
    "text": "Tidying up OSM data\n\nThis doesn’t look quite right! Although we queried within the greater Melbourne area, the lines and polygons can extend outside this bounding box. We can deal with this a number of ways. Here we’ll use a combination of trim_osmdata() from within osmdata to clip our lines and the coord_sf() function to adjust the limits of plot to match the bounding box of Melbourne we’ve been using.\n\nmelb_bb_poly &lt;- getbb(place_name = \"Melbourne, Australia\",\n                      format_out = \"sf_polygon\") %&gt;% \n#There are two very similar bounding boxes. We'll use the first one.\n  slice(1)\n\nmelbourne_trainline_lines &lt;- melbourne_trainline_lines %&gt;% \n  sf::st_filter(melb_bb_poly)\n  #Use exclude = FALSE to include lines that partially overlap our boundaries\n  # trim_osmdata(bb_poly = melb_bb_poly, exclude = FALSE)\n\nmelbourne_station_points &lt;- melbourne_station_points %&gt;% \n  sf::st_filter(melb_bb_poly)\n  # trim_osmdata(bb_poly = melb_bb_poly, exclude = FALSE)\n\n# melbourne_trainline_lines &lt;- melbourne_trainline_trim$osm_lines\n# melbourne_station_points  &lt;- melbourne_station_trim$osm_points\n\nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points, size = 1, shape = 21, colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])\n\n\n\n\nUsing OpenStreetMap gives us access to a huge source of spatial information, but because the data is created by a community of volunteers it can often require some cleaning. In our case, we can see there are a few stations that do not have corresponding train lines. This may occur because the train lines have been stored with a different key-value combination or are stored as a different object type, such as a polygon. For now, we will only include stations that are close (less than 1km) from a train line.\n\n\n\n\n\n\nNote\n\n\n\nWe do not use overlap because this will require points to sit exactly on the lines, which will often not occur due to things such as measurement error in the point locations.\n\n\n\n#For each station, determine the distance to all train lines\nmin_dist &lt;- sf::st_distance(melbourne_station_points,\n                            melbourne_trainline_lines) %&gt;% \n  #Determine the minimum distance for each station\n  apply(MARGIN = 1, FUN = min)\n\nmelbourne_station_points_subset &lt;- melbourne_station_points %&gt;% \n  dplyr::mutate(dist = min_dist) %&gt;% \n  dplyr::filter(dist &lt;= 1000)\n  \nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points_subset, size = 1, shape = 21,\n          colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#filtering-by-name",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#filtering-by-name",
    "title": "Building maps using OpenStreetMap",
    "section": "Filtering by name",
    "text": "Filtering by name\n\nWe now have a cleaned dataset of trainlines and stations within greater Melbourne, now we can work to make this map more informative by adding other features, like rivers and water bodies. We can also source this information from OpenStreetMap.\nWe want to add the Yarra river the bays and ocean around Melbourne to our map. Rivers can be found using the waterway-river key pair, but we need to apply additional filters to make sure we don’t get all rivers in Melbourne. To do this, we can apply a second filter with add_osm_feature() to search for objects with a specific name. By using value_exact = FALSE we allow for possible spelling differences in the name of the objects. Looking at Nominatim, we can see that the water bodies near Melbourne are all encompassed by the ‘Bass Strait’ multi-polygon (found with the natural-strait key pair).\n\nbass_strait &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'natural', value = 'strait') %&gt;% \n  add_osm_feature(key = \"name\", value = \"Bass Strait\", value_exact = FALSE) %&gt;%\n  osmdata_sf()\n\nbass_strait_polygon &lt;- bass_strait$osm_multipolygons\n\nbays &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'natural', value = 'bay') %&gt;% \n  osmdata_sf()\n\nbays_polygon &lt;- bays$osm_multipolygons\n\nyarra &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'waterway', value = 'river') %&gt;% \n  add_osm_feature(key = \"name\", value = \"Yarra River\", value_exact = FALSE) %&gt;%\n  osmdata_sf()\n\nyarra_line &lt;- yarra$osm_multilines\n\nggplot() +\n  geom_sf(data = bass_strait_polygon, fill = \"blue\", colour = NA) +\n  geom_sf(data = bays_polygon, fill = \"blue\", colour = NA) +\n  geom_sf(data = yarra_line, colour = \"blue\", size = 1.5) +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"gray\") +\n  geom_sf(data = melbourne_station_points_subset, size = 2, shape = 21,\n          colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#making-things-pretty",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#making-things-pretty",
    "title": "Building maps using OpenStreetMap",
    "section": "Making things pretty",
    "text": "Making things pretty\n\nWe now have the data we need to create a nice looking map of the Melbourne metro-train network. We can add some nice aesthetic touches to make it look nicer. For this we’ll load some packages:\n\nshowtext: Adding custom fonts from Google fonts\n\n\nlibrary(showtext)\n\nfont_add_google(\"Alice\", \"Alice\")\nfont_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\nshowtext_auto()\n\n\nggplot() +\n  geom_sf(data = bass_strait_polygon, fill = \"#33658A\", colour = NA) +\n  geom_sf(data = bays_polygon, fill = \"#33658A\", colour = NA) +\n  geom_sf(data = yarra_line, colour = \"#33658A\", size = 1.5) +\n  geom_sf(data = melbourne_trainline_lines, size = 1.2, colour = \"gray40\") +\n  #Add a label for port phillip bay and bass strait\n  geom_text(aes(label = \"Port Phillip Bay\"), x = 144.859280, y = -38.092014,\n            family = \"Alice\", fontface = \"bold\", colour = \"grey90\") +\n  geom_text(aes(label = \"Bass Strait\"), x = 144.513047, y = -38.421423,\n            family = \"Alice\", fontface = \"bold\", colour = \"grey90\") +\n  #Add title as text inside the plot\n  geom_text(aes(label = \"- Trains of Melbourne -\"), x = 145.75, y = -37.43,\n            family = \"Alice\", size = 8,\n            colour = \"grey45\", fontface = \"bold\") +\n  geom_text(aes(label = \"Using OpenStreetMap data\"), x = 145.75, y = -37.5,\n            family = \"Alice\", size = 4,\n            colour = \"grey45\", fontface = \"bold\") +\n  labs(caption = \"Data: OSM | Plot: @ldbailey255\") +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ]) +\n  theme_classic() +\n  theme(panel.background = element_rect(fill = \"#f2eadf\"),\n        panel.border = element_rect(fill = NA, colour = \"grey45\", linewidth = 1.5),\n        axis.text = element_text(family = \"Alice\", size = 14),\n        plot.caption = element_text(family = \"Ubuntu Mono\", size = 12,\n                                  colour = \"grey45\"),\n        axis.line = element_blank())"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html",
    "title": "Making beautiful tables with the gt package",
    "section": "",
    "text": "👴 Old post\n\n\n\nThis post was created before the release of R v4.1.0. Some code might be outdated."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-dreaded-table",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-dreaded-table",
    "title": "Making beautiful tables with the gt package",
    "section": "The dreaded table",
    "text": "The dreaded table\n\nI dread encountering a data table in an academic paper. A jumble of numbers, column sub-headers, and confusing footnotes, seemingly bereft of purpose. Why, I cry, is this not in the supplementary material?! Surely, I think, they could’ve created a graph! Despite my prejudice, the fact is that we often do need to use tables to present data. While a plot is a beautiful way to show trends or patterns, it falls short if your goal is to compare individual values. This year saw the release of the gt package, a new addition to the growing number of packages that can produce publication quality tables in R. It seemed like as good a time as any to overcome my dread of tables. Here is my attempt."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-data",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-data",
    "title": "Making beautiful tables with the gt package",
    "section": "The data",
    "text": "The data\n\nAs a test case I decided to work with data on greenhouse gas emissions from aviation travel, recently published in Our World in Data. Hannah Ritchie does a great job explaining the complexity of calculating per capita aviation emissions and I would highly recommend giving her article a read. For now, though, we’ll side-step this complexity and focus on the simplest case, emissions from domestic travel. The map from Hannah’s post clearly shows the range and disparity in emissions across the globe, but if we want to compare individual values between countries, or maybe within continents, such a map isn’t the best option. How about a table? While the original post includes a basic table to compare emissions, I wanted to see whether I could create a table of my own using only R. A table that I might be happy to encounter in a publication."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-basics-clean-the-data-and-add-some-context",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-basics-clean-the-data-and-add-some-context",
    "title": "Making beautiful tables with the gt package",
    "section": "The basics: Clean the data and add some context",
    "text": "The basics: Clean the data and add some context\n\nWe’ll need dplyr to tidy some of the data and gt to build our tables. scales will be used for creating colour palettes.\n\nlibrary(dplyr)\nlibrary(scales)\nlibrary(gt)\npackageVersion(\"gt\")\n\n[1] '0.8.0'\n\n\ngt integrates well into the existing tidyverse, and creating a gt table is as simple as two lines of code.\n\n\n\n\n\n\nNote\n\n\n\nFor this example we’re just showing the worst emitters.\n\n\n\n#Load data and arrange in descending order of emissions\nemissions_data &lt;- read.csv(here::here(\"posts/2020-11-27-making-beautiful-tables-with-gt/assets/per-capita-co2-domestic-aviation.csv\")) %&gt;% \n  arrange(desc(Per.capita.domestic.aviation.CO2))\n\n#Generate a gt table from head of data\nhead(emissions_data) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\nEntity\nCode\nPer.capita.domestic.aviation.CO2\n\n\n\n\nUnited States\nUSA\n0.3855204\n\n\nAustralia\nAUS\n0.2671672\n\n\nNorway\nNOR\n0.2092302\n\n\nNew Zealand\nNZL\n0.1741885\n\n\nCanada\nCAN\n0.1682674\n\n\nJapan\nJPN\n0.0739585\n\n\n\n\n\n\n\nA table, yes, but not one you’re likely to publish! To take our first steps towards table perfection we need to tidy up the data and add a title and data source. The reader needs to understand what’s going on. While much of the tidying could be completed using dplyr before converting to a table I’ll demonstrate how this could be achieved exclusively inside gt.\n\n\n\n\n\n\nCoding tip\n\n\n\nNotice how we select columns using the generic c(). For larger, more complex data tidy-select functions like starts_with() or contains() may come in handy.\n\n\n\n(emissions_table &lt;- head(emissions_data) %&gt;% \n   gt() %&gt;% \n   #Hide unwanted columns\n   cols_hide(columns = c(Code)) %&gt;% \n   #Rename columns\n   cols_label(Entity = \"Country\",\n              Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n   #Add a table title\n   #Notice the `md` function allows us to write the title using markdown syntax (which allows HTML)\n   tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n   #Add a data source footnote\n   tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\"))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (tonnes)\n    \n  \n  \n    United States\n0.3855204\n    Australia\n0.2671672\n    Norway\n0.2092302\n    New Zealand\n0.1741885\n    Canada\n0.1682674\n    Japan\n0.0739585\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\n    \n  \n  \n\n\n\n\nA few lines of code and the table is already much better. To me there is still one issues that needs to be fixed before we’ve finished the basics. While it’s technically correct to report emissions in tonnes I feel the data would be much more suitable in kilograms. For this we’ll use fmt_number().\n\n\n\n\n\n\nCoding tip\n\n\n\nThere are fmt_xxx() functions for many different data types, including fmt_currency() and fmt_date().\n\n\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #Format numeric column. Use `scale_by` to divide by 1,000. (Note: we'll need to rename the column again)\n   fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n              scale_by = 1000) %&gt;%\n   #Our second call to cols_label overwrites our first\n   cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\"))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-touchup-adding-some-style-and-colour",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-touchup-adding-some-style-and-colour",
    "title": "Making beautiful tables with the gt package",
    "section": "The touchup: Adding some style and colour",
    "text": "The touchup: Adding some style and colour\n\nWe’ve got a working table, but it won’t be winning any points for style. Our next step is to change the style of different cells to help the reader more clearly find the information they’re after. For this we’ll use the tab_style() function. The style choices here follow some of the ‘Ten Guidelines for Better Tables’ from Jon Schwabish.\nFirstly, we need to more clearly distinguish between the column headers and the body of the table (and while we’re at it, also the title!)\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #Apply new style to all column headers\n   tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       #Give a thick border below\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       #Make text bold\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   #Apply different style to the title\n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\n    \n  \n  \n\n\n\n\nAs our reader is interested in comparing emissions values between countries, we can add a heatmap to our cells to more clearly show the differences. This will require us to set a colour palette and apply a conditional colouring using data_color(). We’ll use the same palette employed in Hannah Ritchie’s map above.\n\n#Apply our palette explicitly across the full range of values so that the top countries are coloured correctly\nmin_CO2 &lt;- min(emissions_data$Per.capita.domestic.aviation.CO2)\nmax_CO2 &lt;- max(emissions_data$Per.capita.domestic.aviation.CO2)\nemissions_palette &lt;- col_numeric(c(\"#FEF0D9\", \"#990000\"), domain = c(min_CO2, max_CO2), alpha = 0.75)\n\n(emissions_table &lt;- emissions_table %&gt;% \n    data_color(columns = c(Per.capita.domestic.aviation.CO2),\n               colors = emissions_palette))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-finer-details-table-options",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-finer-details-table-options",
    "title": "Making beautiful tables with the gt package",
    "section": "The finer details: Table options",
    "text": "The finer details: Table options\n\nWe’ve added some colour and style, but if we really want to customise our table and tweak the fine details we need to start using the opt_xxx() and tab_options() functions. The opt_xxx() functions adjust specific elements of the table, while tab_options() is similar to the theme() function used with ggplot2. Below, we’ll adjust the options to resemble tables from fivethirtyeight (working from Thomas Mock’s great blog).\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #All column headers are capitalised\n   opt_all_caps() %&gt;% \n   #Use the Chivo font\n   #Note the great 'google_font' function in 'gt' that removes the need to pre-load fonts\n   opt_table_font(\n     font = list(\n       google_font(\"Chivo\"),\n       default_fonts()\n     )\n   ) %&gt;%\n   #Change the width of columns\n   cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n              c(Entity) ~ px(400)) %&gt;% \n   tab_options(\n     #Remove border between column headers and title\n     column_labels.border.top.width = px(3),\n     column_labels.border.top.color = \"transparent\",\n     #Remove border around table\n     table.border.top.color = \"transparent\",\n     table.border.bottom.color = \"transparent\",\n     #Reduce the height of rows\n     data_row.padding = px(3),\n     #Adjust font sizes and alignment\n     source_notes.font.size = 12,\n     heading.align = \"left\"\n   ))\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-1-adding-images",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-1-adding-images",
    "title": "Making beautiful tables with the gt package",
    "section": "Bonus round 1: Adding images",
    "text": "Bonus round 1: Adding images\n\nI’m already really happy with this table. It clearly allows a comparison of exact emissions between countries, and the colour provides additional assistance to the reader. We could leave it here, but for a little bonus let’s add country flags to our plot. We can use flags sources from wikicommons (a database for with URL locations is available through data.world).\nTo link this flag data base to our emissions data we need to translate country names into 3-letter country codes. We can do this using the countrycode package. The code for this is not really relevant for using gt but you can see it below if you’re interested. The end goal is to create a column containing the URL of a flag image for each country.\n\n\nDetails…\n\n\n#To convert country codes\nlibrary(countrycode)\n\nflag_db &lt;- read.csv(\"assets/Country_Flags.csv\") %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\nflag_data &lt;- emissions_data %&gt;% \n  left_join(flag_db, by = \"Code\") %&gt;% \n  select(flag_URL, Entity, everything())\n\n#We'll need to refit our table using this new data\n#Code below with comments removed.\nemissions_table &lt;- head(flag_data) %&gt;% \n  gt() %&gt;% \n  cols_hide(columns = c(Code)) %&gt;% \n  cols_label(Entity = \"Country\",\n             Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n  tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n  tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\") %&gt;% \n  fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n             scale_by = 1000) %&gt;%\n  cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\") %&gt;% \n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ) %&gt;% \n  data_color(columns = c(Per.capita.domestic.aviation.CO2),\n             colors = emissions_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n             c(Entity) ~ px(400)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\")\n\n\n\n\nhead(flag_data)\n\n                                                                                   flag_URL\n1              https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg\n2 https://upload.wikimedia.org/wikipedia/commons/8/88/Flag_of_Australia_%28converted%29.svg\n3                    https://upload.wikimedia.org/wikipedia/commons/d/d9/Flag_of_Norway.svg\n4               https://upload.wikimedia.org/wikipedia/commons/3/3e/Flag_of_New_Zealand.svg\n5                         https://upload.wikimedia.org/wikipedia/en/c/cf/Flag_of_Canada.svg\n6                          https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg\n         Entity Code Per.capita.domestic.aviation.CO2\n1 United States  USA                        0.3855204\n2     Australia  AUS                        0.2671672\n3        Norway  NOR                        0.2092302\n4   New Zealand  NZL                        0.1741885\n5        Canada  CAN                        0.1682674\n6         Japan  JPN                        0.0739585\n\n\nWe can now use the text_transform() function to add our country flags. text_transform() allows us to apply any custom function to a column. We can use the web_image function in gt to convert a URL to an embedded image.\n\nemissions_table %&gt;% \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    \nUnited States\n385.52\n    \nAustralia\n267.17\n    \nNorway\n209.23\n    \nNew Zealand\n174.19\n    \nCanada\n168.27\n    \nJapan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-2-within-group-comparison",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-2-within-group-comparison",
    "title": "Making beautiful tables with the gt package",
    "section": "Bonus round 2: Within-group comparison",
    "text": "Bonus round 2: Within-group comparison\n\nIt’s pretty clear by now that the US and Australia are the worst for domestic aviation emissions, but what if we wanted to compare within continents. Which country has the highest emissions within Africa or S. America? For this, we can use the row grouping functionality in gt.\nBefore we can do this, we’ll need to assign each country to its corresponding continent. As above, this isn’t really gt relevant, but the code is included below if you’re interested.\n\n\nDetails…\n\n\ncontinent_data &lt;- flag_data %&gt;% \n  #Convert iso3 codes to FIPS\n  mutate(continent = countrycode(sourcevar = Code, origin = \"iso3c\", destination = \"continent\", warn = FALSE)) %&gt;% \n  select(continent, flag_URL, Entity, Per.capita.domestic.aviation.CO2)\n\n\n\n\nhead(continent_data)\n\n  continent\n1  Americas\n2   Oceania\n3    Europe\n4   Oceania\n5  Americas\n6      Asia\n                                                                                   flag_URL\n1              https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg\n2 https://upload.wikimedia.org/wikipedia/commons/8/88/Flag_of_Australia_%28converted%29.svg\n3                    https://upload.wikimedia.org/wikipedia/commons/d/d9/Flag_of_Norway.svg\n4               https://upload.wikimedia.org/wikipedia/commons/3/3e/Flag_of_New_Zealand.svg\n5                         https://upload.wikimedia.org/wikipedia/en/c/cf/Flag_of_Canada.svg\n6                          https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg\n         Entity Per.capita.domestic.aviation.CO2\n1 United States                        0.3855204\n2     Australia                        0.2671672\n3        Norway                        0.2092302\n4   New Zealand                        0.1741885\n5        Canada                        0.1682674\n6         Japan                        0.0739585\n\n\nIn this case, we need to start from the beginning apply grouping to our table before we begin.\n\n(emissions_table_continent &lt;- continent_data %&gt;%\n  #Just take the top 5 from each continent for our example\n  group_by(continent) %&gt;% \n  slice(1:5) %&gt;% \n  #Just show Africa and Americas for our example\n  filter(continent %in% c(\"Africa\", \"Americas\")) %&gt;%\n  #Group data by continent\n  gt(groupname_col = \"continent\") %&gt;% \n  #Add flag images as before\n  gt::text_transform(\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\") %&gt;% \n  #Original changes as above.\n  cols_label(Entity = \"Country\",\n             Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n  tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n  tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\") %&gt;% \n  fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n             scale_by = 1000) %&gt;%\n  cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\") %&gt;% \n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ) %&gt;% \n  data_color(columns = c(Per.capita.domestic.aviation.CO2),\n             colors = emissions_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n             c(Entity) ~ px(400)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\",\n    #Adjust grouped rows to make them stand out\n    row_group.background.color = \"grey\"))\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    \n      Africa\n    \n    \nSouth Africa\n25.09\n    \nNamibia\n11.49\n    \nMauritius\n8.75\n    \nKenya\n7.10\n    \nAlgeria\n4.43\n    \n      Americas\n    \n    \nUnited States\n385.52\n    \nCanada\n168.27\n    \nChile\n70.46\n    \nBrazil\n42.86\n    \nMexico\n39.93\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-conclusion-tables-can-be-nice",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-conclusion-tables-can-be-nice",
    "title": "Making beautiful tables with the gt package",
    "section": "The Conclusion: Tables can be nice!",
    "text": "The Conclusion: Tables can be nice!\n\nA package like gt really unlocks the power of a table to display data. Using a combination of style and colour it’s easy to create a clear output that allows a reader to compare individual values. More over, we have the potential to easily include neat additions such as embedded images or graphs. The one draw back of gt is that it only generates static tables, to build interactive tables with filters or tabs we will need to move into other packages like reactable. Still, it’s a fun tool to add to the data visualisation toolbox of anybody working in R."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "",
    "text": "👴 Old post\n\n\n\nThis post was created before the release of R v4.1.0. Some code might be outdated.\nThis will hopefully be the first in many TidyTuesday data visualization posts! This week I joined the TidyTuesday group at CorrelAid to brainstorm and troubleshoot ideas. You can see the plots from all the CorrelAid TidyTuesday meetups here."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#introduction-analysing-income-inequality",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#introduction-analysing-income-inequality",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Introduction: Analysing income inequality",
    "text": "Introduction: Analysing income inequality\n\nIncome and wealth inequality in the world’s major economies has become an increasing concern over the past decade [1, 2, 3]. We often see income inequality quantified using tools such as the Gini Coeffient or other more complex metrics, which compare the income of all individuals across society. Unfortunately, broad societal metrics often fail to highlight other inequalities between subgroups, such as gender and race. In this week’s TidyTuesday, I visualise a rough metric of racial inequality in the US since 2010, and demonstrate how things have become more unequal over time."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#setup-data-and-packages",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#setup-data-and-packages",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Setup: Data and packages",
    "text": "Setup: Data and packages\n\nTidyTuesday Week 9 (2021) used employment and earning data from the US Bureau of Labor Statistics collected since 2010. There are a number of interesting ways we can wrangle and visualise this data, I’ll focus on racial inequality in income. We won’t need many advanced packages for this:\n\nlibrary(tidytuesdayR) #To download the data\nlibrary(dplyr) #For data wrangling\nlibrary(ggplot2) #For plotting\nlibrary(showtext) #Use fonts stored on the Google Fonts database\n\nTo start with we need to download the data and separate out employment and earnings data.\n\ntuesdata &lt;- tt_load(2021, week = 9)\n\n\n    Downloading file 1 of 2: `earn.csv`\n    Downloading file 2 of 2: `employed.csv`\n\nemployed &lt;- tuesdata$employed\nearn     &lt;- tuesdata$earn\n\nWe’ll create a custom ggplot theme so we don’t need to make these aesthetic changes in every plot.\n\nmy_theme &lt;- function(){\n  \n  theme_classic() %+replace% \n    #Remove legend by default\n    theme(legend.position = \"none\",\n          #Make axes black\n          axis.text = element_text(colour = \"black\"),\n          #Make clear titles and caption\n          plot.title = element_text(size = 18, face = \"bold\", hjust = 0),\n          plot.caption = element_text(size = 8, hjust = 1))\n  \n}"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#exploration-how-does-the-data-look",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#exploration-how-does-the-data-look",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Exploration: How does the data look?",
    "text": "Exploration: How does the data look?\n\nMy idea was to look at racial differences in income, so we’ll focus on the earn data frame. Before we go any further, we need to have a quick look at the data and see how it’s structured.\n\nhead(earn)\n\n# A tibble: 6 × 8\n  sex       race  ethnic_origin age    year quarter n_persons median_weekly_earn\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1 Both Sex… All … All Origins   16 y…  2010       1  96821000                754\n2 Both Sex… All … All Origins   16 y…  2010       2  99798000                740\n3 Both Sex… All … All Origins   16 y…  2010       3 101385000                740\n4 Both Sex… All … All Origins   16 y…  2010       4 100120000                752\n5 Both Sex… All … All Origins   16 y…  2011       1  98329000                755\n6 Both Sex… All … All Origins   16 y…  2011       2 100593000                753\n\n\nI can already see a few difficulties! Firstly, the columns sex, race, and age include aggregate categories like ‘Both Sexes’ and ‘All Races’. We need to be careful how we deal with these columns so we don’t double count individuals. To start with, we’ll group all ages and sexes and just look at racial differences. Secondly, out measure of ‘time’ is separated into two columns ‘year’ and ‘quarter’. If we want to analyse trends over time we’ll need to combine these two pieces of information. We can create an aggregate ‘time’ column where each financial quarter is added to year as a fraction (e.g. 2010 quarter 1 = 2010.0, 2010 quarter 2 = 2010.25).\n\n\n\n\n\n\nNote\n\n\n\n‘Hispanic’ does not fit neatly within the ‘race’ categories and is instead included under ‘ethnic_origin’. For simplicity we’ll be ignoring this column, but we are likely missing some nuance by doing so!\n\n\n\nplot_data &lt;- earn %&gt;%\n  #Consider aggregate groups for sex and age, but separate race.\n  filter(sex == \"Both Sexes\" & race != \"All Races\" & age == \"16 years and over\") %&gt;% \n  #Create new time column (combine year and quarter)\n  mutate(time = year + (quarter-1)/4)"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Initial plots: What can we see when we start plotting?",
    "text": "Initial plots: What can we see when we start plotting?\n\nNow we’ve got our data we can start plotting. I find it’s always best to visualise the data in as simple a format as possible before we try to achieve anything more complex and I always like the violin/boxplot combination as a first step.\n\nggplot(data = plot_data) +\n  geom_violin(aes(x = race, y = median_weekly_earn, fill = race),\n              colour = \"black\", width = 0.75) +\n  geom_boxplot(aes(x = race, y = median_weekly_earn),\n               colour = \"black\", width = 0.15, alpha = 0.25) +\n  scale_fill_manual(values = c(\"#DD5844\", \"#67B689\", \"#4383B4\")) +\n  labs(x = \"\", y = \"Median weekly income (USD$)\",\n       title = \"Comparison of earnings between racial groups\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme()\n\n\n\n\nWe can already clearly see a difference in income between racial groups at this very broad aggregation. Has this improved or worsened over the past decade? We can break our plot down into individual quarters to show how income in the three racial groups has changed.\n\nggplot(data = plot_data) +\n  geom_col(aes(x = time, y = median_weekly_earn, fill = race),\n           colour = \"black\") +\n  scale_fill_manual(values = c(\"#DD5844\", \"#67B689\", \"#4383B4\"), name = \"Race:\") +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  labs(x = \"\", y = \"Median weekly income (USD$)\",\n       title = \"Change in income over time\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme() +\n  #Legend would be useful here\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#digging-deeper-quantifying-inequality",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#digging-deeper-quantifying-inequality",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Digging deeper: Quantifying inequality",
    "text": "Digging deeper: Quantifying inequality\n\nIt looks like inequality has worsened over time, with earnings for those who identify as Asian appearing to increase more rapidly than Black or White. If we’re really interested in visualising this trend, we need some way to quantify the level of racial income inequality at any time point.\nA crude (but easy!) way we can do this is just to look at the proportional difference between the lowest and highest income group. If racial income inequality is small, we should get a value close to 1. When inequality is large, this number will be below 1 as the income from the lowest earning group will be only a fraction of the higher earning group.\n\n\n\n\n\n\nNote\n\n\n\nOur measure seeks to identify racial income inequality regardless of who the lowest and highest earning racial group is in any quarter. Although it doesn’t occur in this data, it’s possible that the lowest and/or highest earning group could change over time!\n\n\n\nplot_data_inequality &lt;- plot_data %&gt;% \n  #For every quarter, determine the ratio betwen lowest and highest earner\n  group_by(time) %&gt;% \n  summarise(income_ratio = min(median_weekly_earn)/max(median_weekly_earn))\n\n\nggplot(data = plot_data_inequality) +\n  geom_line(aes(x = time, y = income_ratio), colour = \"#193D58\", size = 1) +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(plot_data_inequality$time), y = 0.99, label = \"Income parity\"), size = 5) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  labs(x = \"\", y = \"Racial income inequality\",\n       title = \"Change in racial inequality over time\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Use of `plot_data_inequality$time` is discouraged.\nℹ Use `time` instead.\n\n\n\n\n\nIn 2010, our measure of racial inequality gives a value around 0.75, meaning that the lowest earning group (Black) earned on average only 75% of the highest earning group (Asian). By 2020, this number was closer to 65%, a change of 10 percentage points! It seems that racial inequality is getting worse, but is this the same for both sexes? To answer this we’ll have to go back to the original data and include separate male and female data rather than ‘Both Sexes’.\n\ninequality_by_sex &lt;- earn %&gt;% \n  filter(sex != \"Both Sexes\" & race != \"All Races\" & age == \"16 years and over\") %&gt;% \n  #Create a time values (combine year and quarter)\n  mutate(time = year + quarter/4) %&gt;% \n  #Racial income inequality at each time point for each sex\n  group_by(sex, time) %&gt;% \n  summarise(income_ratio = min(median_weekly_earn)/max(median_weekly_earn), .groups = \"drop\")\n\n\nggplot() +\n  geom_line(data = inequality_by_sex, aes(x = time, y = income_ratio, colour = sex), size = 0.75) +\n  labs(x = \"\", y = \"Racial income inequality\",\n       title = \"Change in racial inequality for men and women\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(inequality_by_sex$time), y = 0.99, label = \"Income parity\"), size = 5) +\n  scale_colour_manual(name = \"Sex\", values = c(\"#266742\", \"#3255A2\")) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  scale_y_continuous(limits = c(NA, 1)) +\n  my_theme() +\n  theme(legend.position = \"right\")\n\n\n\n\nInterestingly, we see that racial income inequality is less severe in women (although it has also become worse over time).\n\n\n\n\n\n\nNote\n\n\n\nThis doesn’t show that women earn more, just that the difference in income due to race is less for women!"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#the-finishing-touches-making-things-look-pretty",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#the-finishing-touches-making-things-look-pretty",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "The finishing touches: Making things look pretty",
    "text": "The finishing touches: Making things look pretty\n\nThis a really interesting result, and a good foundation for a TidyTuesday submission. Now we just need to make it prettier. One issue for somebody viewing this data is the large quarter to quarter variation in our inequality measure. We can perhaps visualise the trends a little better by creating a rolling mean of inequality over 3 quarters. We can also add custom fonts and more informative titles/subtitles to make the plot easier to read.\n\n#Create rolling mean data\nrolling_mean &lt;- inequality_by_sex %&gt;% \n  group_by(sex) %&gt;% \n  mutate(lag1 = lag(income_ratio),\n         lead1 = lead(income_ratio)) %&gt;% \n  rowwise() %&gt;% \n  mutate(rollingmean = mean(c(income_ratio, lag1, lead1)))\n\n#Add sex symbols at last point to remove the legend\ninequality_by_sex %&gt;% \n  group_by(sex) %&gt;% \n  slice(n()) %&gt;% \n  ungroup() %&gt;% \n  mutate(time = time + 0.5,\n         sex_symbol = c(\"\\u2642\", \"\\u2640\")) -&gt; sex_symbol\n\n\n#Load a font from Google Fonts\nsysfonts::font_add_google(\"Oswald\", \"Oswald\", regular.wt = 300)\nsysfonts::font_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\n#Specify that the showtext package should be used\n#for rendering text\nshowtext::showtext_auto()\n\nggplot() +\n  geom_line(data = inequality_by_sex, aes(x = time, y = income_ratio, group = sex),\n            colour = \"grey50\", size = 1, alpha = 0.5) +\n  geom_line(data = rolling_mean, aes(x = time, y = rollingmean, colour = sex), size = 2) +\n  labs(x = \"\", y = \"&lt;- Less equal                           More equal -&gt;\",\n       title = \"Racial income inequality in the USA\",\n       subtitle = \"Ratio between the income of the lowest and highest earning racial group\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(inequality_by_sex$time), y = 0.98, label = \"Income parity\"),\n            size = 7, family = \"Oswald\") +\n  geom_text(data = sex_symbol, aes(x = time, y = income_ratio, label = sex_symbol), size = 17) +\n  scale_colour_manual(name = \"Sex\", values = c(\"#266742\", \"#3255A2\")) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  scale_y_continuous(limits = c(NA, 1)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.title.y = element_text(face = \"bold\", vjust = 0.9, size = 18, family = \"Oswald\"),\n        axis.text = element_text(colour = \"black\", size = 12, family = \"Oswald\"),\n        plot.title = element_text(face = \"bold\", size = 32, family = \"Oswald\"),\n        plot.subtitle = element_text(size = 20, family = \"Oswald\"),\n        plot.caption = element_text(size = 10, family = \"Ubuntu Mono\"),\n        plot.margin = margin(30, 30, 30, 30))"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#conclusion",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#conclusion",
    "title": "TidyTuesday - Week 9 (2021)",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThis plot isn’t the most technically difficult, we’re just using basic geom_xxx() functions, but we used a bit of data wrangling to extract more information from some messy original data and present an interesting (and troubling) story."
  },
  {
    "objectID": "posts/2021-12-13-advent-of-code-day-8/index.html#the-data",
    "href": "posts/2021-12-13-advent-of-code-day-8/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\n\nDay 8 is really more a puzzle than a coding challenge. We’re given the inputs and outputs from a broken 7-segment display (like on a digital clock). We can imagine each segment of the clock as having a corresponding letter, from a (top) to g (bottom).\n\nWe can create a dictionary that will translate a string of letters into a corresponding number following the image above.\n\n#Actual letter number combos\ncorrect_codes &lt;- 0:9\nnames(correct_codes) &lt;- c(\"abcefg\", \"cf\", \"acdeg\", \"acdfg\", \"bcdf\", \"abdfg\", \"abdefg\", \"acf\", \"abcdefg\", \"abcdfg\")\n\nThe only problem is that the data we are given doesn’t have the correct letters! Each row of the input data we are given includes all 10 possible numbers (0-9) but the letters used are wrong. We have to work out a number of rules to decode this information! See the full explanation for today’s challenge here.\n\n#Load data\nday8_data &lt;- readr::read_delim(file = \"./data/Day8.txt\", delim = \" | \",\n                               col_names = c(\"inputs\", \"outputs\"), show_col_types = FALSE)\n\nhead(day8_data)\n\n# A tibble: 6 × 2\n  inputs                                                     outputs            \n  &lt;chr&gt;                                                      &lt;chr&gt;              \n1 fdceba bafdgc abeg afbdgec gbeacd abced bgc fcdge bg bedgc bafdec cgefd gcebd…\n2 gbfac fegbda fcedagb bea ea abcdef dgbfe gfabe dgea gbdfec gdea bgefdc bea ef…\n3 eg dagef gbcfeda ageb cegbfd gfe dbefa facdg abfged cedbaf befda daefb egf gc…\n4 edgacfb gcfd dgb degfab bcega bdagc cgafbd fbacd gd fceabd fbdac gd gdbcaf dgb\n5 eaf bedgaf dbafc bfceag fedcbg eafdcgb debfa ae adge gdebf abcdfeg febdg ae d…\n6 afbdc aefg ea edbacfg dbefg eab gcbfde abecgd bgefad bfdae gfea bfdea gbdef f…\n\n\nConvert our data into nested lists so that it’s easier to work with.\n\nday8_data &lt;- day8_data %&gt;% \n  mutate(inputs = stringr::str_split(inputs, pattern = \" \"),\n         outputs = stringr::str_split(outputs, pattern = \" \"))"
  },
  {
    "objectID": "posts/2021-12-13-advent-of-code-day-8/index.html#the-challenges",
    "href": "posts/2021-12-13-advent-of-code-day-8/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1 & 2\nIn combination, the two challenges from Day 8 aim to decode the outputs so we’ll combine them together. The first rules we can use are simple. There are 4 numbers that can be easily identified just by the number of characters in the input.\n\n2 characters = 1\n3 characters = 7\n4 characters = 4\n7 characters = 8\n\nSo if we see an input ‘ab’ this must correspond to the number 1 or ‘cf’ when correctly translated to the 7-segment display. But we don’t know if ‘a’ is ‘c’ or ‘f’. We need some more rules to work this out. We’ll apply all these rules below:\n\nnumbers &lt;- NULL\nfor (i in 1:nrow(day8_data)) {\n\n  #Split input into list with individual letters\n  split_input  &lt;- stringr::str_extract_all(day8_data$inputs[[i]], pattern = \"[a-z]{1}\")\n  split_output &lt;- stringr::str_extract_all(day8_data$outputs[[i]], pattern = \"[a-z]{1}\")\n\n  #If there are two values, it must correspond to c and f\n  #Remove from the list afterwards\n  number1 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 2))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 2))] &lt;- NULL\n\n  #If there are three values, it must correspond to a, c and f\n  #Remove from the list afterwards\n  number7 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 3))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 3))] &lt;- NULL\n\n  #If there are four values, it must correspond to b, c, d and f\n  #Remove from the list afterwards\n  number4 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 4))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 4))] &lt;- NULL\n\n  #If there are seven values, it must correspond to a-g\n  #Remove from the list afterwards\n  number8 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 7))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 7))] &lt;- NULL\n\n  #Letter that's not shared between 1 and 7 must be a\n  a &lt;- setdiff(number7, number1)\n\n  #Value of 6 characters that contains the letters in 4 must be 9.\n  #Remove from the list afterwards\n  number9 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number4 %in% letters)))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number4 %in% letters)))] &lt;- NULL\n  \n  #The missing letter in 9 is e\n  e &lt;- setdiff(number8, number9)\n\n  #Remaining 6 letter character that includes the same letters as number1 must be 0.\n  #Remove from the list afterwards\n  number0 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number1 %in% letters)))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number1 %in% letters)))] &lt;- NULL\n  \n  #The missing value here is d\n  d &lt;- setdiff(number8, number0)\n\n  #Last remaining length 6 is 6\n  number6 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6))] &lt;- NULL\n  \n  #Missing value is c\n  c &lt;- setdiff(number8, number6)\n\n  #The letter in 7 that wasn't just extracted is f\n  f &lt;- setdiff(number1, c)\n\n  #Letter in 4 that hasn't yet been deciphered in b\n  b &lt;- setdiff(number4, c(c, d, f))\n\n  #Remaining letter in 0 after removing all known ones is g\n  g &lt;- setdiff(number0, c(a, b, c, e, f))\n\n  #Dictionary\n  dict &lt;- letters[1:7]\n  names(dict) &lt;- c(a, b, c, d, e, f, g)\n\n  #Now we can decode our output\n  output &lt;- correct_codes[unlist(lapply(split_output, \\(letters) paste(dict[letters][order(dict[letters])], collapse = \"\")))]\n\n  #Paste all the decoded numbers together\n  numbers &lt;- append(numbers, as.integer(paste(output, collapse = \"\")))\n\n}\n\n\nnumbers\n\n  [1] 6233 4675 5572 5197 8513 4356  307 8507 4088 3756 9295 2569 5165 3061 2862\n [16] 1316 7879 2323 2593 5720 6695 7066 9013 9050 4501 2886 1326  734 9840 8003\n [31] 1752 4944 2038 9216 9250 4237 2145 7182 1101 5063 2136 8496 8523 3326 4598\n [46]  227 6537 3390 5065 9306 8460 6335 2664 6446 5550 4303 6955 3520 9376 2586\n [61] 6520 2121 6685 3109 2838 8257 3082 7739 7132 2770 5996  521 3470  940 2060\n [76] 6396  400 6050 2314 8464  262 9628 4719  706 8307 5355 4289 2422 2634  714\n [91] 2016 2778 6695  474 6970 3689 3305   12 5074 8060 5359 9331 8993 3307 9329\n[106]  458 9872 7089 6058 5030 3634 5702 3756 4506 2957 8995 5657 8375  415 2412\n[121] 3021 7241 8368 9298 6666 8567 2648 9621 3215 5533 4972 2236 8329 2582 2929\n[136] 2089 3436 3594 1383 5555 5461 2268 6377 1440 9494 8314 9483 3267 3037 5547\n[151] 9184 4900 6696 7304 1294 1349 8642 6126 7367 5525 3861 3393 2958 5312 2028\n[166] 5058 8513 4258 1780 3937 5568 6236 5159  518 8059 1014 1080 8538   40 6722\n[181] 3193 2519 5877 2460 3348 7329 8622 4380 3629  771 3734 6563 1900 4502 1472\n[196] 6293 7392 8285 9928 7535\n\n\nThe answer of our final challenge is the sum of all these values.\n\nsum(numbers)\n\n[1] 982158\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6\nDay 7"
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html",
    "title": "R 4.1.0",
    "section": "",
    "text": "It’s been talked about for awhile, but the next big changes to R, v.4.1.0, are here! For many R users, new versions don’t make a huge amount of difference to their day to day work. I often find people still happpily running R v3.x! This time around, however, v4.1.0 includes some major (and interesting) changes to the base R syntax, which might affect (improve?) people’s workflow. We’ll cover the most talked about changes below. Put on your party hat and lets get into it."
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html#the-native-pipe",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html#the-native-pipe",
    "title": "R 4.1.0",
    "section": "The native pipe",
    "text": "The native pipe\n\nEver since the rise of the tidyverse and magrittr I’ve noticed a worrying divergence in how people write R code. New users, or those that jumped on the tidyverse bandwagon, tended to write long piped code using dplyr functions. Older users, who learned R before the rise of the tidyverse, tended to use nested base functions to achieve the same result. Having a variety of tools to approach the same problem isn’t inherently bad, but it seemed like we were heading towards a world where the two groups were almost using different, incomprehensible, programming languages. Enter the native pipe: |&gt;. With v4.1.0 we now have an inbuilt pipe included in base R syntax. With |&gt; you can build piped code without ever needing to load a tidyverse library!\n\n#The tidyverse way\ntidy &lt;- iris %&gt;% \n  filter(Species == \"setosa\") %&gt;% \n  select(Species, Sepal.Length)\n\n#The base R way\nbase &lt;- iris[which(iris$Species == \"setosa\"), c(\"Species\", \"Sepal.Length\")]\n\n#The hybrid way\nhybrid &lt;- iris |&gt;\n  subset(Species == \"setosa\", select = c(\"Species\", \"Sepal.Length\"))\n\n#Same results...different inputs\nidentical(tidy, base)\n\n[1] TRUE\n\nidentical(base, hybrid)\n\n[1] TRUE\n\n\nThis is an awesome change for the base R syntax. I guess we can all forget about magrittr and move to using the native pipe, right? Well, not so fast. While the native pipe is a huge improvement, there are still a few features that might keep you using the more familiar %&gt;% (at least for now).\n\n\nFlexible placement\n\nmagrittr pipe allows the left-hand side to be used at any point in the following function with the placement of the .. In comparison, native pipe requires the left-hand side to always fill the first argument on the right.\n\nnames &lt;- c(\"John_Doe\", \"Jeff_Jones\", \"Rachel_Black\")\n\nnames %&gt;% \n  gsub(pattern = \"_\", x = ., replacement = \" \")\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\nWe can achieve the same thing in base R by writing a nested function, but it’s definitely not as effective and defeats some of the benefits of writing piped code (though see the changes to anonymous functions below.\n\nnames |&gt;\n  {function(x) gsub(pattern = \"_\", x = x, replacement = \" \")}()\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n\n\nShort-hand pipe functions\n\nA less well known, but sometimes useful, feature of the magrittr pipe is the ability to quickly turn a piece of piped code into a function. This functionality isn’t available with the native pipe…although that might not be a deal breaker for many people.\n\n#Create a function to run same pipe\nreplace_underscore &lt;- . %&gt;% \n  gsub(pattern = \"_\", x = ., replacement = \" \")\n\nreplace_underscore(names)\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n#The slightly messier base approach\nreplace_underscore &lt;- function(x){\n  \n  x |&gt;\n    {function(x) gsub(pattern = \"_\", x = x, replacement = \" \")}()\n  \n}\n\nreplace_underscore(names)\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n\n\nThe rest of the magrittr package\n\nUntil now, we’ve comparing %&gt;% and |&gt;, but if you’re actually loading the magrittr package there are a few other special pipes that are also powerful. One particularly useful one is the tee-pipe, %T&gt;%, which allows you to create an output in the middle of your pipe (e.g. plot or view your data) without needing to stop the pipe.\n\n#Return the sum of two normally distributed variables\ncol_sums &lt;- data.frame(x = rnorm(10), y = rnorm(10)) %T&gt;%\n  #View the data to make sure it all looks fine\n  print() %&gt;%\n  #Return the column sums\n  colSums()\n\n             x           y\n1   1.14361424 -0.50450397\n2  -0.39098475  0.08614385\n3   1.49968453  1.70784391\n4  -0.89958298  0.56469891\n5   1.18865975  0.34393890\n6   1.57441061  0.88022468\n7  -0.85532933 -0.50792175\n8   0.45638054 -0.28057388\n9   0.06339918 -0.48972224\n10 -0.33705409  1.51309608\n\n\n\ncol_sums\n\n       x        y \n3.443198 3.313224 \n\n\n\n\n\nDependencies\n\nLet’s not pretend that %&gt;% has things all its own way. One major benefit of the native pipe is that it means you can use pipe with no dependencies. Working with the tidyverse can often mean working with quite a few dependencies, and there’s no guarantee that updates will be backward compatible. If you’re building an R package, even just for personal use, having the option of writing piped code without needing additional package dependencies might be exactly what you’re looking for."
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html#anonymous-functions",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html#anonymous-functions",
    "title": "R 4.1.0",
    "section": "Anonymous functions",
    "text": "Anonymous functions\n\nAnother major syntax change that provides base R with capabilities previously available only in the tidyverse are changes to how anonymous functions are written. tidyverse gave the possibility to quickly write anonymous functions using ~. With v4.1.0, we can now easily create anonymous functions with the \\(x) syntax.\n\n#Tidy anon\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarise(across(.fns = ~mean(., na.rm = TRUE)))\n\n#New base anon\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(.fns = \\(x) mean(x, na.rm = TRUE)))\n\nUnlike our pipe comparison, the new base anonymous functions have a number of benefits over tidyverse equivalents.\n\n\nUse anywhere in R\n\nThe ~ syntax is useful within tidyverse but can’t easily be used outside of this context. \\(x), on the other hand, can be used to write functions anywhere you want.\n\n#This won't work!\n#tidy_func &lt;- {~mean(., na.rm = TRUE)}\n#tidy_func(c(1, 4, 7))\n\n#Works fine\nbase_func &lt;- \\(x)mean(x, na.rm = TRUE)\nbase_func(c(1, 4, 7))\n\n[1] 4\n\n\n\n\n\nMultiple arguments and argument names\n\nAnonymous functions in tidyverse allow for a single argument, but \\(x) allows for any number of arguments with any name you feel like!\n\nthree_arguments &lt;- \\(a, b, c)a*b/c\n\nthree_arguments(1.5, 4, 3)\n\n[1] 2\n\n\nSo, while I might be holding off on using native pipe, I think the new base R anonymous functions will quickly become part of my coding workflow!"
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html#concatenating-factors",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html#concatenating-factors",
    "title": "R 4.1.0",
    "section": "Concatenating factors",
    "text": "Concatenating factors\n\nOne smaller change that might be useful to regular R users is the new ability to concatenate factors together. Previously, trying to concatenate different factors would coerce all levels into their underlying integers (how the data are actually stored within R). With v4.1.0, factors can be concatenated and stay as factors, combining together all the levels from the two original factors.\n\nfactor1 &lt;- factor(c(\"Apple\", \"Orange\", \"Apple\"), levels = c(\"Apple\", \"Orange\"))\nfactor2 &lt;- factor(c(\"Banana\", \"Strawberry\"), levels = c(\"Banana\", \"Strawberry\"))\n\nc(factor1, factor2)\n\n[1] Apple      Orange     Apple      Banana     Strawberry\nLevels: Apple Orange Banana Strawberry\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of the factor levels in the new factor are dependent on the order in which the two factors are concatenated together.\n\n\n\n#Levels start with Apple\nc(factor1, factor2)\n\n[1] Apple      Orange     Apple      Banana     Strawberry\nLevels: Apple Orange Banana Strawberry\n\n#Levels start with Banana\nc(factor2, factor1)\n\n[1] Banana     Strawberry Apple      Orange     Apple     \nLevels: Banana Strawberry Apple Orange"
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html#wrap-up",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html#wrap-up",
    "title": "R 4.1.0",
    "section": "Wrap up",
    "text": "Wrap up\n\nR v4.1.0 marks an exciting step in R development. The introduction of native pipe and easy anonymous functions will allow users to take advantage of these useful tools even if they’re more comfortable in base R than the tidyverse. While I might not be switching over to using native pipe just yet, I’m really excited to see where this leads in future updates!"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html",
    "title": "Missing data",
    "section": "",
    "text": "A common problem we encounter when analysing data is the presence of missing data, NAs. How do we deal with NAs when we encounter them? In many cases we have to accept that this information is missing; however, in time series where data are strongly auto-correlated we have a chance to estimate (of ‘impute’) these missing data using other data points where data were recorded. In this blog, I’ll look through different methods available in R that might be considered for imputing missing data in time series."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#introduction",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#introduction",
    "title": "Missing data",
    "section": "",
    "text": "A common problem we encounter when analysing data is the presence of missing data, NAs. How do we deal with NAs when we encounter them? In many cases we have to accept that this information is missing; however, in time series where data are strongly auto-correlated we have a chance to estimate (of ‘impute’) these missing data using other data points where data were recorded. In this blog, I’ll look through different methods available in R that might be considered for imputing missing data in time series."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#preparing-our-workspace",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#preparing-our-workspace",
    "title": "Missing data",
    "section": "Preparing our workspace",
    "text": "Preparing our workspace\n\n\nPackages\n\nBelow are all the packages that we’ll use in this example. Most are the standard data science packages in R, but notice the inclusion of {imputeTS}, which is a tool specifically designed to deal with NAs in a time series.\n\nlibrary(arrow) #To (quickly) read csv files\nlibrary(dplyr) #For data wrangling\nlibrary(tidyr) #Pivoting data\nlibrary(ggplot2) #For plotting\nlibrary(imputeTS) #To impute data in data pipelines\n\n\n\nOur example data\n\nWe’ll focus on a time series of temperature data from Melbourne, Australia (Source: Australian Bureau of Meteorology). This is actually a complete time series with no missing data, but we’ll generate 1000 NAs in the time series at random.\n\nfull_data &lt;- arrow::read_csv_arrow(file = \"data/melbourne_temp.csv\") %&gt;% \n  mutate(date = lubridate::ymd(paste(Year, Month, Day)),\n         maxT_missing = maxT)\n\n#Generate some NAs in the data\n#We ensure that the last row can't become NA so that linear interpolation is always possible\nset.seed(1234) ## Set seed to make it repeatable\nfull_data$maxT_missing[sample(1:(nrow(full_data) - 1), size = 1000)] &lt;- NA"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#step-1-visualize-the-missing-data",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#step-1-visualize-the-missing-data",
    "title": "Missing data",
    "section": "Step 1: Visualize the missing data",
    "text": "Step 1: Visualize the missing data\n\nThe first step to any analysis should be to inspect and visualize your data. Dealing with NAs is no different. If we know there are NAs in our time series we need to see when they occur and how often. {imputeTS} includes a number of in-built plotting functions to visualize NAs (e.g. ?imputeTS::ggplot_na_distribution) but here I’ve used {ggplot2} to make a set of custom plots.\nFirst, we can look at which year the NAs occur in.\n\nplot_data &lt;- full_data %&gt;% \n  group_by(Year) %&gt;% \n  summarise(perc_NA = sum(is.na(maxT_missing))/n() * 100)\n\n#The limit of the y-axis will be the nearest decile above the data\nyaxis_lim &lt;- (max(plot_data$perc_NA) %/% 10)*10 + 10\n\nggplot(data = plot_data) +\n  geom_col(aes(x = Year, y = perc_NA),\n           fill = \"indianred\", colour = \"black\",\n           linewidth = 0.25) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(breaks = seq(0, 100, 5),\n                     labels = paste0(seq(0, 100, 5), \"%\")) +\n  coord_cartesian(ylim = c(0, yaxis_lim)) +\n  labs(title = \"Missing Values per Year\",\n       subtitle = \"Percentage of missing data in each year of the time-series\",\n       x = \"Year\", y = \"Percentage of records\") +\n  theme_classic() +\n  theme(legend.title = element_blank(),panel.grid.major.y = element_line(colour = \"grey75\", linewidth = 0.25))\n\n\n\n\nPercentage of missing data in each year of the time-series\n\n\n\n\nWe can also look at the occurrence of NA values at the scale of days.\n\nggplot() +\n  geom_line(data = full_data, aes(x = date, y = maxT_missing), colour = \"steelblue2\") +\n  geom_vline(data = filter(full_data, is.na(maxT_missing)), aes(xintercept = date),\n             colour = \"indianred\", alpha = 0.5) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Location of missing values\",\n       subtitle = \"Time series with highlighted missing regions\",\n       y = \"Temperature (C)\",\n       x = \"\") +\n  theme_classic()\n\n\n\n\nEach red line in the plot above is an NA in our time series. With 1000 NAs and a long time-series this is impossible to read. To visualise this more clearly, we can create the same graph but only for the year 2022.\n\nfull_data %&gt;% \n  filter(Year == 2022) %&gt;% \n  {ggplot() +\n      geom_line(data = ., aes(x = date, y = maxT_missing), colour = \"steelblue2\") +\n      geom_vline(data = filter(., is.na(maxT_missing)), aes(xintercept = date),\n                 colour = \"indianred\", alpha = 0.5) +\n      scale_x_date(expand = c(0, 0)) +\n      scale_y_continuous(expand = c(0, 0)) +\n      labs(title = \"Location of missing values\",\n           subtitle = \"Time series with highlighted missing regions\",\n           y = \"Temperature (C)\",\n           x = \"\") +\n      theme_classic()}\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThe inbuilt function ggplot_na_gapsize() from {imputeTS} is handy to see how often NAs occur consecutively. We can see that most NAs occur alone.\n\nggplot_na_gapsize(full_data$maxT_missing)\n\n\n\n\nSo we now know where the NAs are. This is important because it tells us if there is anything systematic about our missing data (e.g. do they all occur before a certain date? Do they occur in consecutive chunks?). Structure or patterns in missing data might be a sign of some underlying drivers of NAs that would need to be investigated further. In our case, the NAs are distributed randomly (we know this because we created them!). Now we need to work out what to do with them."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#the-basic-method-deletion",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#the-basic-method-deletion",
    "title": "Missing data",
    "section": "The basic method: Deletion",
    "text": "The basic method: Deletion\n\nThe most obvious solution when encountering NAs is to simply delete them. This method is perfectly appropraite in many cases, but it can raise 2 key issues:\n\nBias: If missing data are biased in some way, removing missing data will create bias in analyses. e.g. if temperature sensors tend to fail more at higher temperatures, removing NA values will bias our analysis towards lower temperatures.\nReduced statistical power: If we discard all records with any missing data, we may end up removing a lot of other useful information from covariates that were recorded at the same time (e.g. if we measured temperature and precipitation simultaneously). This will give us lower statistical power to answer our questions.\n\nSo, what is the alternative? Instead of deleting NAs from a time series, we will try and impute the missing values based on other points where data are available."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-1-global-mean-substitution",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-1-global-mean-substitution",
    "title": "Missing data",
    "section": "❌ Method 1: Global mean substitution ❌",
    "text": "❌ Method 1: Global mean substitution ❌\n\nThe simplest approach that is sometimes proposed is to replace missing data with a global mean.\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_meansub = case_when(is.na(maxT_missing) ~ mean(maxT_missing, na.rm = TRUE),\n                                  TRUE ~ maxT_missing))\n\nSeems easy enough, but this raises a host of serious issues. Firstly, by replacing NAs with the global mean we artificially reduce the uncertainty when we try to estimate population statistics.\n\n## The mean and SE of temperature from the real data\nfullmean &lt;- mean(full_data$maxT)\nfullSE   &lt;- sd(full_data$maxT)/sqrt(nrow(full_data))\nprint(paste(\"Full sample: Mean: \", round(fullmean, 4), \" SE:\", round(fullSE, 4)))\n\n[1] \"Full sample: Mean:  19.9204  SE: 0.0483\"\n\n\n\n## The mean and SE of temperature data with global mean substitution\n## SE is (incorrectly) reduced.\nimputedmean &lt;- mean(full_data$maxT_meansub)\nimputedSE   &lt;- sd(full_data$maxT_meansub)/sqrt(nrow(full_data))\nprint(paste(\"With global mean substitution: Mean: \", round(imputedmean, 4), \" SE:\", round(imputedSE, 4)))\n\n[1] \"With global mean substitution: Mean:  19.8958  SE: 0.0469\"\n\n\nIn the example above, we can see that the standard error of our temperature estimate is (incorrectly) reduced when we replace NAs with the global mean.\nWe will also create bias if we want to use our variable as a response in statistical analysis, like linear regression. We will tend to under-estimate the relationship between our predictor and response variable.\nAn extreme example:\n\n## Create data with a known relationship between x and y\neg_lmdata &lt;- tibble(x = 1:100) %&gt;% \n  mutate(y = x*0.5 + rnorm(n = 100))\n\n## Remove some data at random\neg_lmdata$y[sample(x = 1:nrow(eg_lmdata), size = 40)] &lt;- NA\n\n## Use global mean imputation\neg_lmdata &lt;- eg_lmdata %&gt;% \n  mutate(y_fill = case_when(is.na(y) ~ mean(y, na.rm = TRUE),\n                            TRUE ~ y))\n\n## Fit a model with the true data\nlm(y ~ x, data = eg_lmdata) %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ x, data = eg_lmdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6492 -0.6466  0.1199  0.5332  2.5088 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.366144   0.288255   -1.27    0.209    \nx            0.505625   0.005236   96.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.086 on 58 degrees of freedom\n  (40 observations deleted due to missingness)\nMultiple R-squared:  0.9938,    Adjusted R-squared:  0.9937 \nF-statistic:  9324 on 1 and 58 DF,  p-value: &lt; 2.2e-16\n\n\n\n## Fit a model with the data with NA replacement\nlm(y_fill ~ x, data = eg_lmdata) %&gt;% summary()\n\n\nCall:\nlm(formula = y_fill ~ x, data = eg_lmdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.9176  -6.2557   0.4168   6.0790  13.2589 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.77587    1.49423   7.212 1.17e-10 ***\nx            0.26096    0.02569  10.159  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.415 on 98 degrees of freedom\nMultiple R-squared:  0.5129,    Adjusted R-squared:  0.508 \nF-statistic: 103.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nAlthough we know the relationship between x and y is 0.5, we heavily underestimate this when we replace NAs with the global mean.\nWe can see the problem with global mean substitution quite clearly if we compare the imputed values to the true values that we know are behind the NAs. In our temperature time series, we can see that global mean substitution tends to underestimate true temperature.\n\n\nShow the code\nfilter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(diff = maxT - maxT_meansub) %&gt;%\n  {ggplot(.) +\n      geom_density(aes(x = diff), colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_y_continuous(expand = c(0, 0)) +\n      theme_classic() +\n      theme(legend.title = element_blank(),legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-local-mean-substitution",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-local-mean-substitution",
    "title": "Missing data",
    "section": "Method 2: Local mean substitution",
    "text": "Method 2: Local mean substitution\n\nSo, replacing every NA with a global mean is not acceptable. The next method we might consider is to estimate a local mean from the two nearest known values.\nLocal mean substitution can already be achieved with the approx() function in the base {stats} package; however, this does not work nicely if we want to code using pipes. As an alternative, we can use the na_interpolation() function from the package {imputeTS}, which has a wrapper around basic {stats} functions (e.g. approx(), spline()) as well as other more complex imputation functions from other packages that we’ll discuss below.\nHere’s what this looks like:\n\n# Use the imputeTS package\nimputeTS::na_interpolation(x = c(1, NA, 5),\n                           ## We need to specify the 'f' argument at 0.5\n                           ## to weight each point equally.\n                           method = \"constant\", f = 0.5)\n\n[1] 1 3 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nChunks of consecutive NAs are all given the same mean value.\n\n\n\nimputeTS::na_interpolation(x = c(1, NA, NA, 5), method = \"constant\", f = 0.5)\n\n[1] 1 3 3 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnly the nearest values are considered.\n\n\n\nimputeTS::na_interpolation(x = c(10, 1, NA, NA, 5, 1), method = \"constant\", f = 0.5)\n\n[1] 10  1  3  3  5  1\n\n\nWe can then use this function within a call the mutate() to easily create a new imputed column.\n\n### Replace missing values using \nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_localmean = imputeTS::na_interpolation(x = maxT_missing, method = \"constant\", f = 0.5))\n\nIf we compare this local mean substitution to our results from global mean substitution, we can see that things have greatly improved. The difference between our imputed values and true values appears to be evenly distributed around 0, suggesting that this method is not systematically over- or under-estimating.\n\n\nShow the code\nplot_data &lt;- filter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(globalmean = maxT - maxT_meansub,\n         localmean = maxT - maxT_localmean)\n\n  ggplot(plot_data) +\n      geom_density(aes(x = globalmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = localmean, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_y_continuous(expand = c(0, 0)) +\n      scale_colour_manual(values = c(\"red\", \"grey50\")) +\n      theme_classic() +\n      theme(legend.title = element_blank(),axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-3-weighted-moving-average",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-3-weighted-moving-average",
    "title": "Missing data",
    "section": "Method 3: Weighted moving average",
    "text": "Method 3: Weighted moving average\n\nBy using the two closest points to impute a local mean we can substantial improveme upon the untenable method of global mean substitution; however, in many cases we can also gain information from other data points, not just the nearest neighbours. Of course, as we move further from the NA we will gain less and less information, so we can weight the information based on the distance from our missing data. This leads us to the weighted moving average method that we can employ with the na_ma() function from {imputeTS}.\nna_ma() has two key arguments that you need to understand:\n\nk: Size of the window over which to calculate the moving average. This is the number of time steps to either side of the NA that will be used.So, for an NA at position \\(i\\) and \\(k = 2\\), the weighted moving averaging would use observations at \\(i-2\\), \\(i-1\\), \\(i+1\\), and \\(i+2\\).\nweighting: The method used to weight data. This can include:\n\nsimple: All values are equally weighted.\nlinear: Weight decreases linearly with distance from \\(i\\). If \\(x\\) is the distance from \\(i\\), then the value has a weight \\(1/(1 + x)\\), such that the nearest values have weight of 1/2, 1/3, 1/4 etc.\nexponential: Weight decreases exponentially. If \\(x\\) is the distance from \\(i\\), then value has a weight \\(1/2^x\\) (default).\n\n\nIf k=1, we will get the same result as na_interpolation().\n\n#Results are the same with each function.\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 1)\n\n[1]  4  3  1  2  3  4 10\n\nimputeTS::na_interpolation(x = c(4, 3, 1, NA, 3, 4, 10), option = \"linear\", method = \"constant\", f = 0.5)\n\n[1]  4  3  1  2  3  4 10\n\n\nIf we expand the window (e.g. k=3), we include more information. Our result is no longer the same as using na_interpolation().\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"simple\")\n\n[1]  4.000000  3.000000  1.000000  4.166667  3.000000  4.000000 10.000000\n\n\nUsing equal weights for all points is not ideal, so we can try using alternative weighting methods. In our simple example, using linear or exponential weighting minimizes the influence of large values further away from NA.\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"linear\")\n\n[1]  4.000000  3.000000  1.000000  3.615385  3.000000  4.000000 10.000000\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"exponential\")\n\n[1]  4.000000  3.000000  1.000000  3.142857  3.000000  4.000000 10.000000\n\n\n\n\n\n\n\n\nNote\n\n\n\nMoving average will give different values to consecutive NAs.\n\n\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, NA, 3, 4, 10), k = 3, weighting = \"simple\")\n\n[1]  4.0  3.0  1.0  3.0  4.2  3.0  4.0 10.0\n\n\nTo compare the benefit of different weighting methods we will use all three. For now, we just use default window size k = 4 (i.e. using 8 values in total towards mean).\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_mas = imputeTS::na_ma(x = maxT_missing, weighting = \"simple\"),\n         maxT_mal = imputeTS::na_ma(x = maxT_missing, weighting = \"linear\"),\n         maxT_mae = imputeTS::na_ma(x = maxT_missing, weighting = \"exponential\"))\n\nNow we can compare all our mean substitution methods: global, local, moving average (equal weight), moving average (linear weight), moving average (exponential weight). All methods, other than global mean, seem to perform reasonably well.\n\n\nShow the code\nplot_data &lt;- filter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(globalmean = maxT - maxT_meansub,\n         localmean = maxT - maxT_localmean,\n         movingavg_simple = maxT - maxT_mas,\n         movingavg_linear = maxT - maxT_mal,\n         movingavg_exp = maxT - maxT_mae)\n\n  ggplot(plot_data) +\n      geom_density(aes(x = globalmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = localmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_simple, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_linear, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_exp, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_colour_manual(values = c(\"red\", \"grey50\")) +\n      scale_y_continuous(expand = c(0, 0)) +\n      theme_classic() +\n      theme(legend.title = element_blank(),\n            axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-4-linear-imputation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-4-linear-imputation",
    "title": "Missing data",
    "section": "Method 4: Linear imputation",
    "text": "Method 4: Linear imputation\n\nSo far we’ve focussed on different forms of mean substitution, but there are still other methods that we might consider. Linear imputation is one of the more common alternatives. Like our basic local mean substitution, linear imputation uses information from the two nearest known points, but instead of taking the mean of these two points it imputes the missing value(s) assuming there is a linear trend in our variable over time.\nTo do this, we are going back to the na_interpolation() function, but this time we use the argument option=‘linear’. If we have a single NA value, linear imputation and local mean substitution will produce the same result.\n\n# Assume a linear relationship of x over time,\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"linear\")\n\n[1] 1 2 3\n\n# Take the mean of the two closest points\nimputeTS::na_interpolation(x = c(1, NA, 3), method = \"constant\", f = 0.5)\n\n[1] 1 2 3\n\n\nUnlike local mean substitution, chunks of NAs are given different values.\n\n## If there is more than one NA, it will impute from a single linear relationship fitted between the two nearest known values.\nimputeTS::na_interpolation(x = c(1, NA, NA, 4), option = \"linear\")\n\n[1] 1 2 3 4\n\n\nWe saw above that most of the NAs in our data are non-consecutive, so our linear imputation method will be very similar to local mean substitution in this case.\n\n## Use linear imputation on our missing data\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_linearinterp = imputeTS::na_interpolation(x = maxT_missing, option = \"linear\"))\n\n\n\nShow the code\nplot_data &lt;- filter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(globalmean = maxT - maxT_meansub,\n         localmean = maxT - maxT_localmean,\n         movingavg_simple = maxT - maxT_mas,\n         movingavg_linear = maxT - maxT_mal,\n         movingavg_exp = maxT - maxT_mae,\n         linearinterp = maxT - maxT_linearinterp)\n\n  ggplot(plot_data) +\n      geom_density(aes(x = globalmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = localmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_simple, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_linear, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_exp, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = linearinterp, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_colour_manual(values = c(\"red\", \"grey50\")) +\n      scale_y_continuous(expand = c(0, 0)) +\n      theme_classic() +\n      theme(legend.title = element_blank(),axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-spline-interpolation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-spline-interpolation",
    "title": "Missing data",
    "section": "Method 5: Spline interpolation",
    "text": "Method 5: Spline interpolation\n\nIf we want to relax the assumption of linear change over time, we can instead use spline interpolation. Spline interpolation, as the name suggests, imputes missing values by fitting a spline through nearby known points. Unlike linear interpolation, spline interpolation uses more data either side of missing value, making it potentially more powerful.\nTo achieve spline interpolation we still use the na_interpolation() function, but now specify option = “spline” instead of “linear”.\n\n# Fit spline through known data to estimate NA\nimputeTS::na_interpolation(x = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1), option = \"spline\")\n\n[1]  3.0000000  2.5000000  2.0000000  1.0000000  0.8142007  0.7500000  0.0000000\n[8] -0.5000000 -1.0000000\n\n\nThere are a range of different spline functions that might have different success. We can pick the spline method used with the ‘method’ argument. There are 5 types of splines available. We can see the different spline fits below.\n\n#Demonstrate how it works\ntest_df &lt;- tibble(x = 1:9,\n                  y = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1))\n\nfmm_func &lt;- splinefun(x = test_df$y, method = \"fmm\")\nperiodic_func &lt;- splinefun(x = test_df$y, method = \"periodic\")\n\nWarning in splinefun(x = test_df$y, method = \"periodic\"): spline: first and\nlast y values differ - using y[1L] for both\n\nnatural_func &lt;- splinefun(x = test_df$y, method = \"natural\")\nmonoH.FC_func &lt;- splinefun(x = test_df$y, method = \"monoH.FC\")\nhyman_func &lt;- splinefun(x = test_df$y, method = \"hyman\")\n\n\n\nShow the code\nspline_df &lt;- tibble(x = seq(1, 9, 0.1)) %&gt;% \n  mutate(fmm = fmm_func(x = x),\n         periodic = periodic_func(x = x),\n         natural = natural_func(x = x),\n         monoH.FC = monoH.FC_func(x = x),\n         hyman = hyman_func(x = x)) %&gt;% \n  tidyr::pivot_longer(cols = fmm:hyman, names_to = \"spline\", values_to = \"y\")\n\nggplot()+\n  geom_point(data = test_df, aes(x = x, y = y)) +\n  geom_line(data = spline_df, aes(x = x, y = y, lty = spline)) +\n  theme_classic()\n\n\n\n\n\nFor our temperature data, we focus on three methods only (‘fmm’, ‘periodic’, and ‘natural’). The two other available methods (‘monoH’ and ‘hyman’) assume monotonic data (i.e. data should either never increase or decrease) and so are not appropriate for temperature data.\n\n## Fit all different splines that we can compare\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_fmm = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"fmm\"),\n         maxT_periodic = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"periodic\"),\n         maxT_natural = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"natural\")\n  )\n\n\n\nShow the code\nplot_data &lt;- filter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(globalmean = maxT - maxT_meansub,\n         localmean = maxT - maxT_localmean,\n         movingavg_simple = maxT - maxT_mas,\n         movingavg_linear = maxT - maxT_mal,\n         movingavg_exp = maxT - maxT_mae,\n         linearinterp = maxT - maxT_linearinterp,\n         splinefmm = maxT - maxT_fmm,\n         splineperiodic = maxT - maxT_periodic,\n         splinenatural = maxT - maxT_natural)\n\n  ggplot(plot_data) +\n      geom_density(aes(x = globalmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = localmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_simple, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_linear, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_exp, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = linearinterp, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splinefmm, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splineperiodic, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splinenatural, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_colour_manual(values = c(\"red\", \"grey50\")) +\n      scale_y_continuous(expand = c(0, 0)) +\n      theme_classic() +\n      theme(legend.title = element_blank(),axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-stineman-imputation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-stineman-imputation",
    "title": "Missing data",
    "section": "Method 5: Stineman imputation",
    "text": "Method 5: Stineman imputation\n\nThe final method we can try is the Stineman imputation method. The algorithm of Stineman (1980) is designed specifically for data imputation. We won’t go into the algorithm details, but Russell Stineman state that “The complete assurance that the procedure will never generate ‘wild’ points makes it attractive as a general purpose procedure”.\n\n#Here essentially still linear\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"stine\")\n\n[1] 1 2 3\n\n\n\nimputeTS::na_interpolation(x = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1), option = \"stine\")\n\n[1]  3.000  2.500  2.000  1.000  0.875  0.750  0.000 -0.500 -1.000\n\n\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_stine = imputeTS::na_interpolation(x = maxT_missing, option = \"stine\"))\n\n\n\nShow the code\nplot_data &lt;- filter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(globalmean = maxT - maxT_meansub,\n         localmean = maxT - maxT_localmean,\n         movingavg_simple = maxT - maxT_mas,\n         movingavg_linear = maxT - maxT_mal,\n         movingavg_exp = maxT - maxT_mae,\n         linearinterp = maxT - maxT_linearinterp,\n         splinefmm = maxT - maxT_fmm,\n         splineperiodic = maxT - maxT_periodic,\n         splinenatural = maxT - maxT_natural,\n         stinemethod = maxT - maxT_stine)\n\n  ggplot(plot_data) +\n      geom_density(aes(x = globalmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = localmean, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_simple, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_linear, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = movingavg_exp, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = linearinterp, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splinefmm, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splineperiodic, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = splinenatural, colour = \"Previous methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_density(aes(x = stinemethod, colour = \"New methods\"),\n      fill = NA, linewidth = 1, alpha = 0.5) +\n      geom_vline(xintercept = 0, lty = 2) +\n      labs(x = \"Difference between imputed and true value\") +\n      scale_x_continuous(limits = c(-30, 30),\n      breaks = seq(-30, 30, 10)) +\n      scale_colour_manual(values = c(\"red\", \"grey50\")) +\n      scale_y_continuous(expand = c(0, 0)) +\n      theme_classic() +\n      theme(legend.title = element_blank(),axis.text.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank(),\n            axis.text.x = element_text(size = 15),\n            axis.title.x = element_text(size = 17),\n            plot.margin = margin(10, 10, 10, 10))"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#overview-comparing-different-techniques",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#overview-comparing-different-techniques",
    "title": "Missing data",
    "section": "Overview: Comparing different techniques",
    "text": "Overview: Comparing different techniques\n\nUntil now we’ve used a rough comparison of real and imputed temperature data to compare methods. This is good enough to spot bad methods (like global mean substitution), but it doesn’t allow for a quantitative comparison of methods. For a more refined comparison we can use metrics such as mean squared error (MSE) and root mean squared error (RMSE). These two metrics are slightly different, but in both cases we want to minimize error. RMSE benefits from being in the same units as our variable (degrees centigrade in our case). MSE is not in true units, but is more sensitive to outliers.\n\n\nShow the code\nvalidation_stats &lt;- full_data %&gt;%\n  filter(is.na(maxT_missing)) %&gt;%\n  ## Filter only those cases where data are m\n  tidyr::pivot_longer(cols = maxT_meansub:maxT_stine, values_to = \"imputed\", names_to = \"method\") %&gt;%\n  mutate(sqdiff = (imputed - maxT)^2) %&gt;%\n  group_by(method) %&gt;%\n  summarise(MSE = sum(sqdiff)/n(),\n            RMSE = sqrt(MSE)) %&gt;%\n  arrange(RMSE)\n\nfull_names &lt;- data.frame(method = c(\"maxT_stine\", \"maxT_linearinterp\", \"maxT_localmean\", \"maxT_periodic\", \"maxT_fmm\", \"maxT_natural\", \"maxT_mae\", \"maxT_mal\", \"maxT_mas\", \"maxT_meansub\"),\n                         name = c(\"Stineman\", \"Linear imputation\", \"Local mean\", \"Spline (periodic)\", \"Spline (fmm)\", \"Spline (Natural)\", \"Weighted mean (exponential)\", \"Weighted mean (linear)\", \"Weighted mean (Equal weight)\", \"Global mean\"))\n\nvalidation_stats %&gt;%\n  left_join(full_names, by = \"method\") %&gt;%\n  select(name, MSE, RMSE)\n\n\n# A tibble: 10 × 3\n   name                           MSE  RMSE\n   &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt;\n 1 Stineman                      12.0  3.46\n 2 Linear imputation             12.1  3.48\n 3 Local mean                    12.4  3.52\n 4 Spline (periodic)             14.4  3.80\n 5 Spline (fmm)                  14.4  3.80\n 6 Spline (Natural)              14.4  3.80\n 7 Weighted mean (exponential)   14.5  3.80\n 8 Weighted mean (linear)        16.2  4.02\n 9 Weighted mean (Equal weight)  18.5  4.30\n10 Global mean                   42.3  6.50\n\n\nAs expected, global mean substitution is noticably worse than all other methods. It’s particularly bad in terms of MSE, as it can produce very large outliers. At the other end, our Stineman method comes out on top, but it is almost indistinguishable from results with much simpler linear imputation or local mean substitution. For our temperature data, any of these methods would likely be acceptable.\n\n\n\n\n\n\nCareful\n\n\n\nJust because Stineman and linear imputation methods were best here, does not mean they are best for all time series. This will depend on the type of data and the structure of temporal auto-correlation. You should test different methods for your particular time series by imputing values where the true value is known, as we did here."
  },
  {
    "objectID": "posts/2021-12-08-advent-of-code-day-5/index.html#the-data",
    "href": "posts/2021-12-08-advent-of-code-day-5/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nIt’s Day 5 and we’re 20% of the way through the advent challenge! See the explanation for today’s challenge here. Today’s challenges will again involve working with indexing numbers in The Matrix a matrix.\n\nWe’re given two sets of 2-dimensional coordinates (X,Y), which represent the start and end of a line. We then need to count the number of points at which at least two lines overlap. The data includes an unusual separator (-&gt;), so we’ll use read_delim() from the readr package to read in the data. I find the readr functions more powerful than base functions for reading data because they allow for more complex separators. See the example below with base function read.delim that cannot use a separator larger than 1 byte.\n\nread.delim(\"./data/Day5.txt\", sep = \" -&gt; \")\n\nError in scan(file, what = \"\", sep = sep, quote = quote, nlines = 1, quiet = TRUE, : invalid 'sep' value: must be one byte\n\n\n\nlibrary(readr)\n\n#Read in data where each set of coordinates is a character string\nraw_data &lt;- readr::read_delim(\"./data/Day5.txt\",\n                              delim = \" -&gt; \", col_names = c(\"start\", \"end\"),\n                              show_col_types = FALSE,\n                              col_types = list(start = readr::col_character(),\n                                               end = readr::col_character()))\n\nhead(raw_data)\n\n# A tibble: 6 × 2\n  start   end    \n  &lt;chr&gt;   &lt;chr&gt;  \n1 503,977 843,637\n2 437,518 437,225\n3 269,250 625,250\n4 846,751 646,751\n5 18,731  402,731\n6 749,923 749,986\n\n\nWe need to be able to access each of the X and Y values separately, so we’ll separate this data out into 4 columns.\n\nlibrary(stringr)\n\n#Convert the characters into a 4col numeric matrix\nstart_point &lt;- str_split(raw_data$start, pattern = \",\", simplify = TRUE)\nend_point   &lt;- str_split(raw_data$end, pattern = \",\", simplify = TRUE)\nall_points  &lt;- cbind(start_point, end_point)\nall_points  &lt;- as.numeric(all_points)\ndim(all_points) &lt;- c(nrow(raw_data), 4)\n\nhead(all_points)\n\n     [,1] [,2] [,3] [,4]\n[1,]  503  977  843  637\n[2,]  437  518  437  225\n[3,]  269  250  625  250\n[4,]  846  751  646  751\n[5,]   18  731  402  731\n[6,]  749  923  749  986"
  },
  {
    "objectID": "posts/2021-12-08-advent-of-code-day-5/index.html#the-challenges",
    "href": "posts/2021-12-08-advent-of-code-day-5/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor challenge 1 we just focus on horizontal or vertical lines. Just like in our bingo challenge on day 4, we will create an empty matrix (all 0s) on which to map our results. Our matrix is 1000 x 1000 to accomodate all the possible coordinates in our data.\n\nzero_mat &lt;- matrix(0, nrow = 1000, ncol = 1000)\n\n#Look at a section of the matrix\nzero_mat[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    0    0    0    0    0    0    0     0\n [2,]    0    0    0    0    0    0    0    0    0     0\n [3,]    0    0    0    0    0    0    0    0    0     0\n [4,]    0    0    0    0    0    0    0    0    0     0\n [5,]    0    0    0    0    0    0    0    0    0     0\n [6,]    0    0    0    0    0    0    0    0    0     0\n [7,]    0    0    0    0    0    0    0    0    0     0\n [8,]    0    0    0    0    0    0    0    0    0     0\n [9,]    0    0    0    0    0    0    0    0    0     0\n[10,]    0    0    0    0    0    0    0    0    0     0\n\n\nAs we’re just focussing on the horizontal or diagonal lines, we filter only those cases where x or y are constant.\n\n#Identify only horizontal or vertical lines\nnondiag &lt;- all_points[apply(all_points, MARGIN = 1, FUN = function(x){\n\n  x[1] == x[3] | x[2] == x[4]\n\n}), ]\n\nNow, adding lines to our matrix is fairly straight forward but we need to deal with two small issues before we do. First, the coordinates we’re given start at 0,0; however, in R indexing begins at 1 (a shocking fact for people familiar with other programming languages!). This means we’ll have to add 1 to all our coordinates so they can be properly used for indexing.\nThe second hurdle is that our coordinates are provided as X,Y, but to index a matrix in R we need to provide coordinates as Y (i.e. rows), X (i.e. columns). In the for loop below I create a sequence of coordinate to map the horizontal and vertical lines and then reverse their order to be used as our index values.\n\nfor (i in 1:nrow(nondiag)) {\n\n  line &lt;- nondiag[i, ]\n  \n  #Make sequence of first coordinates (X)\n  #Add one to use for R indexing\n  xs   &lt;- (line[1]:line[3]) + 1\n  #Make sequence of second coordinates (Y)\n  ys   &lt;- (line[2]:line[4]) + 1\n\n  #Create a matrix of index values, BUT we need to write this as Y,X instead of X,Y\n  index_values &lt;- cbind(ys, xs)\n\n  zero_mat[index_values] &lt;- zero_mat[index_values] + 1\n\n}\n\nNow we can get our first answer, the number of points at which at least two lines overlap.\n\nsum(zero_mat &gt; 1)\n\n[1] 5124\n\n\n\n\nChallenge 2\n\nChallenge 2 requires us to include the diagonal lines too. This might seem more complex at first, but we are given the helpful caveat that all diagonal lines are 45 degrees, or, in other words, the slope of all lines will be 1. Because of this, our previous approach that creates a sequence of X and Y values increasing by 1 will still be appropriate. Only this time both the X and Y values will change.\n\n#Find all diagonal lines\ndiag &lt;- all_points[apply(all_points, MARGIN = 1, FUN = function(x){\n\n  x[1] != x[3] & x[2] != x[4]\n\n}), ]\n\n\n#Use the same approach to add all diagonal lines to our original matrix\nfor (i in 1:nrow(diag)) {\n\n  line &lt;- diag[i, ]\n\n  #Make sequence of first coordinates (X)\n  #Add one to use for R indexing\n  xs   &lt;- (line[1]:line[3]) + 1\n  #Make sequence of second coordinates (Y)\n  ys   &lt;- (line[2]:line[4]) + 1\n\n  #Create a matrix of index values, BUT we need to write this as Y,X instead of X,Y\n  index_values &lt;- cbind(ys, xs)\n\n  zero_mat[index_values] &lt;- zero_mat[index_values] + 1\n\n}\n\nOnce we have also added diagonal lines, we can get our second result.\n\nsum(zero_mat &gt; 1)\n\n[1] 19771\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-4/index.html#the-data",
    "href": "posts/2021-12-07-advent-of-code-day-4/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here. Most of the data we’ve dealt with so far has been 1 dimensional vectors, but for Day 4 we’re going to start working with 3-dimensional data. We’re playing BINGO!\n\nThe first line of data is a vector of numbers that are called out in the bingo game. We will extract these separately.\n\n#Read first line of data that includes the announced numbers\nnumber_calls_chr &lt;- readLines(\"./data/Day4.txt\", n = 1)\n\n#Covert this to a vector of integers\nnumber_calls_int &lt;- as.integer(stringr::str_split(number_calls_chr, pattern = \",\", simplify = TRUE))\n\nnumber_calls_int\n\n  [1] 13 47 64 52 60 69 80 85 57  1  2  6 30 81 86 40 27 26 97 77 70 92 43 94  8\n [26] 78  3 88 93 17 55 49 32 59 51 28 33 41 83 67 11 91 53 36 96  7 34 79 98 72\n [51] 39 56 31 75 82 62 99 66 29 58  9 50 54 12 45 68  4 46 38 21 24 18 44 48 16\n [76] 61 19  0 90 35 65 37 73 20 22 89 42 23 15 87 74 10 71 25 14 76 84  5 63 95\n\n\nWe then have a set of 100 bingo boards, each of which has 2 dimensions (5 rows and 5 columns). So how do we deal with this? One solution is to build a multi-dimensional array. A multi-dimensional array can be indexed just like a vector (1D) or matrix (2D), so we can easily work with and manipulate the data.\n\n#Read in the bingo boards as integers\nallboards &lt;- as.integer(scan(\"./data/Day4.txt\", what = \"list\", skip = 2))\n\n#Convert into 3D matrix\nboard_array  &lt;- array(allboards, dim = c(5, 5, length(allboards)/25))\n\n#Index the 1st and 2nd boards\nboard_array[,,1:2]\n\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   88   22    7   12   97\n[2,]   67   76   42   68   45\n[3,]   20   86    6   92   13\n[4,]   19   44   69   21   52\n[5,]   15   73   25   75   70\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   75   17   92   44   23\n[2,]   98   93   56    0    6\n[3,]   24   46   97   65   53\n[4,]   18   49   57   54   42\n[5,]   77   13   66   74   20\n\n\nWe can return a 3-dimensional position of a given number using which() and the argument arr.ind = TRUE. This will come in handy for our challenges!\n\n#Find the location of the number 0 on the first two boards\n#It is at position 2,4 on the 2nd board\nwhich(board_array[,,1:2] == 0, arr.ind = TRUE)\n\n     dim1 dim2 dim3\n[1,]    2    4    2"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-4/index.html#the-challenges",
    "href": "posts/2021-12-07-advent-of-code-day-4/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor the first challenge we need to determine which of our 100 boards will win first. To keep track of all the boards I’ll create a corresponding 3D array of logical information (TRUE/FALSE) that records whether a number on a board has been marked.\n\n#Array of logical data for all boards\nresult_array &lt;- array(rep(FALSE, n = length(allboards)), dim = c(5, 5, length(allboards)/25))\n\n#Show array for the same 2 boards\nresult_array[,,1:2]\n\n, , 1\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\n, , 2\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\n\nNow we need to work through each of the called numbers and work out which board gets a full row or column first (we’re ignoring diagonals here). We’ll just use a for loop to run through all these numbers.\n\n#Loop through all numbers called\nresult &lt;- NULL\nfor (number in number_calls_int){\n\n  #Use which to find all locations where this number occurs\n  #Update results on the corresponding array of logicals.\n  result_array[which(board_array == number, arr.ind = TRUE)] &lt;- TRUE\n\n  #Use apply to run through every board (the 3rd dimension, thus MARGIN = 3)\n  #Check if any boards have full rows or columns\n  board_status &lt;- apply(result_array, MARGIN = 3, FUN = function(x){\n\n    any(rowSums(x) == 5) | any(colSums(x) == 5)\n\n  })\n\n  has_winner &lt;- any(board_status)\n\n  #If there is a winner compute our answer\n  if (has_winner) {\n\n    winner &lt;- board_array[,,board_status]\n    unmarked &lt;- winner[!result_array[,,board_status]]\n\n    #Challenge asks for unmarked numbers * current number called\n    result &lt;- sum(unmarked) * number\n\n    break()\n\n  }\n\n}\n\nresult\n\n[1] 49686\n\n\n\n\nChallenge 2\n\nFor the second challenge, we need to find the board that will win last. This time around I’ll practice building a recursive function again.\n\n#Reset our results array\nresult_array &lt;- array(rep(FALSE, n = length(allboards)), dim = c(5, 5, length(allboards)/25))\n\n\n#Create recursive function.\nplay_bingo &lt;- function(bingo_boards, numbers, i, current_results){\n\n  #Find current number being called\n  number &lt;- numbers[i]\n\n  #Update results board with new number called.\n  current_results[which(bingo_boards == number, arr.ind = TRUE)] &lt;- TRUE\n\n  #Check status of all boards\n  board_status &lt;- apply(current_results, MARGIN = 3, FUN = function(x){\n\n    any(rowSums(x) == 5) | any(colSums(x) == 5)\n\n  })\n\n  #If there is only one board left and it is finished...\n  if (length(board_status) == 1 & sum(board_status) == 1) {\n\n    #Then return our answer\n    last_winner &lt;- bingo_boards[,,1]\n    unmarked &lt;- last_winner[!current_results[,,1]]\n\n    return(sum(unmarked) * number)\n\n  #Otherwise, remove all winning boards and call the function again...\n  } else {\n\n    #Filter only losing boards...\n    losing_boards  &lt;- bingo_boards[,,!board_status, drop = FALSE]\n    losing_results &lt;- current_results[,,!board_status, drop = FALSE]\n\n    #Recall function with next number\n    Recall(bingo_board = losing_boards, numbers = numbers, i = i + 1, current_results = losing_results)\n\n  }\n\n}\n\n\n#Let's get our result!\nplay_bingo(bingo_boards = board_array, numbers = number_calls_int, i = 1, current_results = result_array)\n\n[1] 26878\n\n\nBy using a 3D array we were able to easily work with this data. And there’s no reason to stop at 3 dimensions, you can build larger multi-dimensional arrays that can be indexed and searched through in just the same way.\n\n#Go crazy and build a 4D array with 2 sets of boards!\nfourD_array &lt;- array(c(allboards, allboards), dim = c(5, 5, length(allboards)/25, 2))\n\n#Return 1st board being played in the 2nd game\nfourD_array[,,1,2]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   88   22    7   12   97\n[2,]   67   76   42   68   45\n[3,]   20   86    6   92   13\n[4,]   19   44   69   21   52\n[5,]   15   73   25   75   70\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-3/index.html#the-data",
    "href": "posts/2021-12-07-advent-of-code-day-3/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here. This time we need to work with binary numbers.\n\nWe’re given a vector of numbers written in binary (e.g. 00100, 11110) and we need to use these to extract two values. It’s not as simple as just converting binary to decimal, we need to first generate new binary numbers that represent the most and least frequent value in each column. Let’s read the data in.\n\nlibrary(readr)\n\n#Read in data where each binary number is a character string\nday3_data &lt;- readr::read_delim(file = \"./data/Day3.txt\", delim = \"/t\",\n                               col_names = \"binary\", show_col_types = FALSE)\n\nhead(day3_data)\n\n# A tibble: 6 × 1\n  binary      \n  &lt;chr&gt;       \n1 000010000011\n2 001010000111\n3 011010000010\n4 000011111110\n5 101101000101\n6 000100010100"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-3/index.html#the-challenges",
    "href": "posts/2021-12-07-advent-of-code-day-3/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nTo find the most and least common value in each column we first need to separate the character strings so that each binary bit is a separate column. We can do this using separate() in tidyr (and a bit of regex).\nMy first thought was just to use separate() where the separator is an empty string (““), but when we try this we end up with an erroneous empty column at the start.\n\nlibrary(tidyr)\n\nbinary_length &lt;- nchar(day3_data$binary[1])\n\nday3_data %&gt;%\n  separate(col = binary, into = as.character(1:binary_length), sep = \"\") %&gt;% \n  head()\n\n# A tibble: 6 × 12\n  `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`   `10`  `11`  `12` \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 \"\"    0     0     0     0     1     0     0     0     0     0     1    \n2 \"\"    0     0     1     0     1     0     0     0     0     1     1    \n3 \"\"    0     1     1     0     1     0     0     0     0     0     1    \n4 \"\"    0     0     0     0     1     1     1     1     1     1     1    \n5 \"\"    1     0     1     1     0     1     0     0     0     1     0    \n6 \"\"    0     0     0     1     0     0     0     1     0     1     0    \n\n\nInstead, we can use regex to specify that we only want to separate by blank spaces that were preceded by a number. We can do this using the regex lookbehind operation (?&lt;=). In our example, by adding (?&lt;=[0-1]) we are specifying that a separator must have a number 0 or 1 preceding it.\n\nseparated_binary &lt;- day3_data %&gt;%\n  #Use convert = TRUE to automatically coerce to numeric\n  tidyr::separate(col = binary, into = as.character(1:binary_length), sep = \"(?&lt;=[0-1])\", convert = TRUE)\n\nhead(separated_binary)\n\n# A tibble: 6 × 12\n    `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0     0     1     0     0     0     0     0     1     1\n2     0     0     1     0     1     0     0     0     0     1     1     1\n3     0     1     1     0     1     0     0     0     0     0     1     0\n4     0     0     0     0     1     1     1     1     1     1     1     0\n5     1     0     1     1     0     1     0     0     0     1     0     1\n6     0     0     0     1     0     0     0     1     0     1     0     0\n\n\nNow identifying the most or least common value is easy. If the sum of the column is greater than the number of rows, 1 is most common and visa-versa. Below we return TRUE if 1 is most common or FALSE when 0 is most common, which directly corresponds to 1 and 0 respectively when converted to an integer.\n\n#Is 1 most common?\nmost_common &lt;- separated_binary %&gt;%\n  #Return sum of each col\n  #If it's greater than half nrow() then 1 is most common (and the inverse is true)\n  summarise(across(.cols = everything(), .fns = ~sum(.) &gt; (n()/2)))\n\nas.integer(most_common)\n\n [1] 1 0 1 1 1 1 0 0 1 1 1 0\n\n\nAs a final step, we can use the strtoi() function to convert from binary to decimal. This function requires a single string input, so we need to convert our vector of most/least common numbers to a single character string.\n\n(most_common_binary &lt;- paste(as.integer(most_common), collapse = \"\"))\n\n[1] \"101111001110\"\n\n(least_common_binary &lt;- paste(as.integer(!most_common), collapse = \"\"))\n\n[1] \"010000110001\"\n\n\n\n#Convert each number to decimal\n(most_common_decimal    &lt;- strtoi(most_common_binary, base = 2))\n\n[1] 3022\n\n(least_common_decimal  &lt;- strtoi(least_common_binary, base = 2))\n\n[1] 1073\n\n\nOur answer is the product of these two numbers.\n\nmost_common_decimal * least_common_decimal\n\n[1] 3242606\n\n\n\n\nChallenge 2\n\nThe second challenge is just a slightly more complex version of challenge 1, so I’m going to skip the explanation for now. If you’re interested, you can see the code on GitHub.\n\n\nSee previous solutions here:\n\nDay 1\nDay 2"
  },
  {
    "objectID": "posts/2021-12-12-advent-of-code-day-7/index.html#the-data",
    "href": "posts/2021-12-12-advent-of-code-day-7/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nFor Day 7 we move from fish to crabs, that are supposedly using submarines 🤷. I did not come up with the context for this! Either way, we are given a vector of (1-dimensional) positions for each crab and need to identify the position where all crabs can meet that involves the smallest amount of movement. Technically, this challenge isn’t too hard so we can spend a bit of time comparing two approaches.\n\n\n#Load data\nday7_data &lt;- scan(file = \"./data/Day7.txt\", sep = \",\")\n\nday7_data[1:100]\n\n  [1] 1101    1   29   67 1102    0    1   65 1008   65   35   66 1005   66   28\n [16]    1   67   65   20    4    0 1001   65    1   65 1106    0    8   99   35\n [31]   67  101   99  105   32  110   39  101  115  116   32  112   97  115   32\n [46]  117  110  101   32  105  110  116   99  111  100  101   32  112  114  111\n [61]  103  114   97  109   10   14   94  153  512 1778 1219  522  207  112  148\n [76] 1185  380  530  502  957  898   71   10  719   47   51  188  188 1277  446\n [91]  156  188  990  370  549 1086   49  150   42   50"
  },
  {
    "objectID": "posts/2021-12-12-advent-of-code-day-7/index.html#the-challenges",
    "href": "posts/2021-12-12-advent-of-code-day-7/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor the first challenge we assume that movement costs increase linearly with distance (i.e. moving 1 positions costs 1 ‘energy’, moving 10 positions costs 10 ‘energy’). We’ll compare the amount of energy expended to reach locations ranging from position 0 until 1991 (the furthest position in our data):\n\n(possible_positions &lt;- range(day7_data))\n\n[1]    0 1991\n\n\nBecause costs of movement are linear, the amount of energy used to move to any position is just the distance covered.\n\nenergy_use &lt;- NULL\nfor (i in possible_positions[1]:possible_positions[2]) {\n  \n  energy_use &lt;- append(energy_use, sum(abs(day7_data - i)))\n  \n}\n\nWhat is the minimum amount of energy used?\n\nmin(energy_use)\n\n[1] 351901\n\n\n\n\nChallenge 2\n\nFor our second puzzle, we assume that energy use increases non-linearly with distance, such that the energy used is the sum of the sequence of distances traveled (i.e. moving 3 positions will use 1 + 2 + 3 = 6 energy). Luckily, there is an easy mathematical solve to determine the sum of any arithmetic sequence (i.e. a sequence that increases at a constant rate):\n\\[S_{n} = n(a_{1} + a_{n})/2\\]\nWhere \\(n\\) is the length of the sequence, \\(a_{1}\\) is the first value of the sequence, and \\(a_{n}\\) is the last term in the sequence. We can make this into a function for easy use.\n\n#Sum of values in a sequence\n#Source: https://www.varsitytutors.com/hotmath/hotmath_help/topics/sum-of-the-first-n-terms-of-an-arithmetic-sequence\nsum_seq &lt;- function(start, end, n){\n  \n  (n*(start + end))/2\n  \n}\n\nNow we can apply the for loop approach from before to find the minimum energy use under this non-linear energy use assumption.\n\nenergy_use2 &lt;- NULL\nfor (i in possible_positions[1]:possible_positions[2]) {\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  energy_use2 &lt;- append(energy_use2, sum(sum_seq(start = 1, end = diffs, n = diffs)))\n  \n}\n\nmin(energy_use2)\n\n[1] 101079875\n\n\n\n\nBONUS ROUND\n\nSo this was all fairly easy it seems, but was this the most efficient method we could use? If we look at the energy use values we’ve calculated we can see that there is a clear ‘trough’ where minimum energy use is found. This is exactly the type of problem that we could solve with an optimization algorithm!\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(aes(x = 0:1991, y = energy_use2)) +\n  #Need to subtract one to index of min energy use because R starts index at 1\n  geom_vline(xintercept = which(energy_use2 == min(energy_use2)) - 1, lty = 2) +\n  labs(x = \"Position\", y = \"Energy use\") +\n  theme_classic()\n\n\n\n\nBecause we are only optimizing a single variable, we only need to use a one-dimensional optimizer. To use an optimizer, we first need to create a function that will return the value we want to minimize. This is simply a re-work of the for loop we used above.\n\n#Create function to optimize\noptim_func &lt;- function(i){\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  sum(sum_seq(start = 1, end = diffs, n = diffs))\n  \n}\n\nNow we can feed this function into the optimizer using the optim() function in R. There are many possible optimization algorithms that exist, but in R the ‘Brent’ optimizer is the default algorithm designed for one-dimensional problems.\n\noptimal_energy &lt;- optim(par = 1000, #Specify the value where the optimizer will start searching\n                        fn = optim_func, #Specify the function that will be used\n                        lower = 0,\n                        upper = possible_positions[2], #Specify the upper and lower values of the search space\n                        method = \"Brent\"#Specify the algorithm used\n)\n\nmin(energy_use2) == optimal_energy$value\n\n[1] FALSE\n\noptimal_energy$value\n\n[1] 101079758\n\n\nOur optimizer returns…a smaller minimum energy use than with our previous method?! How is this possible? Well, while our challenge only considered integer distances the optimizer has no such constraint! We can see the difference between the best (continuous) position from the optimizer and the best (categorical) position from our previous method below.\n\noptimal_energy$par\n\n[1] 478.484\n\nwhich(energy_use2 == min(energy_use2)) - 1\n\n[1] 478\n\n\nTo demonstrate how we could get the exact same result we can constrain our distances to be integers using round().\n\n#Try doing it using an optimization algorithm\noptim_func_int &lt;- function(i){\n  \n  diffs &lt;- abs(day7_data - round(i))\n  \n  sum(sum_seq(start = 1, end = diffs, n = diffs))\n  \n}\n\noptimal_energy_int &lt;- optim(par = 1000, #Specify the value where the optimizer will start searching\n                            fn = optim_func_int, #Specify the function that will be used\n                            lower = 0,\n                            upper = possible_positions[2], #Specify the upper and lower values of the search space\n                            method = \"Brent\"#Specify the algorithm used\n)\n\noptimal_energy_int$value\n\n[1] 101079875\n\n\nSo we can now see how an optimizer could be used to solve this problem, but is it actually any better? We can do some bench marking on these methods the same way we did on Day 1. In our original for loop method we necessarily have to calculate the energy use for every possible (integer) distance. In our optimization method, we only need to calculate energy use until the algorithm determines that we have reached a minima (although we cannot be certain this will be the global minimum).\nAfter benchmarking we can see that the optimization function is substantially faster than our for loop method. Orders of magnitude faster even! In our rough bootstrap below we are able to run over 1,000 times more iterations of our optimizer per second than our for loop approach. Although for loops are often useful and powerful, in cases like this an optimizer provides a much more efficient alternative.\n\nlibrary(bench)\n\n#set seed so we achieve the same outcomes\nset.seed(123)\n\ntimes &lt;- mark(old = {for (i in possible_positions[1]:2000) {\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  energy_use2 &lt;- append(energy_use2, sum(sum_seq(start = 1, end = diffs, n = diffs)))\n  \n}},\nnew = optim(par = 1000, #Specify the value where the optimizer will start searching\n            fn = optim_func_int, #Specify the function that will be used\n            lower = 0,\n            upper = 2000, #Specify the upper and lower values of the search space\n            method = \"Brent\"#Specify the algorithm used\n), check = FALSE, iterations = 50)\n\n#How many more iterations per second does our optimizer achieve?\ntimes$`itr/sec`[2]/times$`itr/sec`[1]\n\n[1] 1340.237\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nBiography\n",
    "section": "",
    "text": "Dr. Liam D. Bailey\n\n\nFreelance Data Scientist\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n \n\n\n\n\nBiography\n\nI am an ecologist and data scientist with an interest in using data science techniques for conservation outcomes.\nMy scientific work has focussed on measuring and understanding the impacts of environmental change on natural systems, particularly the impacts of climate change. I completed my Ph.D. at the Australian National University with Dr. Martijn van de Pol and Dr. Naomi Langmore, focussing on the impacts of increased flooding on the Eurasian oystercatcher (Haematopus ostralegus). Since then my work has included analysis of extreme heat impacts in semi-arid environments and advancing reproductive timing in European passerines.\nMy scientific work has always involved data science, including programming in R, Python and SQL, statistical analyses, and GIS. In 2019 I officially branched out into the world of data science, first as lead developer for the SPI-Birds Network and Database and then in a corporate setting with the language learning app Babbel.\nI am now based in Berlin using my skills in data science and data visualization as a freelancer.\n\n\nInterests\n\n\nData for Good\n\n\nData Visualisation\n\n\nClimate change ecology\n\n\nPhotography\n\n\n\n\nEducation\n\n\n\n\nPhD in Ecology, 2017\n\n\nAustralian National Unveristy, Canberra\n\n\n\n\n\nBEnvSc in Biology, First Class Honours, 2011\n\n\nMonash University, Melbourne"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Viz",
    "section": "",
    "text": "Data Viz\n  Data visualizations portfolio, mostly in R.\n\n\n\n\n\n\nFerris Wheels\n\n\n\n\n\n\nBerlin\n\n\n\n\n\n\nTV Characters\n\n\n\n\n\n\nThe Portal Project\n\n\n\n\n\n\n2 colours\n\n\n\n\n\n\nGreat British Bake-Off\n\n\n\n\n\n\nNYC Pizza\n\n\n\n\n\n\nFood\n\n\n\n\n\n\nEU Energy Generation\n\n\n\n\n\n\nBlue\n\n\n\n\n\n\nHalloween Movies\n\n\n\n\n\n\nPopulation\n\n\n\n\n\n\nMovement\n\n\n\n\n\n\nIsland\n\n\n\n\n\n\nAmazon\n\n\n\n\n\n\nNZ Bird of the Year\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\nUS Drought\n\n\n\n\n\n\nNetwork\n\n\n\n\n\n\nSpace\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Thoughts and ideas...",
    "section": "",
    "text": "Thoughts and ideas...\n \n\n\n\n\n\n    \n        \n          R\n        \n        \n          28 min read\n        \n  \n  Paris 2024: Olympic debrief\n  Who had the best performance?\n  Aug 16, 2024\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          39 min read\n        \n  \n  Who really wins the Olympics?\n  The top countries might not be who you think\n  Aug 10, 2024\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          33 min read\n        \n  \n  Missing data\n  Dealing with NAs in a time-series\n  May 30, 2022\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 8\n  Dec 13, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          7 min read\n        \n  \n  Advent of Code 2021\n  Day 7\n  Dec 12, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          6 min read\n        \n  \n  Advent of Code 2021\n  Day 6\n  Dec 9, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 5\n  Dec 8, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          4 min read\n        \n  \n  Advent of Code 2021\n  Day 4\n  Dec 6, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          3 min read\n        \n  \n  Advent of Code 2021\n  Day 3\n  Dec 5, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 2\n  Dec 4, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          8 min read\n        \n  \n  Advent of Code 2021\n  Day 1\n  Dec 2, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          6 min read\n        \n  \n  R 4.1.0\n  All the newest features\n  May 20, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          23 min read\n        \n  \n  TidyTuesday - Week 11 (2021)\n  James Bond, the Bechdel test, and adding images to ggplot\n  Mar 24, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          11 min read\n        \n  \n  TidyTuesday - Week 9 (2021)\n  Tracking racial inequality using labour statistics\n  Feb 28, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          9 min read\n        \n  \n  Building maps using OpenStreetMap\n  Up your map game\n  Jan 25, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          14 min read\n        \n  \n  Making beautiful tables with the `gt` package\n  OR: How I learnt to stop worrying and love the table\n  Nov 27, 2020\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-05-21-quarto-website-part1/index.html",
    "href": "posts/2023-05-21-quarto-website-part1/index.html",
    "title": "Building a Quarto website",
    "section": "",
    "text": "Lots has changed in how we work in R over the past year or so. One big change has been the rise of Quarto (URL), which has expanded how we can work across different programming languages. Quarto is also great for generating static websites, particularly if you want to include programming content in languages like R or Python. So, in this blog I’m going to give a brief guide to how you can create a personal website using Quarto, how you can expand this to include a blog, and finally how you can use CSS and custom Javascript to make your website look sleek and professional.\nIt’s important to acknowledge that a lot of my process and website structure is stolen heavily influenced by Maya Gans (URL). Unfortunately, she hasn’t included a guide to her process so hopefully what I’ll include here can make a fantastic website like hers more achievable by others.\nOther great resources you might find useful: - Albert Rapp"
  },
  {
    "objectID": "posts/2023-05-21-quarto-website-part1/index.html#introduction",
    "href": "posts/2023-05-21-quarto-website-part1/index.html#introduction",
    "title": "Building a Quarto website",
    "section": "",
    "text": "Lots has changed in how we work in R over the past year or so. One big change has been the rise of Quarto (URL), which has expanded how we can work across different programming languages. Quarto is also great for generating static websites, particularly if you want to include programming content in languages like R or Python. So, in this blog I’m going to give a brief guide to how you can create a personal website using Quarto, how you can expand this to include a blog, and finally how you can use CSS and custom Javascript to make your website look sleek and professional.\nIt’s important to acknowledge that a lot of my process and website structure is stolen heavily influenced by Maya Gans (URL). Unfortunately, she hasn’t included a guide to her process so hopefully what I’ll include here can make a fantastic website like hers more achievable by others.\nOther great resources you might find useful: - Albert Rapp"
  },
  {
    "objectID": "posts/2023-05-21-quarto-website-part1/index.html#step-1-create-your-website-project",
    "href": "posts/2023-05-21-quarto-website-part1/index.html#step-1-create-your-website-project",
    "title": "Building a Quarto website",
    "section": "STEP 1: Create your website project",
    "text": "STEP 1: Create your website project\n\nBefore you can deploy your fancy new website you need to create a Quarto website project. This is easy to do in either RStudio (File &gt; New Project… &gt; Quarto Website) or VSCode (Ctrl/Cmd+Shft+P &gt; Quarto: Create Project &gt; Website Project). This will create a folder structure something like the one below:\n[img/Quartowebsite_folders.png]\nWhether or not you have an .Rproj file will depend on whether you’re using RStudio or VSCode, but the other files should be the same. We’ll cover each of them in detail, but to briefly describe each file:\n\n*_quarto.yml*: Website meta-data.\nabout.qmd: Placeholder for the ‘about’ page of your website. I think in most cases this page won’t be needed!\nindex.qmd: Placeholder for the landing page of your website.\nstyles.css: Information about how your website will look (using Cascade Style Sheets = css).\n\nIf we want to see what this website will look like we can run the command quarto preview in the terminal.\n\n\nNothing usable yet, but we can improve this very quickly."
  },
  {
    "objectID": "posts/2023-05-21-quarto-website-part1/index.html#step-2-add-in-your-details",
    "href": "posts/2023-05-21-quarto-website-part1/index.html#step-2-add-in-your-details",
    "title": "Building a Quarto website",
    "section": "STEP 2: Add in your details",
    "text": "STEP 2: Add in your details\n\nAt as most simple, a personal website can just be an ‘online CV’ where somebody (say a potential employer) can find out a bit about you. So, let’s make our landing page (index.qmd) into a basic ‘profile’ for ourself. Luckily, Quarto has a standard ‘about’ page, including a number of templates.\nBelow, we update index.qmd to make an ‘about’ page for Joe Blogs. By specifying ‘about’ in our YAML meta-data (between the ---) we make it clear that we are making a page to introduce an individual or organization. You can read more about this (and different templates) here (https://quarto.org/docs/websites/website-about.html).\n---\ntitle: \"Joe Blogs\"\nabout:\n    template: jolla\n    image: profile.jpg\n---\n\nHi I'm Joe Blogs and I like to work with R and Python!\n\n## Education\n\nSchool of Hard Knocks | London, UK | Sept 2012 - June 2018\n\n## Experience\n\nBusiness Co. | Junior Data Scientist | January 2019 - present\n\n\n\n\n\n\nNote\n\n\n\nNotice that we refer to a ‘profile.jpg’, so we will need to add this file into our new website project.\n\n\nWe can also make a few small changes to our *_quarto.yml* file to change our website meta-data. Below you can see the meta-data file, with some comments to explain the key changes.\n\nproject:\n  type: website\n\nwebsite:\n  ## I like to remove the search button\n  search: false\n  ## Update your website title to be informative.\n  ## This also changes the text at the top of the navbar\n  title: \"Joe Blogs\"\n  navbar:\n    ## Add in a GitHub and Twitter link in the navbar\n    left:\n      - icon: github\n        href: https://github.com\n      - icon: twitter\n        href: https://twitter.com\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n    ## I like to remove the 'margin' section by default\n    grid:\n      margin-width: 0px\n\nSee the difference below:"
  },
  {
    "objectID": "posts/2023-05-21-quarto-website-part1/index.html#step-3-basic-styling",
    "href": "posts/2023-05-21-quarto-website-part1/index.html#step-3-basic-styling",
    "title": "Building a Quarto website",
    "section": "STEP 3: Basic styling",
    "text": "STEP 3: Basic styling\n\nThis is already a reasonable (if simple) personal page. Anybody visiting the site can learn a bit about you and find out more through your GitHub or Twitter. To make it stand out even a bit more we can start to customise the style of your page. We can do this in two ways:\n\nSTEP 3a: Quarto themes\nIn the *_quarto.yml* file above you’ll notice that we are using the default ‘cosmo’ theme for our website. Quarto includes 25 themes from the ‘Bootswatch’ project (https://bootswatch.com/). Even without learning CSS or Javascript, we can already customise the look and feel of our website by changing themes.\n\nproject:\n  type: website\n\nwebsite:\n  search: false\n  title: \"Joe Blogs\"\n  navbar:\n    left:\n      - icon: github\n        href: https://github.com\n      - icon: twitter\n        href: https://twitter.com\n\nformat:\n  html:\n    ## Use the 'lux' theme instead of 'cosmo'\n    theme: lux\n    css: styles.css\n    toc: true\n\n\n\n\n\nSTEP 3b: Basic CSS changes\nThe available Quarto themes provide a nice starting point for customisation. If we want to go further, we need to change the .css file our website uses. You’ll notice in *_quarto.yml* that we specify .css is defined in the file styles.css, so this is where we need to look.\nIn (very) short, a .css file will define how HTML elements in your page (e.g. headers, images, links) will look when they are loaded by a browser. Essentially, for particular elements we can specify their style, which might include things like colour, font size, or dimensions. I’m not going to go into more detail about CSS here. I’d recommend looking at Albert’s blog here (Albert Rapp) or other resources like W3School? ().\nYou can spend a long time learning and tweaking CSS. Still, even without being a CSS expert you can start to change elements of you blog. If you want to have the biggest bang for your buck without needing to learn detailed CSS here are some elements I suggest to change:\n\nBackground colour\nThe main body of your website is an HTML element called (surprisingly) ‘body’. If we change the ‘background’ style of this element we can give our blog a different coloured background. Simply add this code to the styles.css file with the hexcode for the colour of your choice:\n/* Everything between {} is the style of the 'body' element */\nbody {\n    background: #FFFFF0\n}\n\n\nCustom Font\nYou can use any available Google font as a custom font on your website. To do this, we first need to import the font in our .css file and then specify how that font should be used in our page. You can find the code needed to import a font directly on the Google Fonts website:\n[[]]\nYou can add this at the start of the .css file and then use the font anywhere in your document. I’d recommend using a couple of different fonts for the body of your text and the headers on your website. You’ll notice below we adjust the ‘font-family’ style of the body and both h1 and h2 headers (which you create in Markdown with one or two #).\n\n\n\n\n\n\nNote\n\n\n\nWhen we specify multiple styles, each one should end with ;\n\n\n/* Import bold Arvo font for title */\n@import url('https://fonts.googleapis.com/css2?family=Arvo:wght@700&display=swap');\n\n/* Use a thin (200 weight) version of Poppins for the main text*/ \n@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@200&display=swap');\n\nbody {\n    font-family: Poppins;\n    background: #FFFFF0\n}\n\n/* Change style of BOTH h1 and h2 */\nh1, h2 {\n    font-family: Arvo\n}\n\n\n\n\n\n\nCoding Tip\n\n\n\nIf you’re importing multiple fonts you can combine with with & like so:\n@import url('https://fonts.googleapis.com/css2?family=Arvo:wght@700&display=swap&family=Poppins:wght@200&display=swap');\n\n\n\n\nFont Size\nWe might want to change not just font we use but also the size of that font. This can be particularly useful to make your name stand out. By adding this code to our .css, we specify that any h1 header with the class ‘title’ should have larger font. I’ll explain how we know what class to focus on in a future blog.\nh1.title {\n    font-size: 12pt;\n}\nAnd with just a few lines of CSS here is our finished product. This is already a nice neat personal website and I think already stands out more than the default we started out with. By changing a bit of meta-data and a few small CSS tweaks we have a unique website of our own.\n\nBeforeAfter"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html",
    "href": "posts/2024-08-10-olympics-part1/index.html",
    "title": "Who really wins the Olympics?",
    "section": "",
    "text": "These type of record breaking headlines are a mainstay of Olympic coverage, but does the medal tally actually give us a true picture of a nation’s Olympic achievements? Is this data suitable to compare countries over time? Although I don’t have the physical prowess to actually participate in the Olympics, maybe I can still analyse it. Let’s dig into the Olympic data bank and search for answers…\n\n\n\nAustralia’s best medal tally (Guardian)"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#introduction-is-there-an-easy-answer",
    "href": "posts/2024-08-10-olympics-part1/index.html#introduction-is-there-an-easy-answer",
    "title": "Who really wins the Olympics?",
    "section": "",
    "text": "These type of record breaking headlines are a mainstay of Olympic coverage, but does the medal tally actually give us a true picture of a nation’s Olympic achievements? Is this data suitable to compare countries over time? Although I don’t have the physical prowess to actually participate in the Olympics, maybe I can still analyse it. Let’s dig into the Olympic data bank and search for answers…\n\n\n\nAustralia’s best medal tally (Guardian)"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#the-data",
    "href": "posts/2024-08-10-olympics-part1/index.html#the-data",
    "title": "Who really wins the Olympics?",
    "section": "The data",
    "text": "The data\n\nFor our source of Olympic knowledge we use the Kaggle dataset “120 years of Olympic history: athletes and results” scraped by rgriffin from http://www.sports-reference.com/. This data provides us with a wealth of knowledge on the athletes participating in the Olympics from Athens 1896 until Rio 2016."
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#the-basics-clean-the-data-and-add-some-context",
    "href": "posts/2024-08-10-olympics-part1/index.html#the-basics-clean-the-data-and-add-some-context",
    "title": "Who really wins the Olympics?",
    "section": "The basics: Clean the data and add some context",
    "text": "The basics: Clean the data and add some context\n\nLet’s load all the required packages. For data wrangling we have a bunch of packages from the tidyverse: tidyr, stringr, readr, and dplyr. We also bring in the packages lay, and janitor which help with wrangling. Next we bring in countrycode, which allows us to more easily switch between IOC country codes and the ISO3c standard (yes they are different!). Finally, for data visualisation, we have ggplot2, shadowtext, scales and gt (for tables).\nThat’s quite a few packages and I’ll try to make it clear where each package is needed in the code.\n\n## Data wrangling\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lay)\nlibrary(janitor)\nlibrary(stringr)\n## Dealing with country codes\nlibrary(countrycode)\n## Plotting\nlibrary(ggplot2)\nlibrary(shadowtext)\nlibrary(gt)\nlibrary(scales)\n\nLet’s get started on our Olympic investigation. The first thing to note is that we have data on all athletes, even those that did not win a medal. We also have data on each individual athlete within a team sport (e.g. basketball). This is super interesting data, but if we want to know the best performing countries, we’ll need to adapt this data a bit to show national medal tallies.\n\n## Load data\nathletes &lt;- readr::read_csv(\"./data/athlete_events.csv\", show_col_type = FALSE)\n\nathletes\n\n# A tibble: 271,116 × 15\n      ID Name     Sex     Age Height Weight Team  NOC   Games  Year Season City \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijia… M        24    180     80 China CHN   1992…  1992 Summer Barc…\n 2     2 A Lamusi M        23    170     60 China CHN   2012…  2012 Summer Lond…\n 3     3 Gunnar … M        24     NA     NA Denm… DEN   1920…  1920 Summer Antw…\n 4     4 Edgar L… M        34     NA     NA Denm… DEN   1900…  1900 Summer Paris\n 5     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 6     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 7     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 8     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 9     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n10     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n# ℹ 271,106 more rows\n# ℹ 3 more variables: Sport &lt;chr&gt;, Event &lt;chr&gt;, Medal &lt;chr&gt;\n\n\nFor simplicity, let’s just focus on summer Olympics and stick with post-War Olympics (1948 onwards). In our first step of data wrangling, we adjust the data so that we count the number of each medal type for each country at each games. At this stage, data are still in long format.\n\n\n\n\n\n\nNote\n\n\n\nBecause most other datasets will use ISO3c codes as country identifiers, I already substitute IOC codes to ISO3c at this early stage.\n\n\n\n\nShow the code\nathletes_clean &lt;- athletes |&gt; \n  ## Focus only on Summer Olympics since London (1948)\n  filter(Season == \"Summer\" & Year &gt;= 1948) |&gt; \n  ## For simplicity, let's classify NA as medal \"None\"\n  ## Keeping None ensures we don't exclude countries that participated in the Olympics \n  ## but didn't win any medals\n  mutate(Medal = replace_na(Medal, \"None\"),\n         ## NOTE: There are some country names that have \"-1\" in the name\n         ## We fix these typos\n         Team = stringr::str_remove_all(Team, pattern = \"-[0-9]*\"),\n         ## Some other known typos\n         Team = case_when(Team == \"Nadine\" ~ \"Netherlands\",\n                          Team == \"Don Schufro\" ~ \"Denmark\",\n                          Team == \"Rush VII\" ~ \"Sweden\",\n                          Team %in% c(\"Gem\", \"Gem IV\", \"Yeoman\") ~ \"Bahamas\",\n                          Team == \"Tango\" ~ \"Argentina\",\n                          Team == \"Nirefs\" ~ \"Greece\",\n                          Team %in% c(\"Pan\", \"Sirene\", \"Encore\") ~ \"Norway\",\n                          Team %in% c(\"Symphony\", \"Espadarte\", \"Ma'Lindo\") ~ \"Portugal\",\n                          Team == \"June Climene\" ~ \"Singapore\",\n                          Team == \"Kurush II\" ~ \"Cuba\",\n                          Team %in% c(\"Pam\", \"Bermudes\") ~ \"Bermuda\",\n                          Team == \"Hirondelle\" ~ \"Monaco\",\n                          TRUE ~ Team)) |&gt; \n  ## We're much more familiar with ISO codes (and need them to join other data below)\n  ## So we'll take this opportunity to switch to ISO3c codes\n  ## Convert IOC country codes to ISO3C\n  mutate(\n    ## In most cases, we can convert straight from IOC codes to ISO3c\n    ISO3c_NOC = countrycode::countrycode(NOC, origin = \"ioc\", destination = \"iso3c\"),\n    ## In some cases, the IOC codes are not recognised so we try using country names     \n    ISO3c_name = countrycode(Team, origin = \"country.name\", destination = \"iso3c\"),\n    ## There are a few cases where the countrycode package struggles and we add codes manually\n    ISO3c = case_when(\n      ## countrycode treats Soviet Union and Russia as the same\n      ## We want to keep these separate\n      NOC == \"URS\" ~ \"SUN\",\n      ## Use direct translation from IOC codes where available\n      !is.na(ISO3c_NOC) ~ ISO3c_NOC,\n      ## Otherwise, use estimates based on the English name (less reliable)\n      is.na(ISO3c_NOC) & !is.na(ISO3c_name) ~ ISO3c_name,\n      ## In a few specific cases (Cold War era states), we manually specify the country code\n      grepl(x = Team, pattern = \"Czechoslovakia\") ~ \"CSK\",\n      grepl(x = Team, pattern = \"East Germany\") ~ \"DDR\",\n      ## Rhodesia converted to Zimbabwe\n      NOC == \"RHO\" ~ \"ZWE\",\n      grepl(x = Team, pattern = \"Netherlands Antilles\") ~ \"ANT\",\n      grepl(x = Team, pattern = \"Serbia and Montenegro\") ~ \"SCG\",\n      grepl(x = Team, pattern = \"Kosovo\") ~ \"XKX\", ## Based on WorldBank\n      ## If we can't find a ISO3c match we revert back to the IOC codes\n      ## This includes athletes competing as refugees or individuals\n      TRUE ~ NOC)) |&gt; \n  ## When medals are won by a *team* in an event we only want to count the medal once\n  group_by(Year, Season, ISO3c, Event, Medal) |&gt; \n  slice(1) |&gt; \n  ungroup()\n\nmedals_long &lt;- athletes_clean |&gt; \n  ## Count the number of each medal won by each country at each olympics\n  group_by(Year, Season, ISO3c, Medal) |&gt; \n  summarise(n = n(),\n            country = first(Team),\n            .groups = \"drop\")\n\nmedals_long\n\n\n# A tibble: 4,691 × 6\n    Year Season ISO3c Medal      n country    \n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;      \n 1  1948 Summer AFG   None       2 Afghanistan\n 2  1948 Summer ARG   Bronze     1 Argentina  \n 3  1948 Summer ARG   Gold       3 Argentina  \n 4  1948 Summer ARG   None      96 Argentina  \n 5  1948 Summer ARG   Silver     3 Argentina  \n 6  1948 Summer AUS   Bronze     5 Australia  \n 7  1948 Summer AUS   Gold       2 Australia  \n 8  1948 Summer AUS   None      43 Australia  \n 9  1948 Summer AUS   Silver     6 Australia  \n10  1948 Summer AUT   Bronze     4 Austria    \n# ℹ 4,681 more rows\n\n\nThis is a good start, but we would be better served by data in wide format with one column for each medal type. This is how we’re more accustomed to seeing Olympic data.\n\n\nShow the code\nmedals_wide &lt;- medals_long |&gt; \n  ## Pivot wider to have column for number of medals won for each type\n  tidyr::pivot_wider(names_from = Medal, values_from = n) |&gt; \n  ## Some countries won't have any data for a medal type (i.e. they won none).\n  ## Replace these NAs with 0\n  mutate(across(c(None, Gold, Silver, Bronze), \\(x) replace_na(x, 0))) |&gt; \n  ## Order columns more logically\n  select(Year, Season, ISO3c, country, Gold, Silver, Bronze) |&gt; \n  ## Arrange to show best performing country at each games using traditional ranking\n  arrange(desc(Year), Season, desc(Gold), desc(Silver), desc(Bronze)) |&gt; \n  ## Count the total number of medals\n  mutate(total_medals = lay(pick(c(Gold, Silver, Bronze)), \\(x) sum(x)))\n\nmedals_wide\n\n\n# A tibble: 2,463 × 8\n    Year Season ISO3c country        Gold Silver Bronze total_medals\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;  &lt;int&gt;        &lt;int&gt;\n 1  2016 Summer USA   United States    46     37     38          121\n 2  2016 Summer GBR   Great Britain    27     23     17           67\n 3  2016 Summer CHN   China            26     18     26           70\n 4  2016 Summer RUS   Russia           19     17     20           56\n 5  2016 Summer DEU   Germany          17     10     15           42\n 6  2016 Summer JPN   Japan            12      8     21           41\n 7  2016 Summer FRA   France           10     18     14           42\n 8  2016 Summer KOR   South Korea       9      3      9           21\n 9  2016 Summer ITA   Italy             8     12      8           28\n10  2016 Summer AUS   Australia         8     11     10           29\n# ℹ 2,453 more rows\n\n\nWith this data we can now look at the best Olympic performances since 1948. The table below is generated by gt. See my previous blog post for more detail to understand how such tables are created.\n\n\n\n\n\n\nNote\n\n\n\nFind out the interesting story of the Unified Team at the 1992 Olympics here\n\n\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          Country == \"Olympics\" ~ \"EUN\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\n## Data to convert Years into a convenient games name (e.g. Rio 2016)\ngames_names &lt;- readr::read_csv(\"data/games_names.csv\", show_col_type = FALSE)\n\n## Find the top 10 medal tallies of all time\nplot_data &lt;- medals_wide |&gt; \n  arrange(desc(Gold), desc(Silver), desc(Bronze)) |&gt; \n  slice(1:10) |&gt;\n  mutate(rank = 1:n()) |&gt;\n  ## Join in info on the name of each game and the flag image URLS\n  left_join(games_names, by = \"Year\") |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(rank, Game, flag_URL, country, Gold, Silver, Bronze) |&gt; \n  mutate(total_medals = lay(pick(c(Gold, Silver, Bronze)), sum))\n\n## Create dynamic palettes to colour Gold, Silver, and Bronze columns\nperc_palette_Gold &lt;- col_numeric(c(\"grey99\", \"#fcc861\"), domain = c(0, max(plot_data$Gold)), alpha = 0.75)\nperc_palette_Silver &lt;- col_numeric(c(\"grey99\", \"#e5e5e5\"), domain = c(0, max(plot_data$Silver)), alpha = 0.75)\nperc_palette_Bronze &lt;- col_numeric(c(\"grey99\", \"#dcb386\"), domain = c(0, max(plot_data$Bronze)), alpha = 0.75)\n\n## Create the gt table\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(rank = \"\",\n             Game = \"Games\",\n             country = \"Country\",\n             Gold = \"Gold\",\n             Silver = \"Silver\",\n             Bronze = \"Bronze\",\n             total_medals = \"Total\") %&gt;% \n  tab_header(title = md(\"Olympic medal tally\"),\n             subtitle = \"Best ever national performances (1948 - 2016)\") %&gt;% \n  tab_source_note(source_note = \"Data: www.sports-reference.com\") %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(Gold),\n             fn = perc_palette_Gold) %&gt;%\n  data_color(columns = c(Silver),\n             fn = perc_palette_Silver) %&gt;%\n  data_color(columns = c(Bronze),\n             fn = perc_palette_Bronze) %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(rank ~ px(40),\n             c(country) ~ px(150),\n             c(Gold,\n               Silver,\n               Bronze) ~ px(100)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Olympic medal tally\n    \n    \n      Best ever national performances (1948 - 2016)\n    \n    \n      \n      Games\n      \n      Country\n      Gold\n      Silver\n      Bronze\n      Total\n    \n  \n  \n    1\nLos Angeles 1984\n\nUnited States\n82\n61\n30\n173\n    2\nMoscow 1980\n\nSoviet Union\n80\n69\n46\n195\n    3\nSeoul 1988\n\nSoviet Union\n54\n31\n46\n131\n    4\nBeijing 2008\n\nChina\n51\n21\n28\n100\n    5\nMunich 1972\n\nSoviet Union\n50\n27\n22\n99\n    6\nMontreal 1976\n\nSoviet Union\n49\n41\n35\n125\n    7\nMoscow 1980\n\nEast Germany\n47\n37\n42\n126\n    8\nRio 2016\n\nUnited States\n46\n37\n38\n121\n    9\nLondon 2012\n\nUnited States\n46\n28\n29\n103\n    10\nBarcelona 1992\n\nUnified Team\n45\n38\n29\n112\n  \n  \n    \n      Data: www.sports-reference.com"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#problem-1-the-problem-of-medal-inflation",
    "href": "posts/2024-08-10-olympics-part1/index.html#problem-1-the-problem-of-medal-inflation",
    "title": "Who really wins the Olympics?",
    "section": "Problem 1: The Problem of (medal) Inflation",
    "text": "Problem 1: The Problem of (medal) Inflation\n\nAt first the topic of Olympic success seems pretty straight forward. More medals equals more success. But if we want to compare medal tallies between years we necessarily expect that the number of possible medals stays the same. Otherwise a country might do ‘better’ just because they participated in a particularly lucrative Olympics. Does this assumption of a fixed medal pool hold up?\nLet’s go back to our data and look at the total number of Gold, Silver, and Bronze medals awarded each year.\n\nnumber_medals &lt;- medals_wide |&gt; \n  group_by(Year) |&gt; \n  summarise(across(c(Gold, Silver, Bronze), sum)) |&gt; \n  ## Convert to long format for plotting\n  tidyr::pivot_longer(Gold:Bronze, names_to = \"Medal\", values_to = \"n_medals\")\n\n\n\nShow the code\nplot_data &lt;- number_medals |&gt;\n  mutate(Medal = factor(Medal, levels = c(\"Gold\", \"Silver\", \"Bronze\")))\n\ntext_data &lt;- plot_data |&gt; \n  group_by(Year) |&gt; \n  summarise(total_medals = sum(n_medals))\n\nggplot() +\n  geom_bar(data = plot_data,\n           aes(x = as.factor(Year), y = n_medals, fill = Medal),\n           position = \"stack\", stat = \"identity\", colour = \"grey20\") +\n  geom_text(data = text_data,\n            aes(x = as.factor(Year), y = total_medals,\n                label = Year), angle = 90,\n            hjust = 1.3, fontface = \"bold\", colour = \"grey10\",\n            size = 4, family = \"Chivo\") +\n  shadowtext::geom_shadowtext(data = plot_data |&gt; \n                                slice(1:3) |&gt; \n                                arrange(desc(Medal)) |&gt; \n                                mutate(y = cumsum(n_medals)),\n                              aes(x = as.factor(Year),\n                                  y = y, label = Medal,\n                                  colour = Medal),\n                              bg.colour=\"grey10\",\n                              size = 4,\n                              angle = 90, hjust = 1, vjust = -1.75,\n                              fontface = \"bold\", family = \"Chivo\") +\nlabs(title = \"The Problem of (medal) Inflation\",\nsubtitle = \"Number of medals available at the Olympics has more than doubles since 1948\") +\n  scale_fill_manual(values = c(\"#fcc861\", \"#e5e5e5\", \"#dcb386\")) +\n  scale_colour_manual(values = c(\"#fcc861\", \"#e5e5e5\", \"#dcb386\")) +\n  scale_y_continuous(expand = c(0, 0, 0, 0),\n                     name = \"Number of medals\", limits = c(NA, 1000)) +\n  scale_x_discrete(expand = c(0.075, 0.075, 0.075, 0.075)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.y = element_text(size = 15, colour = \"grey10\",\n                                    face = \"bold\", family = \"Chivo\"),\n        axis.text.y = element_text(size = 10, colour = \"grey10\",\n                                   face = \"bold\", family = \"Chivo\"),\n        plot.margin = margin(t = 10, b = 10, l = 15, r = 15),\n        plot.title = element_text(size = 20, face = \"bold\", family = \"Chivo\"),\n        plot.subtitle = element_text(size = 15, family = \"Chivo\"),\n        plot.background = element_rect(colour = \"black\"))\n\n\n\n\n\nThere were a total of 973 medals available in Rio 2016 (the most recent Olympics available in the data), which is 132 extra medals compared to Atlanta 1996 (841 medals) and more than double that available at London 1948 (439 medals).\nSimply looking at medal counts is therefore misleading. Instead, let’s consider the percentage of medals claimed at a given Olympics.\n\n\nShow the code\nmedals_wide_prop &lt;- medals_long |&gt; \n  ## Join in total number of medals\n  left_join(number_medals, by = c(\"Year\", \"Medal\")) |&gt;\n  ## Determine % of each medal type that a country won at those games\n  ## We keep 'None' but don't calculate a proportion\n  ## None is kept to keep countries that didn't manage to win a medal\n  mutate(perc_medal = case_when(Medal != \"None\" ~ (n/n_medals)*100,\n                                TRUE ~ NA)) |&gt; \n  select(Year, Season, ISO3c, country, Medal, perc_medal) |&gt; \n  ## Pivot wider to have column for number of medals won for each type\n  tidyr::pivot_wider(names_from = Medal, values_from = perc_medal, names_prefix = \"perc_\") |&gt; \n  ## Some countries won't have any data for a medal type. Replace this with 0\n  mutate(across(c(perc_None, perc_Bronze, perc_Silver, perc_Gold), \\(x) replace_na(x, 0))) |&gt; \n  ## Order columns more logically\n  select(Year, Season, ISO3c, country, perc_Gold, perc_Silver, perc_Bronze) |&gt; \n  ## Arrange to show best performing country at each games using percentage ranking\n  arrange(desc(Year), Season, desc(perc_Gold), desc(perc_Silver), desc(perc_Bronze))\n\n## Also join in data on % on total medals won\ntotal_prop &lt;- medals_wide |&gt; \n  left_join(number_medals |&gt; \n              group_by(Year) |&gt; \n              summarise(total = sum(n_medals)), by = \"Year\") |&gt; \n  mutate(perc_Total = (total_medals/total)*100) |&gt; \n  select(Year, ISO3c, perc_Total)\n\nmedals_wide_prop &lt;- medals_wide_prop |&gt; \n  left_join(total_prop, by = c(\"Year\", \"ISO3c\"))\n\nmedals_wide_prop\n\n\n# A tibble: 2,463 × 8\n    Year Season ISO3c country       perc_Gold perc_Silver perc_Bronze perc_Total\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1  2016 Summer USA   United States     15.0       12.1         10.6       12.4 \n 2  2016 Summer GBR   Great Britain      8.79       7.52         4.72       6.89\n 3  2016 Summer CHN   China              8.47       5.88         7.22       7.19\n 4  2016 Summer RUS   Russia             6.19       5.56         5.56       5.76\n 5  2016 Summer DEU   Germany            5.54       3.27         4.17       4.32\n 6  2016 Summer JPN   Japan              3.91       2.61         5.83       4.21\n 7  2016 Summer FRA   France             3.26       5.88         3.89       4.32\n 8  2016 Summer KOR   South Korea        2.93       0.980        2.5        2.16\n 9  2016 Summer ITA   Italy              2.61       3.92         2.22       2.88\n10  2016 Summer AUS   Australia          2.61       3.59         2.78       2.98\n# ℹ 2,453 more rows\n\n\nWe can now adapt our previous table to look at the 10 greatest national performances based on medal percentage.\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\ngames_names &lt;- readr::read_csv(\"data/games_names.csv\", show_col_type = FALSE)\n\nplot_data &lt;- medals_wide_prop |&gt; \n  arrange(desc(perc_Gold), desc(perc_Silver), desc(perc_Bronze)) |&gt; \n  slice(1:10) |&gt;\n  mutate(rank = 1:n()) |&gt;\n  left_join(games_names, by = \"Year\") |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(rank, Game, flag_URL, country, perc_Gold, perc_Silver, perc_Bronze, perc_Total)\n\nperc_palette_Gold &lt;- col_numeric(c(\"grey99\", \"#fcc861\"), domain = c(0, max(plot_data$perc_Gold)), alpha = 0.75)\nperc_palette_Silver &lt;- col_numeric(c(\"grey99\", \"#e5e5e5\"), domain = c(0, max(plot_data$perc_Silver)), alpha = 0.75)\nperc_palette_Bronze &lt;- col_numeric(c(\"grey99\", \"#dcb386\"), domain = c(0, max(plot_data$perc_Bronze)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(rank = \"\",\n             Game = \"Games\",\n             country = \"Country\",\n             perc_Gold = \"Gold (%)\",\n             perc_Silver = \"Silver (%)\",\n             perc_Bronze = \"Bronze (%)\",\n             perc_Total = \"Total (%)\") %&gt;% \n  tab_header(title = md(\"Olympic medal tally (%)\"),\n             subtitle = \"Best ever national performances (1948 - 2016)\") %&gt;% \n  tab_source_note(source_note = \"Data: www.sports-reference.com\") %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(perc_Gold),\n             fn = perc_palette_Gold) %&gt;%\n  data_color(columns = c(perc_Silver),\n             fn = perc_palette_Silver) %&gt;%\n  data_color(columns = c(perc_Bronze),\n             fn = perc_palette_Bronze) %&gt;%\n  fmt_number(columns = c(perc_Gold, perc_Silver, perc_Bronze, perc_Total),\n             decimals = 2) %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(rank ~ px(40),\n             Game ~ px(150),\n             c(country) ~ px(125),\n             c(perc_Gold,\n               perc_Silver,\n               perc_Bronze,\n               perc_Total) ~ px(85)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Olympic medal tally (%)\n    \n    \n      Best ever national performances (1948 - 2016)\n    \n    \n      \n      Games\n      \n      Country\n      Gold (%)\n      Silver (%)\n      Bronze (%)\n      Total (%)\n    \n  \n  \n    1\nMoscow 1980\n\nSoviet Union\n39.22\n33.82\n20.63\n30.90\n    2\nLos Angeles 1984\n\nUnited States\n36.61\n27.98\n12.35\n25.26\n    3\nRome 1960\n\nSoviet Union\n28.29\n19.46\n19.38\n22.34\n    4\nHelsinki 1952\n\nUnited States\n26.85\n12.58\n10.83\n16.63\n    5\nLondon 1948\n\nUnited States\n26.39\n18.49\n12.75\n19.13\n    6\nMexico City 1968\n\nUnited States\n25.86\n16.47\n18.58\n20.30\n    7\nMunich 1972\n\nSoviet Union\n25.64\n13.85\n10.48\n16.50\n    8\nMontreal 1976\n\nSoviet Union\n24.75\n20.60\n16.20\n20.39\n    9\nMelbourne 1956\n\nSoviet Union\n24.18\n18.95\n19.88\n20.99\n    10\nMoscow 1980\n\nEast Germany\n23.04\n18.14\n18.83\n19.97\n  \n  \n    \n      Data: www.sports-reference.com"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#problem-2-the-problem-of-politics-and-pandemics",
    "href": "posts/2024-08-10-olympics-part1/index.html#problem-2-the-problem-of-politics-and-pandemics",
    "title": "Who really wins the Olympics?",
    "section": "Problem 2: The Problem of Politics (and Pandemics)",
    "text": "Problem 2: The Problem of Politics (and Pandemics)\n\nWe start to see performances from older Olympics (e.g. Melbourne 1956) making the cut once we account for the smaller number of medals available in those earlier years. Unfortunately, looking at the top two national performances we see another problem in our quest to calculate Olympic success. 4 of the top 10 national performances in our table come from Olympics marred by boycotts. The impressive display of the Soviet Union in 1980 and USA in 1984 is a bit less impressive when you remember that their major sporting rivals boycotted these games. It’s hard to compare performances from boycotted games to those with full participation.\nOlympics is never devoid of politics, but to make things easier let’s drill down on Olympics from Atlanta 1996 onwards. This avoids major issues of Olympic boycotts and it covers a period where country borders have been a bit more stable. Because our data only goes up to Rio 2016, we also avoid the Covid disrupted event of 2020/2021.\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\ngames_names &lt;- readr::read_csv(\"data/games_names.csv\", show_col_type = FALSE)\n\nplot_data &lt;- medals_wide_prop |&gt; \n  filter(Year &gt;= 1996) |&gt; \n  arrange(desc(perc_Gold), desc(perc_Silver), desc(perc_Bronze)) |&gt; \n  slice(1:10) |&gt;\n  mutate(rank = 1:n()) |&gt;\n  left_join(games_names, by = \"Year\") |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(rank, Game, flag_URL, country, perc_Gold, perc_Silver, perc_Bronze, perc_Total)\n\nperc_palette_Gold &lt;- col_numeric(c(\"grey99\", \"#fcc861\"), domain = c(0, max(plot_data$perc_Gold)), alpha = 0.75)\nperc_palette_Silver &lt;- col_numeric(c(\"grey99\", \"#e5e5e5\"), domain = c(0, max(plot_data$perc_Silver)), alpha = 0.75)\nperc_palette_Bronze &lt;- col_numeric(c(\"grey99\", \"#dcb386\"), domain = c(0, max(plot_data$perc_Bronze)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(rank = \"\",\n             Game = \"Games\",\n             country = \"Country\",\n             perc_Gold = \"Gold (%)\",\n             perc_Silver = \"Silver (%)\",\n             perc_Bronze = \"Bronze (%)\",\n             perc_Total = \"Total (%)\") %&gt;% \n  tab_header(title = md(\"Olympic medal tally (%)\"),\n             subtitle = \"Best national performances (1996 - 2016)\") %&gt;% \n  tab_source_note(source_note = \"Data: www.sports-reference.com\") %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(perc_Gold),\n             fn = perc_palette_Gold) %&gt;%\n  data_color(columns = c(perc_Silver),\n             fn = perc_palette_Silver) %&gt;%\n  data_color(columns = c(perc_Bronze),\n             fn = perc_palette_Bronze) %&gt;%\n  fmt_number(columns = c(perc_Gold, perc_Silver, perc_Bronze, perc_Total),\n             decimals = 2) %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(rank ~ px(40),\n             c(country, Game) ~ px(125),\n             c(perc_Gold,\n               perc_Silver,\n               perc_Bronze,\n               perc_Total) ~ px(85)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Olympic medal tally (%)\n    \n    \n      Best national performances (1996 - 2016)\n    \n    \n      \n      Games\n      \n      Country\n      Gold (%)\n      Silver (%)\n      Bronze (%)\n      Total (%)\n    \n  \n  \n    1\nBeijing 2008\n\nChina\n16.89\n6.95\n7.93\n10.45\n    2\nAtlanta 1996\n\nUnited States\n16.24\n11.72\n8.42\n12.01\n    3\nLondon 2012\n\nUnited States\n15.23\n9.21\n8.15\n10.71\n    4\nRio 2016\n\nUnited States\n14.98\n12.09\n10.56\n12.44\n    5\nLondon 2012\n\nChina\n12.58\n8.88\n6.46\n9.15\n    6\nSydney 2000\n\nUnited States\n12.04\n8.00\n9.51\n9.84\n    7\nAthens 2004\n\nUnited States\n11.96\n13.00\n7.98\n10.90\n    8\nBeijing 2008\n\nUnited States\n11.92\n12.91\n9.92\n11.49\n    9\nSydney 2000\n\nRussia\n10.70\n9.33\n8.90\n9.62\n    10\nAthens 2004\n\nChina\n10.63\n5.67\n4.29\n6.80\n  \n  \n    \n      Data: www.sports-reference.com"
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#problem-3-the-problem-of-money",
    "href": "posts/2024-08-10-olympics-part1/index.html#problem-3-the-problem-of-money",
    "title": "Who really wins the Olympics?",
    "section": "Problem 3: The Problem of Money",
    "text": "Problem 3: The Problem of Money\n\nWe now see the top national performances from the past 20 years (6 Olympics) with the issue of medal inflation removed and political boycotts avoided. China and the US dominate the standings, with 9 of the top 10 performances on record. But is this really surprising considering these are the world’s two largest economies with countless amounts of money to throw at sport (and the prestige it can bring)? If we really want to measure sporting achievement, maybe we should remove the clear advantage available to wealthy countries. Instead of just showing medal percentage, we can instead show medal percentage per $GDP to account for economic opportunity.\n\n\nShow the code\n## Load GDP data\n## From World Bank https://databank.worldbank.org/source/world-development-indicators\nGDP &lt;- readr::read_csv(\"./data/GDP_data.csv\", skip = 4, show_col_types = FALSE) |&gt; \n  ## Pivot longer to allow us to left join\n  tidyr::pivot_longer(cols = `1960`:`2023`, names_to = \"year\", values_to = \"gdp\") |&gt; \n  mutate(year = as.numeric(year)) |&gt; \n  janitor::clean_names() |&gt; \n  select(country_code:indicator_name, year, gdp) |&gt; \n  ## Remove countries with no GDP data\n  filter(!is.na(gdp))\n\nmedals_gdp &lt;- medals_wide_prop |&gt;\n  ## Join in GDP data\n  left_join(GDP, by = c(\"ISO3c\" = \"country_code\", \"Year\" = \"year\")) |&gt; \n  ## Calculate medal % per billion$ GDP\n  mutate(gdp_bil = gdp/1e9,\n         perc_Gold_gdp = perc_Gold/gdp_bil,\n         perc_Silver_gdp = perc_Silver/gdp_bil,\n         perc_Bronze_gdp = perc_Bronze/gdp_bil) |&gt; \n  select(Year, Season, ISO3c, country, perc_Gold_gdp:perc_Bronze_gdp, everything())\n\nmedals_gdp\n\n\n# A tibble: 2,463 × 14\n    Year Season ISO3c country      perc_Gold_gdp perc_Silver_gdp perc_Bronze_gdp\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1  2016 Summer USA   United Stat…      0.000797        0.000643        0.000561\n 2  2016 Summer GBR   Great Brita…      0.00327         0.00280         0.00176 \n 3  2016 Summer CHN   China             0.000754        0.000524        0.000643\n 4  2016 Summer RUS   Russia            0.00485         0.00435         0.00435 \n 5  2016 Summer DEU   Germany           0.00160         0.000942        0.00120 \n 6  2016 Summer JPN   Japan             0.000781        0.000522        0.00117 \n 7  2016 Summer FRA   France            0.00132         0.00238         0.00157 \n 8  2016 Summer KOR   South Korea       0.00195         0.000654        0.00167 \n 9  2016 Summer ITA   Italy             0.00139         0.00209         0.00118 \n10  2016 Summer AUS   Australia         0.00216         0.00298         0.00230 \n# ℹ 2,453 more rows\n# ℹ 7 more variables: perc_Gold &lt;dbl&gt;, perc_Silver &lt;dbl&gt;, perc_Bronze &lt;dbl&gt;,\n#   perc_Total &lt;dbl&gt;, indicator_name &lt;chr&gt;, gdp &lt;dbl&gt;, gdp_bil &lt;dbl&gt;\n\n\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\ngames_names &lt;- readr::read_csv(\"data/games_names.csv\", show_col_type = FALSE)\n\nplot_data &lt;- medals_gdp |&gt;\n  left_join(medals_wide |&gt; select(Year, ISO3c, total_medals),\n            by = c(\"Year\", \"ISO3c\")) |&gt; \n  filter(Year &gt;= 1996 & total_medals &gt;= 5) |&gt; \n  arrange(desc(perc_Gold_gdp), desc(perc_Silver_gdp), desc(perc_Bronze_gdp)) |&gt; \n  mutate(rank = 1:n()) |&gt;\n  filter(rank &lt;= 10 | (Year == 2008 & ISO3c == \"CHN\") | (Year == 1996 & ISO3c == \"USA\")) |&gt; \n  left_join(games_names, by = \"Year\") |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(rank, Game, flag_URL, country, perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp, gdp_bil)\n\n## Create dynamic palettes to colour Gold, Silver, and Bronze columns\nperc_palette_Gold &lt;- col_numeric(c(\"grey99\", \"#fcc861\"), domain = c(0, max(plot_data$perc_Gold_gdp)), alpha = 0.75)\nperc_palette_Silver &lt;- col_numeric(c(\"grey99\", \"#e5e5e5\"), domain = c(0, max(plot_data$perc_Silver_gdp)), alpha = 0.75)\nperc_palette_Bronze &lt;- col_numeric(c(\"grey99\", \"#dcb386\"), domain = c(0, max(plot_data$perc_Bronze_gdp)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(rank = \"\",\n             Game = \"Games\",\n             country = \"Country\",\n             perc_Gold_gdp = md(\"Gold&lt;br&gt;(%/$GDP)\"),\n             perc_Silver_gdp = md(\"Silver&lt;br&gt;(%/$GDP)\"),\n             perc_Bronze_gdp = md(\"Bronze&lt;br&gt;(%/$GDP)\"),\n             gdp_bil = md(\"GDP&lt;br&gt;(Billion $USD)\")) %&gt;% \n  tab_header(title = md(\"Olympic performance (% relative to GDP)\"),\n             subtitle = md(\"Best national performances (1996 - 2016)&lt;br&gt;Considering countries that won at least 5 medals\")) %&gt;% \n  tab_source_note(source_note = md(\"Olympic data: www.sports-reference.com&lt;br&gt;GDP data: World Bank Group\")) %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(perc_Gold_gdp),\n             fn = perc_palette_Gold) %&gt;%\n  data_color(columns = c(perc_Silver_gdp),\n             fn = perc_palette_Silver) %&gt;%\n  data_color(columns = c(perc_Bronze_gdp),\n             fn = perc_palette_Bronze) %&gt;%\n  fmt_number(columns = c(perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp, gdp_bil),\n             decimals = 3) %&gt;%\n  fmt_number(columns = c(gdp_bil), pattern = \"${x}\") %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(rank ~ px(40),\n             c(country, Game) ~ px(125),\n             c(perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp) ~ px(80),\n             gdp_bil ~ px(110)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Olympic performance (% relative to GDP)\n    \n    \n      Best national performances (1996 - 2016)Considering countries that won at least 5 medals\n    \n    \n      \n      Games\n      \n      Country\n      Gold(%/$GDP)\n      Silver(%/$GDP)\n      Bronze(%/$GDP)\n      GDP(Billion $USD)\n    \n  \n  \n    1\nSydney 2000\n\nEthiopia\n0.162\n0.040\n0.112\n$8.24\n    2\nRio 2016\n\nJamaica\n0.139\n0.070\n0.039\n$14.08\n    3\nAtlanta 1996\n\nCuba\n0.133\n0.117\n0.108\n$25.02\n    4\nSydney 2000\n\nBulgaria\n0.126\n0.151\n0.046\n$13.25\n    5\nBeijing 2008\n\nJamaica\n0.121\n0.048\n0.041\n$13.71\n    6\nSydney 2000\n\nCuba\n0.120\n0.120\n0.070\n$30.57\n    7\nSydney 2000\n\nRomania\n0.099\n0.054\n0.074\n$37.25\n    8\nAtlanta 1996\n\nBulgaria\n0.090\n0.209\n0.137\n$12.29\n    9\nLondon 2012\n\nJamaica\n0.089\n0.089\n0.076\n$14.81\n    10\nSydney 2000\n\nBelarus\n0.079\n0.079\n0.265\n$12.74\n    124\nBeijing 2008\n\nChina\n0.004\n0.002\n0.002\n$4,594.34\n    162\nAtlanta 1996\n\nUnited States\n0.002\n0.001\n0.001\n$8,073.12\n  \n  \n    \n      Olympic data: www.sports-reference.comGDP data: World Bank Group\n    \n  \n  \n\n\n\n\nWe finally get a picture of the real Olympic winners! Countries that have been able to achieve sporting success despite having orders of magnitude less money to invest into expensive sporting facilities or training regimes.\n\n\n\n\n\n\nNote\n\n\n\nThis table shows the top 10 countries using our new % medal/$GDP metric, but I’ve filtered it down to show only countries that won 5 or more medals. This hopefully focuses on countries with a quality Olympic team rather than just one individual star. You can see the full (unfiltered) table below."
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#the-conclusion-always-think-twice-about-statistics",
    "href": "posts/2024-08-10-olympics-part1/index.html#the-conclusion-always-think-twice-about-statistics",
    "title": "Who really wins the Olympics?",
    "section": "The Conclusion: Always think twice about statistics",
    "text": "The Conclusion: Always think twice about statistics\n\nHeadlines about record breaking medal tallies may seem catchy, but this is often just a consequence of the increasing number of sports and events being added to the Olympics (e.g. skateboarding, climbing). Even when countries do break new medal records (accounting for medal inflation!) a lot of these wins are only possible with hefty government investment which isn’t available in most places. Next time you’re looking at the Olympic medal tally, maybe scroll down a bit to spot those countries that are really punching above their weight.\nNext week, we can see what our Olympic assessment method can tell us about the results of Paris 2024."
  },
  {
    "objectID": "posts/2024-08-10-olympics-part1/index.html#appendix-top-ranked-countries-without-filtering",
    "href": "posts/2024-08-10-olympics-part1/index.html#appendix-top-ranked-countries-without-filtering",
    "title": "Who really wins the Olympics?",
    "section": "Appendix: Top ranked countries without filtering",
    "text": "Appendix: Top ranked countries without filtering\n\nThis table shows the top ranking countries based on % medals/$GDP, but including countries with small medals hauls (&lt;5). We see a few new appearances but Jamaica, Ethiopia, Cuba, and Bulgaria still feature in the top 10.\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\ngames_names &lt;- readr::read_csv(\"data/games_names.csv\", show_col_type = FALSE)\n\nplot_data &lt;- medals_gdp |&gt;\n  left_join(medals_wide |&gt; select(Year, ISO3c, total_medals),\n            by = c(\"Year\", \"ISO3c\")) |&gt; \n  filter(Year &gt;= 1996) |&gt; \n  arrange(desc(perc_Gold_gdp), desc(perc_Silver_gdp), desc(perc_Bronze_gdp)) |&gt; \n  mutate(rank = 1:n()) |&gt;\n  filter(rank &lt;= 10 | (Year == 2008 & ISO3c == \"CHN\") | (Year == 1996 & ISO3c == \"USA\")) |&gt; \n  left_join(games_names, by = \"Year\") |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(rank, Game, flag_URL, country, perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp, gdp_bil)\n\n## Create dynamic palettes to colour Gold, Silver, and Bronze columns\nperc_palette_Gold &lt;- col_numeric(c(\"grey99\", \"#fcc861\"), domain = c(0, max(plot_data$perc_Gold_gdp)), alpha = 0.75)\nperc_palette_Silver &lt;- col_numeric(c(\"grey99\", \"#e5e5e5\"), domain = c(0, max(plot_data$perc_Silver_gdp)), alpha = 0.75)\nperc_palette_Bronze &lt;- col_numeric(c(\"grey99\", \"#dcb386\"), domain = c(0, max(plot_data$perc_Bronze_gdp)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(rank = \"\",\n             Game = \"Games\",\n             country = \"Country\",\n             perc_Gold_gdp = md(\"Gold&lt;br&gt;(%/$GDP)\"),\n             perc_Silver_gdp = md(\"Silver&lt;br&gt;(%/$GDP)\"),\n             perc_Bronze_gdp = md(\"Bronze&lt;br&gt;(%/$GDP)\"),\n             gdp_bil = md(\"GDP&lt;br&gt;(Billion $USD)\")) %&gt;% \n  tab_header(title = md(\"Olympic performance (% relative to GDP)\"),\n             subtitle = md(\"Best national performances (1996 - 2016)\")) %&gt;% \n  tab_source_note(source_note = md(\"Olympic data: www.sports-reference.com&lt;br&gt;GDP data: World Bank Group\")) %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(perc_Gold_gdp),\n             fn = perc_palette_Gold) %&gt;%\n  data_color(columns = c(perc_Silver_gdp),\n             fn = perc_palette_Silver) %&gt;%\n  data_color(columns = c(perc_Bronze_gdp),\n             fn = perc_palette_Bronze) %&gt;%\n  fmt_number(columns = c(perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp, gdp_bil),\n             decimals = 3) %&gt;%\n  fmt_number(columns = c(gdp_bil), pattern = \"${x}\") %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(rank ~ px(40),\n             c(country, Game) ~ px(125),\n             c(perc_Gold_gdp, perc_Silver_gdp, perc_Bronze_gdp) ~ px(80),\n             gdp_bil ~ px(110)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Olympic performance (% relative to GDP)\n    \n    \n      Best national performances (1996 - 2016)\n    \n    \n      \n      Games\n      \n      Country\n      Gold(%/$GDP)\n      Silver(%/$GDP)\n      Bronze(%/$GDP)\n      GDP(Billion $USD)\n    \n  \n  \n    1\nAtlanta 1996\n\nBurundi\n0.425\n0.000\n0.000\n$0.87\n    2\nLondon 2012\n\nGrenada\n0.414\n0.000\n0.000\n$0.80\n    3\nAtlanta 1996\n\nArmenia\n0.231\n0.229\n0.000\n$1.60\n    4\nSydney 2000\n\nEthiopia\n0.162\n0.040\n0.112\n$8.24\n    5\nRio 2016\n\nJamaica\n0.139\n0.070\n0.039\n$14.08\n    6\nAtlanta 1996\n\nCuba\n0.133\n0.117\n0.108\n$25.02\n    7\nAthens 2004\n\nGeorgia\n0.130\n0.130\n0.000\n$5.13\n    8\nSydney 2000\n\nAzerbaijan\n0.127\n0.000\n0.058\n$5.27\n    9\nSydney 2000\n\nBulgaria\n0.126\n0.151\n0.046\n$13.25\n    10\nBeijing 2008\n\nJamaica\n0.121\n0.048\n0.041\n$13.71\n    184\nBeijing 2008\n\nChina\n0.004\n0.002\n0.002\n$4,594.34\n    234\nAtlanta 1996\n\nUnited States\n0.002\n0.001\n0.001\n$8,073.12\n  \n  \n    \n      Olympic data: www.sports-reference.comGDP data: World Bank Group"
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html",
    "href": "posts/2024-08-16-olympics-part2/index.html",
    "title": "Paris 2024: Olympic debrief",
    "section": "",
    "text": "In my last post, we saw how medal inflation and economic advantage can make it tricky to properly assess a country’s Olympic performance. Now that we have the final results from Paris 2024, we can use our new found knowledge to find the countries that over-(and under) performed at this year’s event."
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#introduction-what-have-we-learnt-so-far",
    "href": "posts/2024-08-16-olympics-part2/index.html#introduction-what-have-we-learnt-so-far",
    "title": "Paris 2024: Olympic debrief",
    "section": "",
    "text": "In my last post, we saw how medal inflation and economic advantage can make it tricky to properly assess a country’s Olympic performance. Now that we have the final results from Paris 2024, we can use our new found knowledge to find the countries that over-(and under) performed at this year’s event."
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#the-data",
    "href": "posts/2024-08-16-olympics-part2/index.html#the-data",
    "title": "Paris 2024: Olympic debrief",
    "section": "The data",
    "text": "The data\n\nWe start out where we left off in the last blog with data on medal percentages between 1996 and 2016. We will add to this with new results from Paris 2024. To include information on GDP, we will use data from the World Bank."
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#the-basics-clean-the-data-and-add-some-context",
    "href": "posts/2024-08-16-olympics-part2/index.html#the-basics-clean-the-data-and-add-some-context",
    "title": "Paris 2024: Olympic debrief",
    "section": "The basics: Clean the data and add some context",
    "text": "The basics: Clean the data and add some context\n\nPackages this time are: dplyr, tidyr, readr, and lay to work with data and ggplot2 and shadowtext to create plots and scales and gt when building tables. This time we’ll also include spaMM, which we’ll use for some basic statistical modelling. Again, countrycode is needed to convert NOC to ISO3c.\n\n## Data wrangling\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(lay)\nlibrary(countrycode)\n## Plots and tables\nlibrary(ggplot2)\nlibrary(shadowtext)\nlibrary(geomtextpath)\nlibrary(scales)\nlibrary(gt)\n## Stats\nlibrary(spaMM)\n\nFirst, let’s read in the data we finished with last time and the new data from Paris.\n\n## Load data\n(historical_data &lt;- readr::read_csv(\"./data/training_data.csv\", show_col_type = FALSE))\n\n# A tibble: 2,463 × 6\n    Year ISO3c country       perc_Gold perc_Silver perc_Bronze\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1  2016 USA   United States     15.0       12.1         10.6 \n 2  2016 GBR   Great Britain      8.79       7.52         4.72\n 3  2016 CHN   China              8.47       5.88         7.22\n 4  2016 RUS   Russia             6.19       5.56         5.56\n 5  2016 DEU   Germany            5.54       3.27         4.17\n 6  2016 JPN   Japan              3.91       2.61         5.83\n 7  2016 FRA   France             3.26       5.88         3.89\n 8  2016 KOR   South Korea        2.93       0.980        2.5 \n 9  2016 ITA   Italy              2.61       3.92         2.22\n10  2016 AUS   Australia          2.61       3.59         2.78\n# ℹ 2,453 more rows\n\n\n\nparis &lt;- readr::read_csv(\"./data/paris_results.csv\", show_col_types = FALSE) |&gt; \n  mutate(across(Gold:Total, \\(x) tidyr::replace_na(x, 0)),\n         ISO3c_countrycode = countrycode::countrycode(NOC, origin = \"ioc\", destination = \"iso3c\"),\n         ISO3c = case_when(!is.na(ISO3c_countrycode) ~ ISO3c_countrycode,\n                           NOC == \"KOS\" ~ \"XKX\",\n                           NOC == \"SIN\" ~ \"SGP\",\n                           is.na(ISO3c_countrycode) ~ NOC),\n         country = countrycode::countrycode(ISO3c, origin = \"iso3c\", destination = \"country.name\")) |&gt; \n  select(country, ISO3c, Gold:Total)\n\nparis\n\n# A tibble: 92 × 6\n   country        ISO3c  Gold Silver Bronze Total\n   &lt;chr&gt;          &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 United States  USA      40     44     42   126\n 2 China          CHN      40     27     24    91\n 3 Japan          JPN      20     12     13    45\n 4 Australia      AUS      18     19     16    53\n 5 France         FRA      16     26     22    64\n 6 Netherlands    NLD      15      7     12    34\n 7 United Kingdom GBR      14     22     29    65\n 8 South Korea    KOR      13      9     10    32\n 9 Italy          ITA      12     13     15    40\n10 Germany        DEU      12     13      8    33\n# ℹ 82 more rows\n\n\nWe know from last time that accounting for the number of available medals is important. There were 71 more medals available at Paris 2024 than Rio 2016 and we need to reflect that in the data!\n\n\nShow the code\ntotal_gold &lt;- sum(paris$Gold)\ntotal_silver &lt;- sum(paris$Silver)\ntotal_bronze &lt;- sum(paris$Bronze)\ntotal &lt;- sum(paris$Total)\n\nparis_perc &lt;- paris |&gt; \n  mutate(perc_Gold = (Gold/total_gold)*100,\n         perc_Silver = (Silver/total_silver)*100,\n         perc_Bronze = (Bronze/total_bronze)*100,\n         perc_Total = (Total/total)*100) |&gt; \n  select(country, ISO3c, perc_Gold:perc_Total)\n\nparis_perc\n\n\n# A tibble: 92 × 6\n   country        ISO3c perc_Gold perc_Silver perc_Bronze perc_Total\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 United States  USA       12.2        13.3        10.9       12.1 \n 2 China          CHN       12.2         8.18        6.23       8.72\n 3 Japan          JPN        6.08        3.64        3.38       4.31\n 4 Australia      AUS        5.47        5.76        4.16       5.08\n 5 France         FRA        4.86        7.88        5.71       6.13\n 6 Netherlands    NLD        4.56        2.12        3.12       3.26\n 7 United Kingdom GBR        4.26        6.67        7.53       6.23\n 8 South Korea    KOR        3.95        2.73        2.60       3.07\n 9 Italy          ITA        3.65        3.94        3.90       3.83\n10 Germany        DEU        3.65        3.94        2.08       3.16\n# ℹ 82 more rows"
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#predicted-performance",
    "href": "posts/2024-08-16-olympics-part2/index.html#predicted-performance",
    "title": "Paris 2024: Olympic debrief",
    "section": "Predicted performance",
    "text": "Predicted performance\n\nWe also know that medal performance is strongly correlated to a countries GDP. To assess current performance, we can model the relationship between GDP and Olympic results using data from previous years (1996 - 2016) and use it to predict outcomes for 2024. The difference between predicted and actual scores allows us to identify those countries that performed better (or worse) than expected.\nThe first sticking point we encounter is that Olympic performance is actually three variables: Gold, Silver, and Bronze medals. To model things more easily, we want to convert these into a single score. To make sure this single score reflects the difference in importance between different medal types we can use a weighted mean. Here we’ll consider gold medals to be 3x more valuable than silver, and silver as 3x more valuable than bronze.\n\\((Gold*\\frac{9}{13} + Silver*\\frac{3}{13} + Bronze*\\frac{1}{13})\\)\n\nhistorical_data_score &lt;- historical_data |&gt;\n  filter(Year &gt;= 1996) |&gt; \n  mutate(score = lay::lay(pick(c(perc_Gold, perc_Silver, perc_Bronze)),\n                               \\(x) sum(x * c(9/13, 3/13, 1/13))))\n\nRanking countries using this single Olympic Score™ returns similar results to using traditional medal rankings (see Appendix), and gives us a good metric to work with moving forward.\nNow that we have a single response variable for our model, we then need to add GDP as a predictor. In the previous blog we considered GDP in the current Olympic year. Because we don’t have official GDP statistics for 2024, let’s adjust this a bit to use GDP of the year preceding the Olympics. I expect GDP to be very strongly temporally auto-correlated (i.e. data from one year is related to data in the previous year) so this shouldn’t cause too many problems.\n\n\nShow the code\nGDP &lt;- readr::read_csv(\"./data/GDP_data.csv\", skip = 4, show_col_types = FALSE) |&gt; \n  ## Pivot longer to allow us to left join\n  tidyr::pivot_longer(cols = `1960`:`2023`, names_to = \"year\", values_to = \"gdp\") |&gt; \n  ## Convert year to year + 1 to join with Olympic data from the *following year*\n  mutate(year = as.numeric(year) + 1) |&gt;\n  janitor::clean_names() |&gt; \n  select(country_code:indicator_name, year, gdp) |&gt; \n  ## Remove countries with no GDP data\n  filter(!is.na(gdp)) |&gt; \n  mutate(gdp_bil = gdp/1e9) |&gt; \n  select(country_code, year, gdp_bil)\n\nhistorical_data_gdp &lt;- historical_data_score |&gt;\n  ## Join in GDP data\n  left_join(GDP, by = c(\"ISO3c\" = \"country_code\", \"Year\" = \"year\")) |&gt; \n  filter(!is.na(gdp_bil))\n\nprint(paste(\"Correlation Olympic score and GDP:\", round(cor(historical_data_gdp$score, historical_data_gdp$gdp_bil), 2)))\n\n\n[1] \"Correlation Olympic score and GDP: 0.76\"\n\n\nUnsurprisingly, our single Olympic Score™ is strongly correlated with GDP (0.76), with countries that have higher GDP tending to have a higher Olympic Score™.\nNow we can use the package spaMM to fit a linear mixed-effects model. We don’t need to go into the statistical details here, but basically this model allows us to estimate the effect of GDP on Olympic score but taking into account that each Olympic games has its own idiosyncrasies that will affect results. This model will be the basis for our Paris assessment.\n\nmodel &lt;- spaMM::fitme(score ~ gdp_bil + (1|Year), data = historical_data_gdp)"
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#actual-results",
    "href": "posts/2024-08-16-olympics-part2/index.html#actual-results",
    "title": "Paris 2024: Olympic debrief",
    "section": "Actual results",
    "text": "Actual results\n\nLet’s convert our Paris data into a comparable format to our historical data by calculating the single Olympic Score™ and adding GDP from the previous year. Then we can use our model from above to include expected medal score given known GDP.\n\n\nShow the code\nparis_perc_gdp &lt;- paris_perc |&gt; \n  mutate(score = lay::lay(pick(c(perc_Gold, perc_Silver, perc_Bronze)), \\(x) sum(x * c(9/13, 3/13, 1/13)))) |&gt; \n  left_join(GDP |&gt; \n              filter(year == 2023) |&gt; \n              select(-year), by = c(\"ISO3c\" = \"country_code\")) |&gt; \n  filter(!is.na(gdp_bil))\n\nparis_predicted &lt;- paris_perc_gdp\nparis_predicted$exp_score &lt;- as.numeric(predict(model, newdata = paris_predicted, re.form = NA))\n\nparis_predicted &lt;- paris_predicted |&gt; \n  ## Calculate absolute and relative difference between expected and actual score\n  mutate(score_diff_abs = score - exp_score,\n         score_diff_rel = score/exp_score)\n\nparis_predicted |&gt; \n  select(country, gdp_bil, score, exp_score)\n\n\n# A tibble: 88 × 4\n   country        gdp_bil score exp_score\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 United States    25700 12.3      26.1 \n 2 China            17900 10.8      18.3 \n 3 Japan             4260  5.31      4.53\n 4 Australia         1690  5.44      1.95\n 5 France            2780  5.62      3.04\n 6 Netherlands       1010  3.89      1.26\n 7 United Kingdom    3090  5.06      3.36\n 8 South Korea       1670  3.56      1.93\n 9 Italy             2070  3.73      2.33\n10 Germany           4080  3.59      4.35\n# ℹ 78 more rows\n\n\n\n\nShow the code\npred_data &lt;- data.frame(gdp_bil = seq(min(historical_data_gdp$gdp_bil), max(historical_data_gdp$gdp_bil) + 15000, length.out = 500))\npred_data$score &lt;- as.numeric(predict(model, newdata = pred_data, re.form = NA))\n\nimage_data &lt;- data.frame(ISO3c = c(\"USA\", \"CHN\"),\n                         imgURL = c(\"./data/USAflag.png\",\n                                    \"./data/CHNflag.png\"))\n\nparis_perc_gdp_png &lt;- paris_perc_gdp |&gt; \n  left_join(image_data, by = c(\"ISO3c\")) |&gt; \n  filter(!is.na(imgURL))\n\nggplot() +\n  geomtextpath::geom_textpath(data = pred_data |&gt; \n                                filter(gdp_bil &gt; 6500),\n                              aes(x = gdp_bil, y = score, label = \"Predicted Olympic score\"),\n                              text_only = TRUE, hjust = 0, vjust = 0, size = 3) +\n  geom_line(data = pred_data,\n            aes(x = gdp_bil, y = score)) +\n  geom_ribbon(data = pred_data,\n              aes(x = gdp_bil, ymin = score, ymax = Inf),\n              fill = \"darkgreen\", alpha = 0.25) +\n  geom_ribbon(data = pred_data |&gt; bind_rows(data.frame(gdp_bil = Inf,\n                                                       score = max(pred_data$score))),\n              aes(x = gdp_bil, ymin = score, ymax = -Inf),\n              fill = \"darkred\", alpha = 0.25) +\n  shadowtext::geom_shadowtext(aes(x = c(500, 40000),\n                                  y = c(10, 2),\n                                  label = c(\"Overperformers\", \"Underperformers\")),\n                              fontface = \"bold\", family = \"Chivo\",\n                              colour = c(\"darkgreen\", \"darkred\"),\n                              bg.colour = \"white\") +\n  geom_point(data = paris_perc_gdp,\n             aes(x = gdp_bil, y = score),\n             shape = 21, size = 1.25, stroke = 0.75) +\n  ggimage::geom_image(data = paris_perc_gdp_png,\n                      aes(x = gdp_bil, y = score, image = imgURL), size = 0.05) +\n  geom_text(data = paris_perc_gdp_png,\n            aes(x = gdp_bil*1.5, y = score, label = country),\n            fontface = \"bold\", family = \"Chivo\", hjust = 0, size = 3.5) +\n  labs(title = \"Paris 2024: Expectation v. Reality\",\n       subtitle = \"China and the US performed below expectations given their economic dominance\",\n       y = \"Olympic score (weighted mean of medal %)\", x = \"GDP (Billion $USD)\") +\n  scale_x_log10() +\n  coord_cartesian(ylim = c(-0.1, 15),\n                  xlim = c(min(paris_perc_gdp$gdp_bil) - 0.5, 500000), expand = FALSE) +\n  theme(plot.title = element_text(size = 20, family = \"Chivo\", face = \"bold\"),\n        plot.subtitle = element_text(size = 12, family = \"Chivo\"),\n        plot.margin = margin(t = 10, b = 10, l = 15, r = 15),\n        axis.title = element_text(size = 12, face = \"bold\", family = \"Chivo\"),\n        axis.title.x = element_text(margin = margin(t = 10)),\n        axis.title.y = element_text(margin = margin(r = 10)),\n        axis.text = element_text(colour = \"black\", size = 10, family = \"Chivo\"),\n        plot.background = element_rect(colour = \"black\"),\n        panel.grid = element_line(colour = \"white\"))\n\n\n\n\n\nIn the above plot, the solid black line shows the predicted Olympic score from our model based on country GDP. Each point shows the actual Olympic score of a country at Paris 2024. Points above the line (green area) show countries that scored higher than expected, given their GDP. Points below the line (red area) are countries that had lower scores than our model predicted.\nThe first thing we can see is that both China and the US underperformed at this years Olympics, given their economic clout. With only ~12% of Gold medals each, they fell well below the previous records they set as host nations in Beijing 2008 (China: 16.9%) and Atlanta 1996 (USA: 16.2%).\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to notice the log scale on the x-axis of the plots. Using a log scale is invaluable to show massive economies like the US on the same plot as much smaller economies; however, remember this means the x-axis does not increase linearly. We actually model a linear relationship between GDP and Olympic score (i.e. one $USD change in GDP always has the same effect), but this looks non-linear in the plots due to the log scale.\n\n\nBelow this headline, we can see other countries that greatly diverged from our modelled expectations. To look for the biggest over and under-performers at Paris 2024 we calculate the proportional difference between a countries actual score and the expected score from our GDP model.\n\n\nShow the code\nbest_and_worst &lt;- paris_predicted |&gt; \n  arrange(score_diff_rel) |&gt; \n  slice(1:5, (n()-4):n()) |&gt; \n  select(country:perc_Total, score, exp_score, score_diff_rel, score_diff_abs)\n\n## Plotting\nylim = c(-0.5, 5)\nxlim = c(20, 7500)\n\npred_data &lt;- data.frame(gdp_bil = seq(min(xlim), max(xlim) + 15000, length.out = 500))\npred_data$score &lt;- as.numeric(predict(model, newdata = pred_data, re.form = NA))\n\nimage_data &lt;- data.frame(ISO3c = best_and_worst$ISO3c,\n                         imgURL = paste0(\"./data/\", best_and_worst$ISO3c, \"flag.png\"))\n\nparis_perc_gdp_png &lt;- paris_perc_gdp |&gt; \n  left_join(image_data, by = c(\"ISO3c\")) |&gt; \n  filter(!is.na(imgURL)) |&gt; \n  mutate(x = case_when(ISO3c == \"QAT\" ~ gdp_bil*0.675,\n                       TRUE ~ gdp_bil * 1.1),\n         y = case_when(ISO3c == \"QAT\" ~ score - 0.2,\n                       TRUE ~ score + 0.2))\n\nggplot() +\n  geomtextpath::geom_textpath(data = pred_data |&gt; \n                                filter(gdp_bil &gt; 1000 & gdp_bil &lt; 7500),\n                              aes(x = gdp_bil, y = score, label = \"Predicted Olympic score\"),\n                              text_only = TRUE, hjust = 0, vjust = 0, size = 3) +\n  geom_line(data = pred_data,\n            aes(x = gdp_bil, y = score)) +\n  geom_ribbon(data = pred_data,\n              aes(x = gdp_bil, ymin = score, ymax = Inf),\n              fill = \"darkgreen\", alpha = 0.25) +\n  geom_ribbon(data = pred_data |&gt; bind_rows(data.frame(gdp_bil = Inf,\n                                                       score = max(pred_data$score))),\n              aes(x = gdp_bil, ymin = score, ymax = -Inf),\n              fill = \"darkred\", alpha = 0.25) +\n  shadowtext::geom_shadowtext(aes(x = c(500, 40000),\n                                  y = c(10, 2),\n                                  label = c(\"Overperformers\", \"Underperformers\")),\n                              fontface = \"bold\", family = \"Chivo\",\n                              colour = c(\"darkgreen\", \"darkred\")) +\n  geom_point(data = paris_perc_gdp,\n             aes(x = gdp_bil, y = score),\n             shape = 21, size = 1.25, stroke = 0.75, alpha = 0.25) +\n  ggimage::geom_image(data = paris_perc_gdp_png,\n                      aes(x = gdp_bil, y = score, image = imgURL), size = 0.075) +\n  geom_text(data = paris_perc_gdp_png,\n            aes(x = x, y = y, label = country),\n            fontface = \"bold\", family = \"Chivo\", hjust = 0, size = 3.5) +\n  labs(title = \"Paris 2024: Expectation v. Reality\",\n       subtitle = \"Uzbekistan scored 5x higher than expected, while India only achieved\\n5% of their expected Olympic score.\",\n       y = \"Olympic score (weighted mean of medal %)\", x = \"GDP (Billion $USD)\") +\n  scale_x_log10() +\n  coord_cartesian(ylim = ylim,\n                  xlim = xlim, expand = FALSE, clip = \"on\") +\n  theme(plot.title = element_text(size = 20, family = \"Chivo\", face = \"bold\"),\n        plot.subtitle = element_text(size = 12, family = \"Chivo\"),\n        plot.margin = margin(t = 10, b = 10, l = 15, r = 15),\n        axis.title = element_text(size = 12, face = \"bold\", family = \"Chivo\"),\n        axis.title.x = element_text(margin = margin(t = 10)),\n        axis.title.y = element_text(margin = margin(r = 10)),\n        axis.text = element_text(colour = \"black\", size = 10, family = \"Chivo\"),\n        plot.background = element_rect(colour = \"black\"))\n\n\n\n\n\n\nThe over and underperformers\nWe can now see those countries that most exceeded or fell short of their Olympic expectations. The outcome of the New Zealand campaign is a direct comparison to that of Qatar and Peru. In 2023, all three countries had a GDP around 250 billion $USD, yet New Zealand went home with 10 Gold medals while Qatar and Peru failed to win a single Gold or Silver medal.\nIndia stands out as a particularly poor performer. The world’s 5th largest economy and most populous nation failed to win a single Gold medal. The return of cricket to the Olympic stage for LA28 might present team India with an opportunity to improve their Olympic fortunes. In contrast, Uzbekistan had a particularly impressive performance, snagging 8 gold medals for boxing and martial arts and scoring almost 100x higher than Slovakia despite having a smaller economy.\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          Country == \"Olympics\" ~ \"EUN\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\nplot_data &lt;- best_and_worst |&gt; \n  mutate(score_diff_rel = score_diff_rel*100) |&gt; \n  select(country, ISO3c, score, exp_score, score_diff_rel) |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(flag_URL, country, score:score_diff_rel) |&gt; \n  arrange(desc(score_diff_rel))\n\nrel_score_palette &lt;- col_numeric(c(\"darkred\", \"darkgreen\"), domain = c(min(plot_data$score_diff_rel),\n                                                                                max(plot_data$score_diff_rel)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(country = \"Country\",\n             exp_score = \"Expected score\",\n             score = \"Actual score\",\n             score_diff_rel = \"% difference\") %&gt;% \n  tab_header(title = md(\"Paris 2024: Expectation v. Reality\")) %&gt;% \n  tab_source_note(source_note = md(\"Olympic data: www.sports-reference.com&lt;br&gt;GDP data: World Bank Group\")) %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(score_diff_rel),\n             fn = rel_score_palette) %&gt;%\n  fmt_number(columns = c(score, exp_score),\n             decimals = 2) %&gt;%\n  fmt_number(columns = c(score_diff_rel),\n             decimals = 1, suffixing = \"%\") %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(country) ~ px(125),\n             c(score, exp_score, score_diff_rel) ~ px(100)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Paris 2024: Expectation v. Reality\n    \n    \n    \n      \n      Country\n      Actual score\n      Expected score\n      % difference\n    \n  \n  \n    \nUzbekistan\n1.88\n0.33\n579.0\n    \nNew Zealand\n2.65\n0.49\n539.0\n    \nHungary\n1.87\n0.42\n443.8\n    \nGeorgia\n0.86\n0.27\n320.4\n    \nNetherlands\n3.89\n1.26\n308.2\n    \nSlovakia\n0.02\n0.36\n5.5\n    \nIndia\n0.17\n3.62\n4.7\n    \nQatar\n0.02\n0.48\n4.2\n    \nPeru\n0.02\n0.49\n4.1\n    \nSingapore\n0.02\n0.75\n2.7\n  \n  \n    \n      Olympic data: www.sports-reference.comGDP data: World Bank Group"
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#the-conclusion-the-winners-and-loser-board",
    "href": "posts/2024-08-16-olympics-part2/index.html#the-conclusion-the-winners-and-loser-board",
    "title": "Paris 2024: Olympic debrief",
    "section": "The Conclusion: The winners and loser board",
    "text": "The Conclusion: The winners and loser board\n\nCatchy headlines about record breaking medal tallies may seem catchy, but this is often just a consequence of the increasing number of sports and events being added to the Olympics (e.g. skateboarding, climbing). Even when countries do break new medal records (accounting for medal inflation!) a lot of these wins are only possible with hefty government investment which isn’t available in most places. Next time you’re looking at the Olympic medal tally, maybe scroll down a bit to spot those countries that are really punching above their weight.\nNext week, we can see what our Olympic assessment method can tell us about the results of Paris 2024."
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#appendix-assessing-our-single-olympic-score",
    "href": "posts/2024-08-16-olympics-part2/index.html#appendix-assessing-our-single-olympic-score",
    "title": "Paris 2024: Olympic debrief",
    "section": "Appendix: Assessing our single Olympic Score",
    "text": "Appendix: Assessing our single Olympic Score\n\nHow does our weighted Olympics Score™ compare to traditional medal tally ranks?\n\n\nShow the code\nflag_db &lt;- readr::read_csv(\"data/Country_Flags.csv\", show_col_type = FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code_raw = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE),\n         ## There are a few cases that need to be specified manually\n         Code = case_when(Country == \"Soviet Union\" ~ \"SUN\",\n                          Country == \"East Germany\" ~ \"DDR\",\n                          Country == \"Yugoslavia\" ~ \"YUG\",\n                          Country == \"Olympics\" ~ \"EUN\",\n                          TRUE ~ Code_raw)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\nscore_diffs &lt;- historical_data_score |&gt; \n  arrange(desc(Year), desc(perc_Gold), desc(perc_Silver), desc(perc_Bronze)) |&gt; \n  group_by(Year) |&gt; \n  mutate(trad_rank = 1:n()) |&gt; \n  ungroup() |&gt; \n  arrange(desc(Year), desc(score)) |&gt; \n  group_by(Year) |&gt; \n  mutate(score_rank = 1:n()) |&gt; \n  ungroup() |&gt; \n  mutate(score_diff = trad_rank - score_rank) |&gt; \n  arrange(desc(score_diff))\n\nplot_data &lt;- score_diffs |&gt; \n  slice(1:5, (n()-4):n()) |&gt; \n  left_join(flag_db, by = c(\"ISO3c\" = \"Code\")) |&gt; \n  select(flag_URL, Year, country, perc_Gold:score_diff)\n\nrank_change_palette &lt;- col_numeric(c(\"darkred\", \"white\", \"darkgreen\"), domain = c(min(score_diffs$score_diff),\n                                                                                  max(score_diffs$score_diff)), alpha = 0.75)\n\nplot_data |&gt; \n  gt() |&gt; \n  cols_label(country = \"Country\",\n             perc_Gold = \"Gold (%)\",\n             perc_Silver = \"Silver (%)\",\n             perc_Bronze = \"Bronze (%)\",\n             score = \"Score\",\n             trad_rank = \"Rank (traditional)\",\n             score_rank = \"Rank (score)\",\n             score_diff = \"Difference\") %&gt;% \n  tab_header(title = md(\"Largest changes in Olympic rank\"),\n             subtitle = paste0(\"Correlation: \",\n                               round(cor(score_diffs$trad_rank,\n                                         score_diffs$score_rank,\n                                         ## Kendall better suited for rank data\n                                         method = \"kendall\"), 2))) %&gt;% \n  tab_source_note(source_note = \"Data: www.sports-reference.com\") %&gt;% \n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style     = list(\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_title(groups = \"title\"),\n    style     = list(\n      cell_text(weight = \"bold\", size = 24)\n    )\n  ) %&gt;% \n  data_color(columns = c(score_diff),\n             fn = rank_change_palette) %&gt;%\n  fmt_number(columns = c(perc_Gold, perc_Silver, perc_Bronze, score),\n             decimals = 2) %&gt;%\n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(country) ~ px(175),\n             c(perc_Gold,\n               perc_Silver,\n               perc_Bronze,\n               score) ~ px(100)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\") |&gt; \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Largest changes in Olympic rank\n    \n    \n      Correlation: 0.99\n    \n    \n      \n      Year\n      Country\n      Gold (%)\n      Silver (%)\n      Bronze (%)\n      Score\n      Rank (traditional)\n      Rank (score)\n      Difference\n    \n  \n  \n    \n2000\nBrazil\n0.00\n2.00\n1.84\n0.60\n53\n32\n21\n    \n2000\nJamaica\n0.00\n2.00\n0.92\n0.53\n54\n36\n18\n    \n2008\nArmenia\n0.00\n0.00\n1.70\n0.13\n78\n61\n17\n    \n2016\nAzerbaijan\n0.33\n2.29\n2.78\n0.97\n39\n25\n14\n    \n2008\nCuba\n0.66\n3.64\n3.12\n1.54\n28\n15\n13\n    \n1996\nTurkey\n1.48\n0.37\n0.34\n1.13\n20\n25\n-5\n    \n2008\nRomania\n1.32\n0.33\n0.85\n1.06\n17\n23\n-6\n    \n2008\nEthiopia\n1.32\n0.33\n0.57\n1.04\n18\n24\n-6\n    \n2016\nGreece\n0.98\n0.33\n0.56\n0.79\n26\n33\n-7\n    \n2016\nArgentina\n0.98\n0.33\n0.00\n0.75\n27\n34\n-7\n  \n  \n    \n      Data: www.sports-reference.com\n    \n  \n  \n\n\n\n\nWe can see the most extreme differences between ranks with our Olympic score™ and traditional rankings in the table above. Our Olympic score tends to provide more rewards for lesser medals. For example, at Sydney 2000 Brazil failed to win any Golds, but received around 2% of both Silver and Bronze medals and is given a boost with our Olympic score. Conversely, at Rio 2016 Argentina won 3 gold medals (0.98% of all available Golds) but won very few other medals and so is ranked down using our Olympic score. Still, ranks using the different methods are highly correlated (Kendall’s \\(\\tau\\): 0.99) and 78% of countries see no rank change at all. This gives us some confidence to use our singular Olympic Score™ for further analyses."
  },
  {
    "objectID": "posts/2024-08-16-olympics-part2/index.html#the-conclusion-the-winners-board",
    "href": "posts/2024-08-16-olympics-part2/index.html#the-conclusion-the-winners-board",
    "title": "Paris 2024: Olympic debrief",
    "section": "The Conclusion: The winners board",
    "text": "The Conclusion: The winners board\n\nA simple model with national GDP is a fairly crude method to estimate Olympic performance. It doesn’t account for a countries level of development and how much of GDP is invested in sports. In the case of India, it also fails to consider whether the sports invested in (i.e. cricket) are included in the Olympics. Some metric like the UNDP Human Development Index might be one way to include more details about each country. Still, GDP does begin to account for a country’s availabe resources that could go towards training Olympic athletes, and does seem to provide some (basic) predictive power. Adding more sophisticated data would be ideal, but finding this type of data for close to 100 countries is easier said than done. Maybe we can reconvene again in 4 years time to refine and improve our Olympic modelling."
  }
]