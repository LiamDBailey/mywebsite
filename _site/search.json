[
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html",
    "title": "Advent of Code 2021",
    "section": "",
    "text": "See my solution for Day 1 here."
  },
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html#the-data",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here.\nFor day 2, our data are a set of directions that we can use to explore the ocean floor that we mapped out on Day 1. Each row of data contains two pieces of information: the direction to move (e.g. ‘forward’, ‘up’) and the distance (an integer). We’ll load the data in and then separate the direction and distance information.\n\nlibrary(readr)\n\n#Load data\nday2_data &lt;- readr::read_delim(file = \"./data/Day2.txt\", delim = \" \",\n                               col_names = c(\"direction\", \"distance\"), show_col_types = FALSE)\n\nhead(day2_data)\n\n# A tibble: 6 × 2\n  direction distance\n  &lt;chr&gt;        &lt;dbl&gt;\n1 forward          8\n2 forward          3\n3 forward          8\n4 down             6\n5 forward          3\n6 up               6"
  },
  {
    "objectID": "posts/2021-12-04-advent-of-code-day-2/index.html#the-challenges",
    "href": "posts/2021-12-04-advent-of-code-day-2/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\n\nChallenge 1\n\nStarting from (0,0) we need to follow each of the directions included in the data and determine our final location. We could simply do this using a for loop (probably the simpler solution), but I wanted to take this opportunity to practice writing recursive functions.\nLet’s build a function that runs through the data row by row and recalls itself each time until there is not data left. Unlike in a for loop, we don’t need to define the number of iterations at the beginning, we simply define the conditions under which the function will continue. Of course…we also need to be careful not to create an infinite loop.\n\nOur function will take a data input, a vector of start coordinates c(0,0), and three functions that will be applied to the start coordinates (this will be important for challenge 2).\n\nmove &lt;- function(data, start,\n                 forward_func,\n                 up_func,\n                 down_func){\n\n  #Separate the first line of data and the remaining data\n  input  &lt;- data[1, ]\n  remain &lt;- data[-1, ]\n\n  #Depending on the directions, apply a difference function\n  new_coord &lt;- switch(input$direction,\n                      forward = forward_func(start = start, distance = input$distance),\n                      down = down_func(start = start, distance = input$distance),\n                      up = up_func(start = start, distance = input$distance))\n\n  #If there are still rows of data remaining, recall the function\n  if (nrow(remain) &gt; 0) {\n\n    Recall(data = remain, start = new_coord,\n           forward_func = forward_func,\n           up_func = up_func,\n           down_func = down_func)\n\n  } else {\n\n    #If we've gone through all rows of data, return the coordinate\n    return(new_coord)\n\n  }\n\n}\n\nWe’ll create some very simply input functions that we can feed into our move() function.\n\n#Write funcs to do each process\n#Forward moves on the x axis\nforward &lt;- function(start, distance){\n  start[1] &lt;- start[1] + distance\n  return(start)\n}\n\n#Up will *decrease* depth\nup &lt;- function(start, distance){\n  start[2] &lt;- start[2] - distance\n  return(start)\n}\n\n#Down will *increase* depth\ndown &lt;- function(start, distance){\n  start[2] &lt;- start[2] + distance\n  return(start)\n}\n\nNow we can run our recursive function.\n\nfinal_position &lt;- move(data = day2_data, start = c(0, 0),\n                       forward_func = forward,\n                       up_func = up,\n                       down_func = down)\n\n#Return the product, which is our answer\nprod(final_position[1], final_position[2])\n\n[1] 1383564\n\n\n\n\n\nChallenge 2\n\nFor the second challenge, the function used for each direction type has changed. We now also need to deal with an ‘aim’ value that affects our forward movement. This aim value can be the third value in our start vector. Luckily, we’ve already written our recursive function, so this is as simple as substituting in new functions for forward, up, and down.\n\nforward_new &lt;- function(start, distance){\n  start[1] &lt;- start[1] + distance\n  start[2] &lt;- start[2] + distance*start[3]\n  return(start)\n}\n\nup_new &lt;- function(start, distance){\n  start[3] &lt;- start[3] - distance\n  return(start)\n}\n\ndown_new &lt;- function(start, distance){\n  start[3] &lt;- start[3] + distance\n  return(start)\n}\n\n\nfinal_position &lt;- move(data = day2_data, start = c(0, 0, 0),\n                       forward_func = forward_new, up_func = up_new, down_func = down_new)\n\n#Return the product, which is our answer\nprod(final_position[1], final_position[2])\n\n[1] 1488311643\n\n\nBy putting in a bit of effort to make a robust function at the beginning we were able to solve both challenges. It’s a good example of how writing good functions can save you a lot of time and effort later on."
  },
  {
    "objectID": "posts/2021-12-09-advent-of-code-day-6/index.html#the-data",
    "href": "posts/2021-12-09-advent-of-code-day-6/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nFor Day 6 we have to model the growth of a population of fish…this is much closer to my comfort zone. See the explanation for today’s challenge here. We are given a vector of integers that represents the time since last reproduction for every fish. When a fish reaches a value of 0 it will reproduce with a clutch size of 1 (which seems a tad too few for a fish…but this is a coding challenge not a biology challenge). The inter-birth interval of an adult is 6 days and the age at first reproduction for a newly born fish is 8 days. Now we have all the information, let’s get started.\n\n\n#Load data\nday6_data &lt;- as.integer(scan(file = \"./data/Day6.txt\", sep = \",\"))\n\nday6_data\n\n  [1] 5 1 1 5 4 2 1 2 1 2 2 1 1 1 4 2 2 4 1 1 1 1 1 4 1 1 1 1 1 5 3 1 4 1 1 1 1\n [38] 1 4 1 5 1 1 1 4 1 2 2 3 1 5 1 1 5 1 1 5 4 1 1 1 4 3 1 1 1 3 1 5 5 1 1 1 1\n [75] 5 3 2 1 2 3 1 5 1 1 4 1 1 2 1 5 1 1 1 1 5 4 5 1 3 1 3 3 5 5 1 3 1 5 3 1 1\n[112] 4 2 3 3 1 2 4 1 1 1 1 1 1 1 2 1 1 4 1 3 2 5 2 1 1 1 4 2 1 1 1 4 2 4 1 1 1\n[149] 1 4 1 3 5 5 1 2 1 3 1 1 4 1 1 1 1 2 1 1 4 2 3 1 1 1 1 1 1 1 4 5 1 1 3 1 1\n[186] 2 1 1 1 5 1 1 1 1 1 3 2 1 2 4 5 1 5 4 1 1 3 1 1 5 5 1 3 1 1 1 1 4 4 2 1 2\n[223] 1 1 5 1 1 4 5 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 4 2 1 1 1 2 5 1 4 1 1 1 4 1\n[260] 1 5 4 4 3 1 1 4 5 1 1 3 5 3 1 2 5 3 4 1 3 5 4 1 3 1 5 1 4 1 1 4 2 1 1 1 3\n[297] 2 1 1 4"
  },
  {
    "objectID": "posts/2021-12-09-advent-of-code-day-6/index.html#the-challenges",
    "href": "posts/2021-12-09-advent-of-code-day-6/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFirst off, we need to determine how many individuals would be in the population after 80 days (assuming no mortality in adults or offspring). The first way we might think to do this is just brute force. Keep the data in its original format and count the number of 0s in every time step to identify reproducers.\n\nfinal_pop80 &lt;- day6_data\n\nsystem.time({for (i in 1:80){\n  \n  #At the start of the time step, work out how many individuals reproduce\n  reproducer_location &lt;- final_pop80 == 0\n  \n  #Reduce the time of all individuals by 1\n  final_pop80 &lt;- final_pop80 - 1\n  \n  #Reset the age of reproducers to 6\n  final_pop80[reproducer_location] &lt;- 6\n  \n  #For every reproducer, add a new individual with a value of 8.\n  final_pop80 &lt;- append(final_pop80, rep(8, times = sum(reproducer_location)))\n  \n}})\n\n   user  system elapsed \n  0.026   0.010   0.037 \n\n\nThis ran in no time and we can quickly get our answer! It seems that the brute force approach works fine.\n\nlength(final_pop80)\n\n[1] 377263\n\n\n\n\nChallenge 2\n\nFor challenge 2 we need to push things a bit further and predict the population after 256 days. Could our brute force approach still work for this? The worrying detail is that without any mortality and a constant rate of reproduction this population of fish is going to be growing exponentially so the size of our integer is going to get very unmanageable very quickly. This is exactly what happens if we try our brute force approach, eventually maxing out our memory and crashing R. I’ve run the code below to show you how much trouble it has but I’ve capped it at 1min run time.\n\nfinal_pop &lt;- day6_data\n\nR.utils::withTimeout({for (i in 1:256){\n    \n    #At the start of the time step, work out how \n    reproducer_location &lt;- final_pop == 0\n    \n    #Reduce the time of all individuals by 1\n    final_pop &lt;- final_pop - 1\n    \n    #Reset the age of reproducers to 6\n    final_pop[reproducer_location] &lt;- 6\n    \n    #For every reproducer, add a new individual with a value of 8.\n    final_pop &lt;- append(final_pop, rep(8, times = sum(reproducer_location)))\n  }\n    #Timeout after 60 seconds\n  }, timeout = 60)\n\n[2023-05-08 18:20:28] TimeoutException: reached elapsed time limit [cpu=60s,\nelapsed=60s]\n\n\nIt seems we need another method. The key here is that (at least in this simplified fish population) every individual with the same number can be grouped together. They have no other distinguishing characteristics. So, instead of starting with a vector of 300 integers, we can group all individuals into age classes and instead just work with a vector of length 9 (values 0 - 8).\n\n#Turn our vector into a table\ntable_of_ages &lt;- table(day6_data)\n\n#Extend this to include all possible ages.\n#Make a named vector so we can easily index it\nages_vector &lt;- tidyr::replace_na(as.vector(table_of_ages[as.character(0:8)]), 0)\nnames(ages_vector) &lt;- 0:8\n\nages_vector\n\n  0   1   2   3   4   5   6   7   8 \n  0 168  31  29  37  35   0   0   0 \n\n\nThe values in our vector might change, but the length of the vector will never grow because individuals cannot have a value outside of this range. Now we can run very similar code to above but using our new age classes rather than individuals.\n\npop_size &lt;- as.numeric(NULL)\nsystem.time({for (i in 1:256){\n\n  reproducers &lt;- as.numeric(ages_vector[\"0\"])\n\n  ages_vector[as.character(0:7)] &lt;- ages_vector[as.character(1:8)]\n\n  ages_vector[\"6\"] &lt;- as.numeric(ages_vector[\"6\"]) + reproducers\n  ages_vector[\"8\"] &lt;- reproducers\n\n  pop_size &lt;- append(pop_size, sum(ages_vector))\n\n}})\n\n   user  system elapsed \n  0.005   0.000   0.004 \n\n\nAs you can see, our final population size is very large…no wonder we couldn’t use the brute force approach.\n\npop_size[length(pop_size)]\n\n[1] 1695929023803\n\n\n\n\nBONUS ROUND\n\nUnlike the data we used for this challenge, in reality when studying a population of animals we probably don’t know the exact age of every individual or the precise rules that might govern their reproduction. In many cases, our data might just be population counts. What could we do then?\nIn this example, we are given information that the population is growing exponentially and this can be modelled using the exponential growth model:\n\\[N_{t+1} = RN_{t}\\]\nWhere the population at time t (\\(N_{t}\\)) will grow by a constant reproduction factor (\\(R\\)). This parameter \\(R\\) will be different for every animal and population, but we can estimate it by looking at the relationship between \\(N_{t}\\) and \\(N_{t+1}\\) in the data we have already. Let’s imagine we only have population size data from the first 100 days. We can use a linear model (lm()) to estimate the relationship between \\(N_{t}\\) and \\(N_{t+1}\\) in this subset of data.\nNote: We fix the intercept of the model below to be 0 by including ~ 0 + ... in the model formula. This normally wouldn’t be advised, but in this case we know a priori that when \\(N_{t}\\) is 0, \\(N_{t+1}\\) must also be 0. An extinct population can’t grow!!\n\ninput_data &lt;- data.frame(nt = pop_size[1:100]) %&gt;% \n  mutate(nt_1 = lead(nt))\n\nour_model &lt;- lm(nt_1 ~ 0 + nt, data = input_data)\n  \nsummary(our_model)\n\n\nCall:\nlm(formula = nt_1 ~ 0 + nt, data = input_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57015  -1033    -39    766  59762 \n\nCoefficients:\n   Estimate Std. Error t value            Pr(&gt;|t|)    \nnt 1.091493   0.002916   374.4 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14150 on 98 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.9993 \nF-statistic: 1.401e+05 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe parameter estimate for nt in our model summary above is an estimation of the reproduction factor (\\(R\\)) for this population. We can now determine the population size at any time using the below equation:\n\\[N(t) = N_{0}R^t\\]\nWhere \\(N_{0}\\) is our starting population size and \\(t\\) is time. How does our population estimate from this exponential model compare to our calculation with exact individual level data?\n\n#Extract R value\nR &lt;- our_model$coefficients[1]\n\n#Write out exponential func to estimate N at any time point\nNt_func &lt;- function(N0, R, t){\n  \n  N0*R^t\n  \n}\n\n#Run this function from 1 - 256 days\npredicted_N &lt;- Nt_func(N0 = length(day6_data), R, t = 1:256)\n\n\n#Percentage difference between our population estimates\n(diff(c(predicted_N[256], pop_size[256]))/pop_size[256])*100\n\n[1] 4.268097\n\n\nAlthough we didn’t have any information about the age of individuals or their exact reproductive behaviour in our exponential growth model we were able to estimate a population size that was only 4% different from the exact value we calculated where we had complete information about every individual. This is an example where estimating a value using a mathematical model can provide a plausible alternative in cases where we don’t know all the details about a particular system.\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "",
    "text": "🕵️ Another week, another TidyTuesday! As before, this figure was created during the CorrelAid TidyTuesday meetup where we brainstorm and troubleshoot ideas. You can see the plots from all the CorrelAid TidyTuesday meetups here."
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#introduction-the-bechdel-test-and-james-bond",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#introduction-the-bechdel-test-and-james-bond",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Introduction: The Bechdel test and James Bond",
    "text": "Introduction: The Bechdel test and James Bond\n\nIn 1985, cartoonist Alison Bechdel published a cartoon that described 3 ‘rules’ used to rate female representation in movies. The movie must:\n\nHave at least 2 named women in it.\nThese women talk with each other.\nThe women talk about something other than a man.\n\nSounds pretty simple, right? But very few films (fewer than 50%) pass the eponymous Bechdel test, including major blockbusters like “The Hobbit: An Unexpected Journey” and “The Avengers”.\nOne film series that we would not expect to do particularly well on the Bechdel test is that of James Bond. Since the release of Dr. No in 1962, 007 has never been known for his healthy relationship to women. But how do the Bond films actually compare to movies at large, and how have they changed over time?"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#setup-data-and-packages",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#setup-data-and-packages",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Setup: Data and packages",
    "text": "Setup: Data and packages\n\nThis week’s TidyTuesday included two datasets taken from FiveThirtyEight, one complete dataset including Bechdel test scores for 8,839 movies since 1888 and a smaller subset with additional movie data from IMDB. The packages used for this post were:\n\nlibrary(tidytuesdayR) #To download the data\nlibrary(dplyr) #For data wrangling\nlibrary(ggplot2) #For plotting\nlibrary(fuzzyjoin) #For string matching\nlibrary(showtext) #Use fonts stored on the Google Fonts database\n\n## For adding images to our plot\nlibrary(png) #For reading .png files\nlibrary(gridExtra) #For turning .png files into grobs\nlibrary(egg) #For adding grob objects to our plot\n\nTo start with we need to download the data and separate out the different datasets.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 11)\n\n\n    Downloading file 1 of 2: `raw_bechdel.csv`\n    Downloading file 2 of 2: `movies.csv`\n\nbechdel &lt;- tuesdata$raw_bechdel\nmovies  &lt;- tuesdata$movies\n\nWe’ll create a custom ggplot theme so we don’t need to make these aesthetic changes in every plot.\n\nmy_theme &lt;- function(){\n  \n  theme_classic() %+replace% \n    #Remove legend by default\n    theme(legend.position = \"none\",\n          #Make axes black\n          axis.text = element_text(colour = \"black\"),\n          #Make clear titles and caption\n          plot.title = element_text(size = 18, face = \"bold\", hjust = 0),\n          plot.caption = element_text(size = 8, hjust = 1))\n  \n}"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#exploration-how-does-the-data-look",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#exploration-how-does-the-data-look",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Exploration: How does the data look?",
    "text": "Exploration: How does the data look?\n\nFor this visualisation, we’re only really interested in the full Bechdel test dataset and don’t need the more detailed movie information. Rather than treating the Bechdel test as pass/fail, each movie will be given a score from 0 (less than 2 named women) to 3 (passes the full Bechdel test). The full Bechdel test dataframe is fairly simple and should be easy to use. The title, rating, and year columns will be important to help us identify Bond (and non-Bond movies) and there don’t appear to be any nasty surprises in the data structure!\n\ntail(bechdel)\n\n# A tibble: 6 × 5\n   year    id imdb_id  title                    rating\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                     &lt;dbl&gt;\n1  2021  9473 10332588 Finding ’Ohana                3\n2  2021  9498 5144174  The Dry                       3\n3  2021  9505 10919362 Sweetheart                    3\n4  2021  9501 10813940 Ginny and Georgia             2\n5  2021  9504 5109280  Raya and the Last Dragon      3\n6  2021  9500 9286908  High Ground                   2\n\n\nIf we want to focus on Bond movies, we need to include some external data to identify which films are in the Bond franchise. We’ll consider all Bond movies except the ‘spy-parody’ Casino Royale from 1967. We’ll use the fuzzyjoin package to join datasets using movie titles. Fuzzy joining will allow us to match similar titles even if they’re not exactly the same (e.g. missing punctuation or different capitalisation).\n\nbond_movies &lt;- read.csv(\"./data/james_bond_movies.csv\") %&gt;% \n  #We use fuzzyjoin incase there are slight differences in the titles (e.g. missing ')\n  fuzzyjoin::stringdist_left_join(bechdel, by = c(\"movie\" = \"title\")) %&gt;% \n  #Remove old Casino Royale and select relevant columns\n  dplyr::filter(!(movie == \"Casino Royale\" & year == \"1967\")) %&gt;% \n  dplyr::select(movie, year, actor, bond_rating = rating) %&gt;% \n  #Unfortunately one movie (The Living Daylights) doesn't have a Bechdel score.\n  #We still want to include this in our graph even if it doesn't have a score so we need to manually add the year\n  mutate(year = case_when(movie == \"The Living Daylights\" ~ 1987,\n                          TRUE ~ year))\n\nhead(bond_movies)\n\n                            movie year          actor bond_rating\n1                          Dr. No 1962   Sean Connery           1\n2           From Russia with Love 1963   Sean Connery           3\n3                      Goldfinger 1964   Sean Connery           1\n4                     Thunderball 1965   Sean Connery           2\n5             You Only Live Twice 1967   Sean Connery           1\n6 On Her Majesty's Secret Service 1969 George Lazenby           2"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Initial plots: What can we see when we start plotting?",
    "text": "Initial plots: What can we see when we start plotting?\n\nWe now have Bechdel scores of all (but one) of the Bond movies. To start exploring we can create a very rough plot to look at the average Bechdel score of each James Bond actor.\n\nggplot(data = bond_movies) +\n  geom_boxplot(aes(x = actor, y = bond_rating, fill = actor),\n               colour = \"black\", width = 0.15, alpha = 0.25) +\n  #Just use viridis fill for now\n  scale_fill_viridis_d() +\n  scale_y_continuous(breaks = c(1, 2, 3)) +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"Bechdel score of each Bond\",\n       subtitle = \"\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  my_theme() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\nWe can see there are differences in the median Bechdel score of each James Bond actor, but this initial plot is a bit lacking. Are the differences we see related to the number of movies each actor starred in? Or are we seeing a cultural shift over time as more movies pass the Bechdel test? We need to disentangle these possibilities.\nFirstly, we want to look at the Bechdel score of all other movies during the time James Bond movies were being made.\n\nother_movies &lt;- bechdel %&gt;% \n  #Remove all Bond movies\n  fuzzyjoin::stringdist_anti_join(bond_movies, by = c(\"title\" = \"movie\")) %&gt;%\n  #Only consider movies in the years since Bond movies started\n  dplyr::filter(year &gt;= min(bond_movies$year) & year &lt;= max(bond_movies$year))\n\n\nrollingmean &lt;- other_movies %&gt;% \n  #Take a mean of Bechdel score for each year\n  dplyr::group_by(year) %&gt;% \n  dplyr::summarise(all_movie_rating = mean(rating),\n                   n = n()) %&gt;% \n  #Create a 3 year rolling mean\n  dplyr::mutate(lag1 = lag(all_movie_rating),\n                lead1 = lead(all_movie_rating)) %&gt;% \n  dplyr::rowwise() %&gt;% \n  dplyr::mutate(rollingmean_allmovies = mean(c(all_movie_rating, lag1, lead1))) %&gt;% \n  dplyr::select(-lag1, -lead1) %&gt;% \n  #Remove years where a rolling mean is not possible\n  dplyr::filter(!is.na(rollingmean_allmovies))\n\nWe next need to define the Bond ‘eras’ (the periods during which each actor played James Bond).\n\nbond_eras &lt;- bond_movies %&gt;% \n  group_by(actor) %&gt;% \n  summarise(start = min(year),\n            end = max(year),\n            midyear = mean(c(start, end)),\n            mean_score = mean(bond_rating, na.rm = TRUE))\n\nbond_eras\n\n# A tibble: 6 × 5\n  actor          start   end midyear mean_score\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Daniel Craig    2006  2015   2010.       1.5 \n2 George Lazenby  1969  1969   1969        2   \n3 Pierce Brosnan  1995  2002   1998.       2   \n4 Roger Moore     1973  1985   1979        1.43\n5 Sean Connery    1962  1971   1966.       1.5 \n6 Timothy Dalton  1987  1989   1988        2   \n\n\nNow we can plot the mean Bechdel scores over time and compare it to the mean Bechdel score of each Bond actor!\n\nggplot() +\n  #Adjust the location so that movies fall in the middle of a year rather than at the start\n  geom_rect(data = bond_eras, aes(xmin = start, xmax = end + 1, ymin = -Inf, ymax = Inf, fill =  actor),\n            colour = NA) +\n  geom_path(data = rollingmean, aes(x = year + 0.5, y = rollingmean_allmovies), size = 1, colour = \"blue\") +\n  geom_point(data = bond_eras, aes(x = midyear, y = mean_score), size = 3, shape = 21, fill = \"grey50\") +\n  scale_fill_viridis_d(name = \"\") +\n  scale_x_continuous(breaks = seq(1960, 2020, 5), name = \"\") +\n  scale_y_continuous(breaks = c(1, 2, 3)) +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"Bechdel score of each Bond\",\n       subtitle = \"\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  my_theme() +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#digging-deeper-adding-images",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#digging-deeper-adding-images",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Digging deeper: Adding images",
    "text": "Digging deeper: Adding images\n\nWhile this graph serves our purposes, I don’t think it’s the ideal format to present the data. For one, by using year on the x-axis, we include many years where James Bond movies were not being made. Another problem we encounter is that the Bond ‘eras’ are not sequential, because one actor (George Lazenby) made a movie during the Sean Connery era. As an alternative, we can use the sequence of Bond movies as the x-axis. We can then compare the mean Bechdel score of each Bond to the mean Bechdel score of contemporary movies during their time in the role.\n\n#Specify the era 'order' of the actors\nbond_order &lt;- data.frame(actor = c(\"Sean Connery\",\n                                   \"George Lazenby\",\n                                   \"Roger Moore\",\n                                   \"Timothy Dalton\",\n                                   \"Pierce Brosnan\",\n                                   \"Daniel Craig\")) %&gt;%\n  mutate(era_nr = 1:n())\n\nbond_era_ratings &lt;- bond_movies %&gt;%\n  left_join(bond_order, by = \"actor\") %&gt;% \n  arrange(era_nr, year) %&gt;% \n  #Have a movie number that can be used as our x axis\n  tibble::rownames_to_column(var = \"movie_nr\") %&gt;%\n  #Separate first name and surname\n  tidyr::separate(actor, into = c(\"given_name\", \"surname\"), sep = \" \") %&gt;% \n  group_by(surname) %&gt;% \n  summarise(mean_bond_rating = mean(bond_rating, na.rm = TRUE),\n            first_yr = first(year),\n            #The first and last number are buffered by 0.5 on either side\n            #This allows George Lazanby (with 1 movie) to still be visible\n            first_nr = as.integer(first(movie_nr)) - 0.5,\n            last_yr = last(year),\n            last_nr = as.integer(last(movie_nr)) + 0.5,\n            middle_point = mean(c(first_nr, last_nr))) %&gt;% \n  #For each actor, determine the mean Bechdel score of all other movies during their era\n  mutate(mean_movie_rating = purrr::map2_dbl(.x = first_yr, .y = last_yr,\n                                             .f = function(first, last, movie_db){\n                                               \n                                               movie_db_subset &lt;- other_movies %&gt;% \n                                                 filter(year &gt;= first & year &lt;= last)\n                                               \n                                               mean(movie_db_subset$rating)\n                                               \n                                             }, movie_db = other_movies),\n         year_text = case_when(first_yr == last_yr ~ as.character(first_yr),\n                               TRUE ~ paste(first_yr, last_yr, sep = \"-\")))\n\n#Although we use movie number as our x axis, we want the labels to be the movie names\n#and the movie's Bechdel score\nx_labels &lt;- bond_movies %&gt;%\n  left_join(bond_order, by = \"actor\") %&gt;% \n  arrange(era_nr, year) %&gt;% \n  tibble::rownames_to_column(var = \"movie_nr\") %&gt;% \n  mutate(movie_nr = as.integer(movie_nr),\n         movie = paste0(movie, ' (', bond_rating, ')'))\n\n\nggplot(data = bond_era_ratings) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = \"black\", alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 1.5) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            size = 5) +\n  geom_point(aes(x = middle_point, y = mean_bond_rating),\n             size = 6, shape = 21, stroke = 2, fill = \"grey50\") + \n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.1,\n                label = surname), size = 7) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.2,\n                label = round(mean_bond_rating, 2)), size = 7) +\n  scale_x_continuous(breaks = x_labels$movie_nr, labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  coord_cartesian(expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, colour = \"black\", size = 15),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.text.y = element_text(size = 15, colour = \"black\"),\n        axis.title = element_text(size = 20),\n        plot.title = element_text(size = 25, face = \"bold\"),\n        plot.subtitle = element_text(size = 17))\n\n\nUsing points to show the ratings of different actors works well enough, but perhaps headshots of the different actors would be more effective. For this, we use the png and grid packages to create a grob, or ‘graphical object’. grobs come from the grid package and also underlie ggplot2 and grobs can be inserted directly into our plot using the geom_custom function from the egg package.\nFirst, we need to convert the .png files into grobs and store this in a dataframe.\n\n#List all .png files\nall_png &lt;- data.frame(file = list.files(\"./data\", pattern = \".png\", full.names = TRUE)) %&gt;%\n  #Extract surname of each actor from the png file path\n  mutate(surname = stringr::str_to_title(stringr::str_sub(basename(file), end = -5))) %&gt;% \n  #For each file, read it in as a .png and then convert to a grob\n  rowwise() %&gt;% \n  mutate(grob = list(grid::rasterGrob(png::readPNG(source = file),\n                                      width = unit(3, \"cm\"), height = unit(3, \"cm\"))))\n\n#Join data frame with grobs into original data\nbond_era_ratings_png &lt;- bond_era_ratings %&gt;% \n  left_join(all_png, by = \"surname\")\n\nThen we can add these png derived grobs into the plot in place of points.\n\nggplot(data = bond_era_ratings_png) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = \"black\", alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 1.5) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            size = 5) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.2,\n                label = round(mean_bond_rating, 2)), size = 7) +\n  egg::geom_custom(aes(x = middle_point, y = mean_bond_rating, data = grob), grob_fun = \"identity\") +\n  scale_x_continuous(breaks = seq(1, 24, 1), labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"Bechdel score\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\",\n       caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  coord_cartesian(expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, colour = \"black\", size = 15),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.text.y = element_text(size = 15, colour = \"black\"),\n        axis.title = element_text(size = 20),\n        plot.title = element_text(size = 25, face = \"bold\"),\n        plot.subtitle = element_text(size = 17),\n        plot.caption = element_text(size = 20))"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#the-finishing-touches-making-things-look-pretty",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#the-finishing-touches-making-things-look-pretty",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "The finishing touches: Making things look pretty",
    "text": "The finishing touches: Making things look pretty\n\nWe’ll now add some custom fonts and additional explanation to the plot. To add additional annotation to a plot I like to again take advantage of grobs. We can make our whole plot into a grob and place it inside a new (empty) plotting environment where the annotation can be included, using the geom_annotate function. With this method you don’t need to mess around with your plot margins, and it’s particularly effective if you want to include multiple plots together or create custom legends.\n\n#Load a font from Google Fonts\nsysfonts::font_add_google(name = \"Mouse Memoirs\", family = \"Mouse\")\nsysfonts::font_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\n#Specify that the showtext package should be used\n#for rendering text\nshowtext::showtext_auto()\n\n\nmain_plot &lt;- ggplot(data = bond_era_ratings_png) +\n  geom_rect(aes(xmin = first_nr, xmax = last_nr, ymin = -Inf, ymax = Inf, fill = surname),\n            colour = NA, alpha = 0.5) +\n  geom_segment(aes(x = first_nr + 0.25, xend = last_nr - 0.25, y = mean_movie_rating, yend = mean_movie_rating), size = 1.5) +\n  geom_text(aes(x = middle_point, y = 2.9, label = year_text),\n            family = \"Mouse\", size = 30/.pt) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.155,\n                label = surname), family = \"Mouse\", colour = \"#D53129\", size = 30/.pt) +\n  geom_text(aes(x = middle_point, y = mean_bond_rating - 0.23,\n                label = format(round(mean_bond_rating, 2), nsmall = 2)), family = \"Mouse\",\n            size = 30/.pt) +\n  geom_text(aes(x = middle_point, y = mean_movie_rating + 0.05,\n                label = format(round(mean_movie_rating, 2), nsmall = 2)), family = \"Mouse\",\n            size = 30/.pt) +\n  egg::geom_custom(aes(x = middle_point, y = mean_bond_rating, data = grob), grob_fun = \"identity\") +\n  scale_x_continuous(limits = c(0, NA), breaks = x_labels$movie_nr, labels = x_labels$movie) +\n  scale_y_continuous(limits = c(1, 3), breaks = 1:3) +\n  scale_fill_manual(values = c(\"grey50\", \"grey50\", \"white\", \"white\", \"white\", \"grey50\"), name = \"\") +\n  scale_colour_viridis_d(name = \"\") +\n  labs(x = \"\", y = \"&lt;- Worse                       Bechdel score                       Better -&gt;\",\n       title = \"How problematic is James Bond?\",\n       subtitle = \"Bechdel test score of Bond movies compared to movies from the same period\") +\n  coord_fixed(ratio = 7, expand = FALSE) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 30,\n                                   family = \"Mouse\", colour = \"black\"),\n        axis.text.y = element_text(family = \"Mouse\", colour = \"black\", size = 30),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.title = element_text(family = \"Mouse\", size = 40),\n        plot.title = element_text(family = \"Mouse\", size = 50, margin = margin(b = 15)),\n        plot.subtitle = element_text(size = 30, margin = margin(b = 30)))\n\nmain &lt;- ggplot2::ggplotGrob(main_plot)\n\nlabels &lt;- data.frame(x = 0.87, y = c(0.8, 0.375),\n                     text = c(\"Movies from the same period\",\n                              \"Score of Bond movies\"))\n\nggplot()+\n  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +\n  annotation_custom(grob = main, xmin = 0, xmax = 0.75, ymin = 0, ymax = 1) +\n  geom_curve(aes(x = 0.825, xend = 0.7, y = 0.8, yend = 0.725), curvature = 0.2,\n             arrow = arrow(length = unit(0.1, \"inches\")), size = 2) +\n  geom_curve(aes(x = 0.825, xend = 0.7, y = 0.375, yend = 0.45), curvature = -0.2,\n             arrow = arrow(length = unit(0.1, \"inches\")), size = 2) +\n  geom_label(data = labels, aes(x = x, y = y, label = text),\n                        family = \"Mouse\", colour = '#D53129', size = 30/.pt) +\n  labs(caption = \"Data: FiveThirtyEight | Plot: @ldbailey255\") +\n  theme_void() +\n  theme(plot.caption = element_text(family = \"Ubuntu Mono\", size = 20))"
  },
  {
    "objectID": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#conclusion",
    "href": "posts/2021-03-24-tidytuesday-week-11-2021-james-bond-s-bechdel-score-and-adding-images-in-ggplot/index.html#conclusion",
    "title": "TidyTuesday - Week 11 (2021)",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThe mean Bechdel score of movies has increased steadily since the first Bond movie was released in 1962, but Bond movies haven’t kept up! James Bond films have never been known for their realistic portrayal of female characters, but it seems that the newest Bond movies with Daniel Craig are particularly bad at passing the Bechdel test when compared to their contemporaries.\nOf course, the Bechdel test has been rightly criticised as a very simple method for assessing female representation and ignores other issues, such as the role of the female characters in the film, the amount of dialogue given to women, and the representation of women of colour. There are now a plethora of new tests that seek to provide a better assessment of the role of women both in front of and behind the camera. Even so, it’s striking that all but one of Daniel Craig’s films as James Bond fails to even include dialogue between two female characters! Will the upcoming No Time To Die be any better? Will a new James Bond actor herald a change to the franchise’s Bechdel scores? Or is 007 just a bit out of date?"
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html",
    "title": "Advent of Code 2021",
    "section": "",
    "text": "It’s that time of year again. Houses are lit up, presents are being bought, and Michael Bublé has emerged from his hibernation to create another album of Christmas songs. This year, in the spirit of giving, I will share my code for the 2021 Advent of Code challenges. Each day is a new small coding challenge and I’ll attempt to upload and explain all my solutions! If I miss posting on any days (or you just want to look at the code in more detail) check out my GitHub repo."
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html#the-data",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nWe are given a numeric vector containing values of ocean depth. I’ll read it in as a data frame here.\n\nlibrary(readr)\n\nday1_data &lt;- read_delim(file = \"./data/Day1.txt\", delim = \"/t\",\n                        col_names = \"depth\", show_col_types = FALSE)\n\nhead(day1_data)\n\n# A tibble: 6 × 1\n  depth\n  &lt;dbl&gt;\n1   191\n2   185\n3   188\n4   189\n5   204\n6   213"
  },
  {
    "objectID": "posts/2021-12-02-advent-of-code-day-1/index.html#the-challenges",
    "href": "posts/2021-12-02-advent-of-code-day-1/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\n\nChallenge 1\n\nFor the first challenge we need to compare values along a vector and determine whether they increase or decrease. We then need to determine the number of times the depth value has increased (i.e. ocean floor became deeper). This one’s pretty straight forward, as we can just use the lead() function from dplyr to compare our vector of depths to its neighbour.\n\n#Each value in the vector is compared to its neighbour. Return logical to check whether depth has increased\nlogical_output &lt;- day1_data$depth &lt; lead(day1_data$depth)\n\n#Example output\nlogical_output[1:10]\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n\n#Final result\nsum(logical_output, na.rm = TRUE)\n\n[1] 1709\n\n\n\n\n\nChallenge 2\n\nFor the second challenge we need to first create a new vector using a sliding window of size 3 (the sum of each value and its next two neighbours). Then we make the same check to see how often depth has increased using this newly created vector. This isn’t too much harder as we can use the n argument in lead() to return the 1st and 2nd neighbours.\n\n#Create sliding window vector with window size 3\nnew_vector &lt;- day1_data$depth + lead(day1_data$depth, n = 1) + lead(day1_data$depth, n = 2)\nsum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n\n[1] 1761\n\n\n\n\n\nBONUS ROUND!\n\nThis works, but this type of code always feels a bit inflexible to me. What if needed to change the size of the window? We would need to re-write our definition of new_vector every time. What if the size of the sliding window becomes very large? The definition for new_vector would also become very long and difficult to follow. Is there a functional way we can deal with this?\nFirstly, I’ll consider an approach using the tidyverse where I work in a data frame. I create a loop with purrr to add any number of new columns each with a different lead value. You might notice the walrus operator (:=) which is used in the tidyverse to create dynamic column names. I then use the package lay and its eponymous function to efficiently apply a function (sum()) across rows of a data frame. You’ll need to install the lay package from GitHub.\n\n#remotes::install_github(\"courtiol/lay\")\nlibrary(lay)\nlibrary(purrr)\n\n#Create a function that can take any window size\ntidy_windowfunc &lt;- function(data, window_size = 3){\n  \n  new_vector &lt;- data %&gt;%\n    #Window includes the existing depth col, so we add window_size - 1 new columns\n    mutate(map_dfc(.x = (1:window_size) - 1,\n                   .f = function(i, depth){\n                     \n                     #Create a new column with lag i\n                     #Allow for dynamic col names\n                     tibble(\"depth_lag{i}\" := lead(depth, n = i))\n                     \n                   }, depth = .data$depth)) %&gt;%\n    #Sum all the newly created lag columns using lay\n    mutate(depth_sum = lay(across(.cols = contains(\"lag\")), .fn = sum)) %&gt;% \n    pull(depth_sum)\n  \n  sum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n  \n}\n\nNow we can solve both puzzles using just one function!\n\n#Recreate Challenge 1\ntidy_windowfunc(data = day1_data, window_size = 1) == sum(day1_data$depth &lt; lead(day1_data$depth), na.rm = TRUE)\n\n[1] TRUE\n\n#Recreate Challenge 2\nnew_vector &lt;- day1_data$depth + lead(day1_data$depth, n = 1) + lead(day1_data$depth, n = 2)\ntidy_windowfunc(data = day1_data, window_size = 3) == sum(new_vector &lt; lead(new_vector), na.rm = TRUE)\n\n[1] TRUE\n\n\nThis was the tidyverse way, but can we do it any better using base R? My first thought here would be to try a while() loop, that continues adding new neighbours until window size is reached.\n\n#Create similar function with base R\nbase_windowfunc &lt;- function(data, window_size = 3){\n  \n  i &lt;- 1\n  depth      &lt;- data$depth\n  new_vector &lt;- data$depth\n  \n  #As long as we haven't reached window size, keep going!\n  while (i &lt; window_size) {\n    \n    #Add depth data shifted by i to our existing vector.\n    #Will ensure vectors are same length (adds NAs at the end)\n    new_vector &lt;- new_vector + depth[1:length(depth) + i]\n    i &lt;- i + 1\n    \n  }\n  \n  sum(new_vector[1:(length(new_vector) - 1)] &lt; new_vector[2:length(new_vector)], na.rm = TRUE)\n  \n}\n\nThis works too and, as with most base R code, it’s 100% dependency free!\n\n#Recreate Challenge 1\ntidy_windowfunc(data = day1_data, window_size = 1) == base_windowfunc(data = day1_data, window_size = 1)\n\n[1] TRUE\n\n#Recreate Challenge 2\ntidy_windowfunc(data = day1_data, window_size = 3) == base_windowfunc(data = day1_data, window_size = 3)\n\n[1] TRUE\n\n\nSo which one is faster? I would assume that by working exclusively with vectors our base R function will be faster than our tidy function that spends some time manipulating a data frame. Is this actually the case? And if so, how much faster do we get? Let’s look at the number of iterations/sec for each function and different window sizes using the mark() function in the bench package.\n\nlibrary(bench)\n\nbench_df &lt;- map_df(.x = 1:20,\n                   .f = function(i){\n                     \n                     times &lt;- mark(tidy_windowfunc(data = day1_data, window_size = i),\n                                   base_windowfunc(data = day1_data, window_size = i))\n                     \n                     data.frame(method = c(\"tidy\", \"base\"),\n                                size = i,\n                                number_per_sec = times$`itr/sec`,\n                                speed_sec = 1/times$`itr/sec`)\n                     \n                   })\n\nggplot(bench_df) +\n  geom_line(aes(x = size, y = number_per_sec, colour = method), size = 1) +\n  geom_point(aes(x = size, y = number_per_sec, colour = method), stroke = 1.5, shape = 21, fill = \"white\") +\n  scale_y_continuous(name = \"Iterations/sec\",\n                     breaks = seq(0, 15000, 5000), limits = c(0, 15000)) +\n  scale_x_continuous(name = \"Window size\") +\n  scale_colour_discrete(name = \"\") +\n  labs(title = \"Advent of Code Day 1\",\n       subtitle = \"base v. tidyverse approach\") +\n  theme_classic(base_family = \"Courier New\") +\n  theme(axis.text = element_text(colour = \"black\"),\n        legend.position = c(0.8, 0.8),\n        legend.background = element_rect(colour = \"black\"),\n        legend.title = element_blank())\n\n\nAs expected, base R is more efficient, especially if we only need to work with short windows; however, this advantage decreases rapidly as we start to work with larger windows. Although it is slower, I think the tidyverse method I’ve used here is also very versatile, particularly if you want to return a data frame and not just a vector. Either way…they both get the correct result!\nStay tuned for days 2 - 25"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html",
    "title": "Building maps using OpenStreetMap",
    "section": "",
    "text": "When I first started making maps and doing spatial analyses in R I often encountered a problem finding the data I needed. Where could I find a line showing state borders or a polygon of a nearby river? There are of course a huge number of ways to source this type of information, but for a beginner (and even sometimes as somebody more experienced) it can be difficult to keep track of all the different data sources. That’s why I was so excited to find out about the osmdata package that allows you to query OpenStreetMap data directly in R. Access to an almost unlimited amount of spatial information from across the globe, what’s not to like?!\nTo show off the power of OpenStreetMap data I’ll build a train map of Melbourne (my home town). Here’s what we’ll need:\n\nosmdata: For querying OpenStreetMap\nggplot2: For plotting the information we extract\nsf: For working with spatial data in R\ndplyr: For data wrangling\n\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#where-are-the-shapefiles",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#where-are-the-shapefiles",
    "title": "Building maps using OpenStreetMap",
    "section": "",
    "text": "When I first started making maps and doing spatial analyses in R I often encountered a problem finding the data I needed. Where could I find a line showing state borders or a polygon of a nearby river? There are of course a huge number of ways to source this type of information, but for a beginner (and even sometimes as somebody more experienced) it can be difficult to keep track of all the different data sources. That’s why I was so excited to find out about the osmdata package that allows you to query OpenStreetMap data directly in R. Access to an almost unlimited amount of spatial information from across the globe, what’s not to like?!\nTo show off the power of OpenStreetMap data I’ll build a train map of Melbourne (my home town). Here’s what we’ll need:\n\nosmdata: For querying OpenStreetMap\nggplot2: For plotting the information we extract\nsf: For working with spatial data in R\ndplyr: For data wrangling\n\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#creating-a-query",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#creating-a-query",
    "title": "Building maps using OpenStreetMap",
    "section": "Creating a query",
    "text": "Creating a query\n\nThe first step is to specify the area within which we want to search for OpenStreetMap features. We can specify the latitude and longitude limits manually, but you can also use the getbb() function to return the limits of a particular place (e.g. a city).\n\n#Example of a bounding box for Melbourne\n#It's often necessary to specify the greater city limit to ensure the\n#bounding box is large enough\nmelb_bb &lt;- getbb(place_name = \"Melbourne, Australia\")\n\nmelb_bb\n\n        min       max\nx 144.44405 146.19250\ny -38.49937 -37.40175\n\n\nThen we can use the opq() function to start querying the OpenStreetMap API. You can build the query using pipes.\nThe first thing we’ll want to add to the query is the type of feature we want to return with add_osm_feature(). There are a lot of possible objects we could extract from OpenStreetMap. In this case we want to find train lines and stations. The key-value pair to use is not always obvious (e.g. a cycle path is under the ‘highway’ key), so you may want to explore different key-value pairs here or search for specific features on Nominatim.\nTIP: If you’re querying over a large area it can be good to increase the timeout argument from the default 25.\n\n#Query railway lines\nmelb_query_line &lt;- opq(bbox = melb_bb, timeout = 120) %&gt;% \n  add_osm_feature(key = 'route', value = 'train')\n\nmelb_query_station &lt;- opq(bbox = melb_bb, timeout = 120) %&gt;% \n  add_osm_feature(key = 'railway', value = 'station')\n\nmelb_query_line\n\n$bbox\n[1] \"-38.49937,144.44405,-37.40175,146.1925\"\n\n$prefix\n[1] \"[out:xml][timeout:120];\\n(\\n\"\n\n$suffix\n[1] \");\\n(._;&gt;;);\\nout body;\"\n\n$features\n[1] \"[\\\"route\\\"=\\\"train\\\"]\"\n\n$osm_types\n[1] \"node\"     \"way\"      \"relation\"\n\nattr(,\"class\")\n[1] \"list\"           \"overpass_query\"\nattr(,\"nodes_only\")\n[1] FALSE"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#extracting-some-data",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#extracting-some-data",
    "title": "Building maps using OpenStreetMap",
    "section": "Extracting some data",
    "text": "Extracting some data\n\nWe now have a query for the OpenStreetMap Overpass API…but we still need to run the query and return the spatial data! For this we can use osmdata_sf() to return information as sf objects (or osmdata_sp() if you’re working with the sp package).\n\nmelbourne_trainline &lt;- melb_query_line %&gt;% \n  osmdata_sf()\n\nmelbourne_station &lt;- melb_query_station %&gt;% \n  osmdata_sf()\n\nmelbourne_trainline\n\nObject of class 'osmdata' with:\n                 $bbox : -38.49937,144.44405,-37.40175,146.1925\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 50231 points\n            $osm_lines : 'sf' Simple Features Collection with 4258 linestrings\n         $osm_polygons : 'sf' Simple Features Collection with 338 polygons\n       $osm_multilines : 'sf' Simple Features Collection with 158 multilinestrings\n    $osm_multipolygons : NULL\n\n\nWe now have a whole range of objects (lines, points, and polygons), that fit our specific key-value pairs. We can already use these data to create a basic map with train lines and stations!\n\nmelbourne_trainline_lines &lt;- melbourne_trainline$osm_lines\nmelbourne_station_points  &lt;- melbourne_station$osm_points\n\nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points, size = 1, shape = 21, colour = \"black\", fill = \"dark grey\") +\n  theme_void()"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#tidying-up-osm-data",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#tidying-up-osm-data",
    "title": "Building maps using OpenStreetMap",
    "section": "Tidying up OSM data",
    "text": "Tidying up OSM data\n\nThis doesn’t look quite right! Although we queried within the greater Melbourne area, the lines and polygons can extend outside this bounding box. We can deal with this a number of ways. Here we’ll use a combination of trim_osmdata() from within osmdata to clip our lines and the coord_sf() function to adjust the limits of plot to match the bounding box of Melbourne we’ve been using.\n\nmelb_bb_poly &lt;- getbb(place_name = \"Melbourne, Australia\",\n                      format_out = \"sf_polygon\") %&gt;% \n#There are two very similar bounding boxes. We'll use the first one.\n  slice(1)\n\nmelbourne_trainline_lines &lt;- melbourne_trainline_lines %&gt;% \n  sf::st_filter(melb_bb_poly)\n  #Use exclude = FALSE to include lines that partially overlap our boundaries\n  # trim_osmdata(bb_poly = melb_bb_poly, exclude = FALSE)\n\nmelbourne_station_points &lt;- melbourne_station_points %&gt;% \n  sf::st_filter(melb_bb_poly)\n  # trim_osmdata(bb_poly = melb_bb_poly, exclude = FALSE)\n\n# melbourne_trainline_lines &lt;- melbourne_trainline_trim$osm_lines\n# melbourne_station_points  &lt;- melbourne_station_trim$osm_points\n\nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points, size = 1, shape = 21, colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])\n\n\n\n\nUsing OpenStreetMap gives us access to a huge source of spatial information, but because the data is created by a community of volunteers it can often require some cleaning. In our case, we can see there are a few stations that do not have corresponding train lines. This may occur because the train lines have been stored with a different key-value combination or are stored as a different object type, such as a polygon. For now, we will only include stations that are close (less than 1km) from a train line.\nNote: We do not use overlap because this will require points to sit exactly on the lines, which will often not occur due to things such as measurement error in the point locations.\n\n#For each station, determine the distance to all train lines\nmin_dist &lt;- sf::st_distance(melbourne_station_points,\n                            melbourne_trainline_lines) %&gt;% \n  #Determine the minimum distance for each station\n  apply(MARGIN = 1, FUN = min)\n\nmelbourne_station_points_subset &lt;- melbourne_station_points %&gt;% \n  dplyr::mutate(dist = min_dist) %&gt;% \n  dplyr::filter(dist &lt;= 1000)\n  \nggplot() +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"black\") +\n  geom_sf(data = melbourne_station_points_subset, size = 1, shape = 21,\n          colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#filtering-by-name",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#filtering-by-name",
    "title": "Building maps using OpenStreetMap",
    "section": "Filtering by name",
    "text": "Filtering by name\n\nWe now have a cleaned dataset of trainlines and stations within greater Melbourne, now we can work to make this map more informative by adding other features, like rivers and water bodies. We can also source this information from OpenStreetMap.\nWe want to add the Yarra river the bays and ocean around Melbourne to our map. Rivers can be found using the waterway-river key pair, but we need to apply additional filters to make sure we don’t get all rivers in Melbourne. To do this, we can apply a second filter with add_osm_feature() to search for objects with a specific name. By using value_exact = FALSE we allow for possible spelling differences in the name of the objects. Looking at Nominatim, we can see that the water bodies near Melbourne are all encompassed by the ‘Bass Strait’ multi-polygon (found with the natural-strait key pair).\n\nbass_strait &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'natural', value = 'strait') %&gt;% \n  add_osm_feature(key = \"name\", value = \"Bass Strait\", value_exact = FALSE) %&gt;%\n  osmdata_sf()\n\nbass_strait_polygon &lt;- bass_strait$osm_multipolygons\n\nbays &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'natural', value = 'bay') %&gt;% \n  osmdata_sf()\n\nbays_polygon &lt;- bays$osm_multipolygons\n\nyarra &lt;- opq(bbox = melb_bb, timeout = 240) %&gt;% \n  add_osm_feature(key = 'waterway', value = 'river') %&gt;% \n  add_osm_feature(key = \"name\", value = \"Yarra River\", value_exact = FALSE) %&gt;%\n  osmdata_sf()\n\nyarra_line &lt;- yarra$osm_multilines\n\nggplot() +\n  geom_sf(data = bass_strait_polygon, fill = \"blue\", colour = NA) +\n  geom_sf(data = bays_polygon, fill = \"blue\", colour = NA) +\n  geom_sf(data = yarra_line, colour = \"blue\", size = 1.5) +\n  geom_sf(data = melbourne_trainline_lines, size = 1, colour = \"gray\") +\n  geom_sf(data = melbourne_station_points_subset, size = 2, shape = 21,\n          colour = \"black\", fill = \"dark grey\") +\n  theme_void() +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ])"
  },
  {
    "objectID": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#making-things-pretty",
    "href": "posts/2020-12-02-building-maps-with-openstreetmap-content/index.html#making-things-pretty",
    "title": "Building maps using OpenStreetMap",
    "section": "Making things pretty",
    "text": "Making things pretty\n\nWe now have the data we need to create a nice looking map of the Melbourne metro-train network. We can add some nice aesthetic touches to make it look nicer. For this we’ll load some packages:\n\nshowtext: Adding custom fonts from Google fonts\n\n\nlibrary(showtext)\n\nfont_add_google(\"Alice\", \"Alice\")\nfont_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\nshowtext_auto()\n\n\nggplot() +\n  geom_sf(data = bass_strait_polygon, fill = \"#33658A\", colour = NA) +\n  geom_sf(data = bays_polygon, fill = \"#33658A\", colour = NA) +\n  geom_sf(data = yarra_line, colour = \"#33658A\", size = 1.5) +\n  geom_sf(data = melbourne_trainline_lines, size = 1.2, colour = \"gray40\") +\n  #Add a label for port phillip bay and bass strait\n  geom_text(aes(label = \"Port Phillip Bay\"), x = 144.859280, y = -38.092014,\n            family = \"Alice\", fontface = \"bold\", colour = \"grey90\") +\n  geom_text(aes(label = \"Bass Strait\"), x = 144.513047, y = -38.421423,\n            family = \"Alice\", fontface = \"bold\", colour = \"grey90\") +\n  #Add title as text inside the plot\n  geom_text(aes(label = \"- Trains of Melbourne -\"), x = 145.75, y = -37.43,\n            family = \"Alice\", size = 8,\n            colour = \"grey45\", fontface = \"bold\") +\n  geom_text(aes(label = \"Using OpenStreetMap data\"), x = 145.75, y = -37.5,\n            family = \"Alice\", size = 4,\n            colour = \"grey45\", fontface = \"bold\") +\n  labs(caption = \"Data: OSM | Plot: @ldbailey255\") +\n  coord_sf(xlim = melb_bb[1, ], ylim = melb_bb[2, ]) +\n  theme_classic() +\n  theme(panel.background = element_rect(fill = \"#f2eadf\"),\n        panel.border = element_rect(fill = NA, colour = \"grey45\", size = 1.5),\n        axis.text = element_text(family = \"Alice\", size = 14),\n        plot.caption = element_text(family = \"Ubuntu Mono\", size = 12,\n                                  colour = \"grey45\"),\n        axis.line = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html",
    "title": "Making beautiful tables with the gt package",
    "section": "",
    "text": "I dread encountering a data table in an academic paper. A jumble of numbers, column sub-headers, and confusing footnotes, seemingly bereft of purpose. Why, I cry, is this not in the supplementary material?! Surely, I think, they could’ve created a graph! Despite my prejudice, the fact is that we often do need to use tables to present data. While a plot is a beautiful way to show trends or patterns, it falls short if your goal is to compare individual values. This year saw the release of the gt package, a new addition to the growing number of packages that can produce publication quality tables in R. It seemed like as good a time as any to overcome my dread of tables. Here is my attempt."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-dreaded-table",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-dreaded-table",
    "title": "Making beautiful tables with the gt package",
    "section": "",
    "text": "I dread encountering a data table in an academic paper. A jumble of numbers, column sub-headers, and confusing footnotes, seemingly bereft of purpose. Why, I cry, is this not in the supplementary material?! Surely, I think, they could’ve created a graph! Despite my prejudice, the fact is that we often do need to use tables to present data. While a plot is a beautiful way to show trends or patterns, it falls short if your goal is to compare individual values. This year saw the release of the gt package, a new addition to the growing number of packages that can produce publication quality tables in R. It seemed like as good a time as any to overcome my dread of tables. Here is my attempt."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-data",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-data",
    "title": "Making beautiful tables with the gt package",
    "section": "The data",
    "text": "The data\n\nAs a test case I decided to work with data on greenhouse gas emissions from aviation travel, recently published in Our World in Data. Hannah Ritchie does a great job explaining the complexity of calculating per capita aviation emissions and I would highly recommend giving her article a read. For now, though, we’ll side-step this complexity and focus on the simplest case, emissions from domestic travel. The map from Hannah’s post clearly shows the range and disparity in emissions across the globe, but if we want to compare individual values between countries, or maybe within continents, such a map isn’t the best option. How about a table? While the original post includes a basic table to compare emissions, I wanted to see whether I could create a table of my own using only R. A table that I might be happy to encounter in a publication."
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-basics-clean-the-data-and-add-some-context",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-basics-clean-the-data-and-add-some-context",
    "title": "Making beautiful tables with the gt package",
    "section": "The basics: Clean the data and add some context",
    "text": "The basics: Clean the data and add some context\n\nWe’ll need dplyr to tidy some of the data and gt to build our tables. scales will be used for creating colour palettes.\n\nlibrary(dplyr)\nlibrary(scales)\nlibrary(gt)\npackageVersion(\"gt\")\n\n[1] '0.8.0'\n\n\ngt integrates well into the existing tidyverse, and creating a gt table is as simple as two lines of code.\nNOTE: For this example we’re just showing the worst emitters.\n\n#Load data and arrange in descending order of emissions\nemissions_data &lt;- read.csv(here::here(\"posts/2020-11-27-making-beautiful-tables-with-gt/assets/per-capita-co2-domestic-aviation.csv\")) %&gt;% \n  arrange(desc(Per.capita.domestic.aviation.CO2))\n\n#Generate a gt table from head of data\nhead(emissions_data) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\nEntity\nCode\nPer.capita.domestic.aviation.CO2\n\n\n\n\nUnited States\nUSA\n0.3855204\n\n\nAustralia\nAUS\n0.2671672\n\n\nNorway\nNOR\n0.2092302\n\n\nNew Zealand\nNZL\n0.1741885\n\n\nCanada\nCAN\n0.1682674\n\n\nJapan\nJPN\n0.0739585\n\n\n\n\n\n\n\nA table, yes, but not one you’re likely to publish! To take our first steps towards table perfection we need to tidy up the data and add a title and data source. The reader needs to understand what’s going on. While much of the tidying could be completed using dplyr before converting to a table I’ll demonstrate how this could be achieved exclusively inside gt.\nTIP: Notice how we select columns using the generic c(). For larger, more complex data tidy-select functions like starts_with() or contains() may come in handy.\n\n(emissions_table &lt;- head(emissions_data) %&gt;% \n   gt() %&gt;% \n   #Hide unwanted columns\n   cols_hide(columns = c(Code)) %&gt;% \n   #Rename columns\n   cols_label(Entity = \"Country\",\n              Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n   #Add a table title\n   #Notice the `md` function allows us to write the title using markdown syntax (which allows HTML)\n   tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n   #Add a data source footnote\n   tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\"))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (tonnes)\n    \n  \n  \n    United States\n0.3855204\n    Australia\n0.2671672\n    Norway\n0.2092302\n    New Zealand\n0.1741885\n    Canada\n0.1682674\n    Japan\n0.0739585\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\n    \n  \n  \n\n\n\n\nA few lines of code and the table is already much better. To me there is still one issues that needs to be fixed before we’ve finished the basics. While it’s technically correct to report emissions in tonnes I feel the data would be much more suitable in kilograms. For this we’ll use fmt_number().\nTIP: There are fmt_xxx() functions for many different data types, including fmt_currency() and fmt_date().\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #Format numeric column. Use `scale_by` to divide by 1,000. (Note: we'll need to rename the column again)\n   fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n              scale_by = 1000) %&gt;%\n   #Our second call to cols_label overwrites our first\n   cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\"))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-touchup-adding-some-style-and-colour",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-touchup-adding-some-style-and-colour",
    "title": "Making beautiful tables with the gt package",
    "section": "The touchup: Adding some style and colour",
    "text": "The touchup: Adding some style and colour\n\nWe’ve got a working table, but it won’t be winning any points for style. Our next step is to change the style of different cells to help the reader more clearly find the information they’re after. For this we’ll use the tab_style() function. The style choices here follow some of the ‘Ten Guidelines for Better Tables’ from Jon Schwabish.\nFirstly, we need to more clearly distinguish between the column headers and the body of the table (and while we’re at it, also the title!)\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #Apply new style to all column headers\n   tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       #Give a thick border below\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       #Make text bold\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   #Apply different style to the title\n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\n    \n  \n  \n\n\n\n\nAs our reader is interested in comparing emissions values between countries, we can add a heatmap to our cells to more clearly show the differences. This will require us to set a colour palette and apply a conditional colouring using data_color(). We’ll use the same palette employed in Hannah Ritchie’s map above.\n\n#Apply our palette explicitly across the full range of values so that the top countries are coloured correctly\nmin_CO2 &lt;- min(emissions_data$Per.capita.domestic.aviation.CO2)\nmax_CO2 &lt;- max(emissions_data$Per.capita.domestic.aviation.CO2)\nemissions_palette &lt;- col_numeric(c(\"#FEF0D9\", \"#990000\"), domain = c(min_CO2, max_CO2), alpha = 0.75)\n\n(emissions_table &lt;- emissions_table %&gt;% \n    data_color(columns = c(Per.capita.domestic.aviation.CO2),\n               colors = emissions_palette))\n\n\n\n\n\n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-finer-details-table-options",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-finer-details-table-options",
    "title": "Making beautiful tables with the gt package",
    "section": "The finer details: Table options",
    "text": "The finer details: Table options\n\nWe’ve added some colour and style, but if we really want to customise our table and tweak the fine details we need to start using the opt_xxx() and tab_options() functions. The opt_xxx() functions adjust specific elements of the table, while tab_options() is similar to the theme() function used with ggplot2. Below, we’ll adjust the options to resemble tables from fivethirtyeight (working from Thomas Mock’s great blog).\n\n(emissions_table &lt;- emissions_table %&gt;% \n   #All column headers are capitalised\n   opt_all_caps() %&gt;% \n   #Use the Chivo font\n   #Note the great 'google_font' function in 'gt' that removes the need to pre-load fonts\n   opt_table_font(\n     font = list(\n       google_font(\"Chivo\"),\n       default_fonts()\n     )\n   ) %&gt;%\n   #Change the width of columns\n   cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n              c(Entity) ~ px(400)) %&gt;% \n   tab_options(\n     #Remove border between column headers and title\n     column_labels.border.top.width = px(3),\n     column_labels.border.top.color = \"transparent\",\n     #Remove border around table\n     table.border.top.color = \"transparent\",\n     table.border.bottom.color = \"transparent\",\n     #Reduce the height of rows\n     data_row.padding = px(3),\n     #Adjust font sizes and alignment\n     source_notes.font.size = 12,\n     heading.align = \"left\"\n   ))\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    United States\n385.52\n    Australia\n267.17\n    Norway\n209.23\n    New Zealand\n174.19\n    Canada\n168.27\n    Japan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-1-adding-images",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-1-adding-images",
    "title": "Making beautiful tables with the gt package",
    "section": "Bonus round 1: Adding images",
    "text": "Bonus round 1: Adding images\n\nI’m already really happy with this table. It clearly allows a comparison of exact emissions between countries, and the colour provides additional assistance to the reader. We could leave it here, but for a little bonus let’s add country flags to our plot. We can use flags sources from wikicommons (a database for with URL locations is available through data.world).\nTo link this flag data base to our emissions data we need to translate country names into 3-letter country codes. We can do this using the countrycode package. The code for this is not really relevant for using gt but you can see it below if you’re interested. The end goal is to create a column containing the URL of a flag image for each country.\n\n\nDetails…\n\n\n#To convert country codes\nlibrary(countrycode)\n\nflag_db &lt;- read.csv(\"assets/Country_Flags.csv\") %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE)) %&gt;% \n  select(Code, flag_URL = ImageURL)\n\nflag_data &lt;- emissions_data %&gt;% \n  left_join(flag_db, by = \"Code\") %&gt;% \n  select(flag_URL, Entity, everything())\n\n#We'll need to refit our table using this new data\n#Code below with comments removed.\nemissions_table &lt;- head(flag_data) %&gt;% \n  gt() %&gt;% \n  cols_hide(columns = c(Code)) %&gt;% \n  cols_label(Entity = \"Country\",\n             Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n  tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n  tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\") %&gt;% \n  fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n             scale_by = 1000) %&gt;%\n  cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\") %&gt;% \n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ) %&gt;% \n  data_color(columns = c(Per.capita.domestic.aviation.CO2),\n             colors = emissions_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n             c(Entity) ~ px(400)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\")\n\n\n\n\nhead(flag_data)\n\n                                                                                   flag_URL\n1              https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg\n2 https://upload.wikimedia.org/wikipedia/commons/8/88/Flag_of_Australia_%28converted%29.svg\n3                    https://upload.wikimedia.org/wikipedia/commons/d/d9/Flag_of_Norway.svg\n4               https://upload.wikimedia.org/wikipedia/commons/3/3e/Flag_of_New_Zealand.svg\n5                         https://upload.wikimedia.org/wikipedia/en/c/cf/Flag_of_Canada.svg\n6                          https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg\n         Entity Code Per.capita.domestic.aviation.CO2\n1 United States  USA                        0.3855204\n2     Australia  AUS                        0.2671672\n3        Norway  NOR                        0.2092302\n4   New Zealand  NZL                        0.1741885\n5        Canada  CAN                        0.1682674\n6         Japan  JPN                        0.0739585\n\n\nWe can now use the text_transform() function to add our country flags. text_transform() allows us to apply any custom function to a column. We can use the web_image function in gt to convert a URL to an embedded image.\n\nemissions_table %&gt;% \n  gt::text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\")\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    \nUnited States\n385.52\n    \nAustralia\n267.17\n    \nNorway\n209.23\n    \nNew Zealand\n174.19\n    \nCanada\n168.27\n    \nJapan\n73.96\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-2-within-group-comparison",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#bonus-round-2-within-group-comparison",
    "title": "Making beautiful tables with the gt package",
    "section": "Bonus round 2: Within-group comparison",
    "text": "Bonus round 2: Within-group comparison\n\nIt’s pretty clear by now that the US and Australia are the worst for domestic aviation emissions, but what if we wanted to compare within continents. Which country has the highest emissions within Africa or S. America? For this, we can use the row grouping functionality in gt.\nBefore we can do this, we’ll need to assign each country to its corresponding continent. As above, this isn’t really gt relevant, but the code is included below if you’re interested.\n\n\nDetails…\n\n\ncontinent_data &lt;- flag_data %&gt;% \n  #Convert iso3 codes to FIPS\n  mutate(continent = countrycode(sourcevar = Code, origin = \"iso3c\", destination = \"continent\", warn = FALSE)) %&gt;% \n  select(continent, flag_URL, Entity, Per.capita.domestic.aviation.CO2)\n\n\n\n\nhead(continent_data)\n\n  continent\n1  Americas\n2   Oceania\n3    Europe\n4   Oceania\n5  Americas\n6      Asia\n                                                                                   flag_URL\n1              https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg\n2 https://upload.wikimedia.org/wikipedia/commons/8/88/Flag_of_Australia_%28converted%29.svg\n3                    https://upload.wikimedia.org/wikipedia/commons/d/d9/Flag_of_Norway.svg\n4               https://upload.wikimedia.org/wikipedia/commons/3/3e/Flag_of_New_Zealand.svg\n5                         https://upload.wikimedia.org/wikipedia/en/c/cf/Flag_of_Canada.svg\n6                          https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg\n         Entity Per.capita.domestic.aviation.CO2\n1 United States                        0.3855204\n2     Australia                        0.2671672\n3        Norway                        0.2092302\n4   New Zealand                        0.1741885\n5        Canada                        0.1682674\n6         Japan                        0.0739585\n\n\nIn this case, we need to start from the beginning apply grouping to our table before we begin.\n\n(emissions_table_continent &lt;- continent_data %&gt;%\n  #Just take the top 5 from each continent for our example\n  group_by(continent) %&gt;% \n  slice(1:5) %&gt;% \n  #Just show Africa and Americas for our example\n  filter(continent %in% c(\"Africa\", \"Americas\")) %&gt;%\n  #Group data by continent\n  gt(groupname_col = \"continent\") %&gt;% \n  #Add flag images as before\n  gt::text_transform(\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\") %&gt;% \n  #Original changes as above.\n  cols_label(Entity = \"Country\",\n             Per.capita.domestic.aviation.CO2 = \"Per capita emissions (tonnes)\") %&gt;% \n  tab_header(title = md(\"Comparison of per capita CO&lt;sub&gt;2&lt;/sub&gt; emissions from domestic aviation (2018)\")) %&gt;% \n  tab_source_note(source_note = \"Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]\") %&gt;% \n  fmt_number(columns = c(Per.capita.domestic.aviation.CO2),\n             scale_by = 1000) %&gt;%\n  cols_label(Per.capita.domestic.aviation.CO2 = \"Per capita emissions (kg)\") %&gt;% \n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style     = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\")\n     )\n   ) %&gt;% \n   tab_style(\n     locations = cells_title(groups = \"title\"),\n     style     = list(\n       cell_text(weight = \"bold\", size = 24)\n     )\n   ) %&gt;% \n  data_color(columns = c(Per.capita.domestic.aviation.CO2),\n             colors = emissions_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;%\n  cols_width(c(Per.capita.domestic.aviation.CO2) ~ px(150),\n             c(Entity) ~ px(400)) %&gt;% \n  tab_options(\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    data_row.padding = px(3),\n    source_notes.font.size = 12,\n    heading.align = \"left\",\n    #Adjust grouped rows to make them stand out\n    row_group.background.color = \"grey\"))\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Comparison of per capita CO2 emissions from domestic aviation (2018)\n    \n    \n  \n  \n    \n      \n      Country\n      Per capita emissions (kg)\n    \n  \n  \n    \n      Africa\n    \n    \nSouth Africa\n25.09\n    \nNamibia\n11.49\n    \nMauritius\n8.75\n    \nKenya\n7.10\n    \nAlgeria\n4.43\n    \n      Americas\n    \n    \nUnited States\n385.52\n    \nCanada\n168.27\n    \nChile\n70.46\n    \nBrazil\n42.86\n    \nMexico\n39.93\n  \n  \n    \n      Data: Graver, Zhang, & Rutherford (2019) [via Our World in Data]"
  },
  {
    "objectID": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-conclusion-tables-can-be-nice",
    "href": "posts/2020-11-27-making-beautiful-tables-with-gt/index.html#the-conclusion-tables-can-be-nice",
    "title": "Making beautiful tables with the gt package",
    "section": "The Conclusion: Tables can be nice!",
    "text": "The Conclusion: Tables can be nice!\n\nA package like gt really unlocks the power of a table to display data. Using a combination of style and colour it’s easy to create a clear output that allows a reader to compare individual values. More over, we have the potential to easily include neat additions such as embedded images or graphs. The one draw back of gt is that it only generates static tables, to build interactive tables with filters or tabs we will need to move into other packages like reactable. Still, it’s a fun tool to add to the data visualisation toolbox of anybody working in R."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "",
    "text": "This will hopefully be the first in many TidyTuesday data visualization posts! This week I joined the TidyTuesday group at CorrelAid to brainstorm and troubleshoot ideas. You can see the plots from all the CorrelAid TidyTuesday meetups here."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#introduction-analysing-income-inequality",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#introduction-analysing-income-inequality",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Introduction: Analysing income inequality",
    "text": "Introduction: Analysing income inequality\n\nIncome and wealth inequality in the world’s major economies has become an increasing concern over the past decade [1, 2, 3]. We often see income inequality quantified using tools such as the Gini Coeffient or other more complex metrics, which compare the income of all individuals across society. Unfortunately, broad societal metrics often fail to highlight other inequalities between subgroups, such as gender and race. In this week’s TidyTuesday, I visualise a rough metric of racial inequality in the US since 2010, and demonstrate how things have become more unequal over time."
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#setup-data-and-packages",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#setup-data-and-packages",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Setup: Data and packages",
    "text": "Setup: Data and packages\n\nTidyTuesday Week 9 (2021) used employment and earning data from the US Bureau of Labor Statistics collected since 2010. There are a number of interesting ways we can wrangle and visualise this data, I’ll focus on racial inequality in income. We won’t need many advanced packages for this:\n\nlibrary(tidytuesdayR) #To download the data\nlibrary(dplyr) #For data wrangling\nlibrary(ggplot2) #For plotting\nlibrary(showtext) #Use fonts stored on the Google Fonts database\n\nTo start with we need to download the data and separate out employment and earnings data.\n\ntuesdata &lt;- tt_load(2021, week = 9)\n\n\n    Downloading file 1 of 2: `earn.csv`\n    Downloading file 2 of 2: `employed.csv`\n\nemployed &lt;- tuesdata$employed\nearn     &lt;- tuesdata$earn\n\nWe’ll create a custom ggplot theme so we don’t need to make these aesthetic changes in every plot.\n\nmy_theme &lt;- function(){\n  \n  theme_classic() %+replace% \n    #Remove legend by default\n    theme(legend.position = \"none\",\n          #Make axes black\n          axis.text = element_text(colour = \"black\"),\n          #Make clear titles and caption\n          plot.title = element_text(size = 18, face = \"bold\", hjust = 0),\n          plot.caption = element_text(size = 8, hjust = 1))\n  \n}"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#exploration-how-does-the-data-look",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#exploration-how-does-the-data-look",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Exploration: How does the data look?",
    "text": "Exploration: How does the data look?\n\nMy idea was to look at racial differences in income, so we’ll focus on the earn data frame. Before we go any further, we need to have a quick look at the data and see how it’s structured.\n\nhead(earn)\n\n# A tibble: 6 × 8\n  sex       race  ethnic_origin age    year quarter n_persons median_weekly_earn\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1 Both Sex… All … All Origins   16 y…  2010       1  96821000                754\n2 Both Sex… All … All Origins   16 y…  2010       2  99798000                740\n3 Both Sex… All … All Origins   16 y…  2010       3 101385000                740\n4 Both Sex… All … All Origins   16 y…  2010       4 100120000                752\n5 Both Sex… All … All Origins   16 y…  2011       1  98329000                755\n6 Both Sex… All … All Origins   16 y…  2011       2 100593000                753\n\n\nI can already see a few difficulties! Firstly, the columns sex, race, and age include aggregate categories like ‘Both Sexes’ and ‘All Races’. We need to be careful how we deal with these columns so we don’t double count individuals. To start with, we’ll group all ages and sexes and just look at racial differences. Secondly, out measure of ‘time’ is separated into two columns ‘year’ and ‘quarter’. If we want to analyse trends over time we’ll need to combine these two pieces of information. We can create an aggregate ‘time’ column where each financial quarter is added to year as a fraction (e.g. 2010 quarter 1 = 2010.0, 2010 quarter 2 = 2010.25).\nNote: ‘Hispanic’ does not fit neatly within the ‘race’ categories and is instead included under ‘ethnic_origin’. For simplicity we’ll be ignoring this column, but we are likely missing some nuance by doing so!\n\nplot_data &lt;- earn %&gt;%\n  #Consider aggregate groups for sex and age, but separate race.\n  filter(sex == \"Both Sexes\" & race != \"All Races\" & age == \"16 years and over\") %&gt;% \n  #Create new time column (combine year and quarter)\n  mutate(time = year + (quarter-1)/4)"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#initial-plots-what-can-we-see-when-we-start-plotting",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Initial plots: What can we see when we start plotting?",
    "text": "Initial plots: What can we see when we start plotting?\n\nNow we’ve got our data we can start plotting. I find it’s always best to visualise the data in as simple a format as possible before we try to achieve anything more complex and I always like the violin/boxplot combination as a first step.\n\nggplot(data = plot_data) +\n  geom_violin(aes(x = race, y = median_weekly_earn, fill = race),\n              colour = \"black\", width = 0.75) +\n  geom_boxplot(aes(x = race, y = median_weekly_earn),\n               colour = \"black\", width = 0.15, alpha = 0.25) +\n  scale_fill_manual(values = c(\"#DD5844\", \"#67B689\", \"#4383B4\")) +\n  labs(x = \"\", y = \"Median weekly income (USD$)\",\n       title = \"Comparison of earnings between racial groups\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme()\n\n\nWe can already clearly see a difference in income between racial groups at this very broad aggregation. Has this improved or worsened over the past decade? We can break our plot down into individual quarters to show how income in the three racial groups has changed.\n\nggplot(data = plot_data) +\n  geom_col(aes(x = time, y = median_weekly_earn, fill = race),\n           colour = \"black\") +\n  scale_fill_manual(values = c(\"#DD5844\", \"#67B689\", \"#4383B4\"), name = \"Race:\") +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  labs(x = \"\", y = \"Median weekly income (USD$)\",\n       title = \"Change in income over time\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme() +\n  #Legend would be useful here\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#digging-deeper-quantifying-inequality",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#digging-deeper-quantifying-inequality",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Digging deeper: Quantifying inequality",
    "text": "Digging deeper: Quantifying inequality\n\nIt looks like inequality has worsened over time, with earnings for those who identify as Asian appearing to increase more rapidly than Black or White. If we’re really interested in visualising this trend, we need some way to quantify the level of racial income inequality at any time point.\nA crude (but easy!) way we can do this is just to look at the proportional difference between the lowest and highest income group. If racial income inequality is small, we should get a value close to 1. When inequality is large, this number will be below 1 as the income from the lowest earning group will be only a fraction of the higher earning group.\nNote: Our measure seeks to identify racial income inequality regardless of who the lowest and highest earning racial group is in any quarter. Although it doesn’t occur in this data, it’s possible that the lowest and/or highest earning group could change over time!\n\nplot_data_inequality &lt;- plot_data %&gt;% \n  #For every quarter, determine the ratio betwen lowest and highest earner\n  group_by(time) %&gt;% \n  summarise(income_ratio = min(median_weekly_earn)/max(median_weekly_earn))\n\n\nggplot(data = plot_data_inequality) +\n  geom_line(aes(x = time, y = income_ratio), colour = \"#193D58\", size = 1) +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(plot_data_inequality$time), y = 0.99, label = \"Income parity\"), size = 5) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  labs(x = \"\", y = \"Racial income inequality\",\n       title = \"Change in racial inequality over time\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  my_theme()\n\n\nIn 2010, our measure of racial inequality gives a value around 0.75, meaning that the lowest earning group (Black) earned on average only 75% of the highest earning group (Asian). By 2020, this number was closer to 65%, a change of 10 percentage points! It seems that racial inequality is getting worse, but is this the same for both sexes? To answer this we’ll have to go back to the original data and include separate male and female data rather than ‘Both Sexes’.\n\ninequality_by_sex &lt;- earn %&gt;% \n  filter(sex != \"Both Sexes\" & race != \"All Races\" & age == \"16 years and over\") %&gt;% \n  #Create a time values (combine year and quarter)\n  mutate(time = year + quarter/4) %&gt;% \n  #Racial income inequality at each time point for each sex\n  group_by(sex, time) %&gt;% \n  summarise(income_ratio = min(median_weekly_earn)/max(median_weekly_earn), .groups = \"drop\")\n\n\nggplot() +\n  geom_line(data = inequality_by_sex, aes(x = time, y = income_ratio, colour = sex), size = 0.75) +\n  labs(x = \"\", y = \"Racial income inequality\",\n       title = \"Change in racial inequality for men and women\",\n       subtitle = \"\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(inequality_by_sex$time), y = 0.99, label = \"Income parity\"), size = 5) +\n  scale_colour_manual(name = \"Sex\", values = c(\"#266742\", \"#3255A2\")) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  scale_y_continuous(limits = c(NA, 1)) +\n  my_theme() +\n  theme(legend.position = \"right\")\n\n\nInterestingly, we see that racial income inequality is less severe in women (although it has also become worse over time). Note: This doesn’t show that women earn more, just that the difference in income due to race is less for women!"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#the-finishing-touches-making-things-look-pretty",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#the-finishing-touches-making-things-look-pretty",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "The finishing touches: Making things look pretty",
    "text": "The finishing touches: Making things look pretty\n\nThis a really interesting result, and a good foundation for a TidyTuesday submission. Now we just need to make it prettier. One issue for somebody viewing this data is the large quarter to quarter variation in our inequality measure. We can perhaps visualise the trends a little better by creating a rolling mean of inequality over 3 quarters. We can also add custom fonts and more informative titles/subtitles to make the plot easier to read.\n\n#Create rolling mean data\nrolling_mean &lt;- inequality_by_sex %&gt;% \n  group_by(sex) %&gt;% \n  mutate(lag1 = lag(income_ratio),\n         lead1 = lead(income_ratio)) %&gt;% \n  rowwise() %&gt;% \n  mutate(rollingmean = mean(c(income_ratio, lag1, lead1)))\n\n#Add sex symbols at last point to remove the legend\ninequality_by_sex %&gt;% \n  group_by(sex) %&gt;% \n  slice(n()) %&gt;% \n  ungroup() %&gt;% \n  mutate(time = time + 0.5,\n         sex_symbol = c(\"\\u2642\", \"\\u2640\")) -&gt; sex_symbol\n\n\n#Load a font from Google Fonts\nsysfonts::font_add_google(\"Oswald\", \"Oswald\", regular.wt = 300)\nsysfonts::font_add_google(\"Ubuntu Mono\", \"Ubuntu Mono\")\n\n#Specify that the showtext package should be used\n#for rendering text\nshowtext::showtext_auto()\n\nggplot() +\n  geom_line(data = inequality_by_sex, aes(x = time, y = income_ratio, group = sex),\n            colour = \"grey50\", size = 1, alpha = 0.5) +\n  geom_line(data = rolling_mean, aes(x = time, y = rollingmean, colour = sex), size = 2) +\n  labs(x = \"\", y = \"&lt;- Less equal                           More equal -&gt;\",\n       title = \"Racial income inequality in the USA\",\n       subtitle = \"Ratio between the income of the lowest and highest earning racial group\",\n       caption = \"Data: www.bls.gov | Plot: @ldbailey255\") +\n  geom_hline(aes(yintercept = 1), lty = 2) +\n  geom_text(aes(x = mean(inequality_by_sex$time), y = 0.98, label = \"Income parity\"),\n            size = 7, family = \"Oswald\") +\n  geom_text(data = sex_symbol, aes(x = time, y = income_ratio, label = sex_symbol), size = 17) +\n  scale_colour_manual(name = \"Sex\", values = c(\"#266742\", \"#3255A2\")) +\n  scale_x_continuous(breaks = seq(2010, 2021, 1)) +\n  scale_y_continuous(limits = c(NA, 1)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        axis.title.y = element_text(face = \"bold\", vjust = 0.9, size = 18, family = \"Oswald\"),\n        axis.text = element_text(colour = \"black\", size = 12, family = \"Oswald\"),\n        plot.title = element_text(face = \"bold\", size = 32, family = \"Oswald\"),\n        plot.subtitle = element_text(size = 20, family = \"Oswald\"),\n        plot.caption = element_text(size = 10, family = \"Ubuntu Mono\"))"
  },
  {
    "objectID": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#conclusion",
    "href": "posts/2021-02-28-tidytuesday-week-9-2021-tracking-racial-inequality-using-labour-statistics/index.html#conclusion",
    "title": "TidyTuesday - Week 9, 2021",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThis plot isn’t the most technically difficult, we’re just using basic geom_xxx() functions, but we used a bit of data wrangling to extract more information from some messy original data and present an interesting (and troubling) story."
  },
  {
    "objectID": "posts/2021-12-13-advent-of-code-day-8/index.html#the-data",
    "href": "posts/2021-12-13-advent-of-code-day-8/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\n\nDay 8 is really more a puzzle than a coding challenge. We’re given the inputs and outputs from a broken 7-segment display (like on a digital clock). We can imagine each segment of the clock as having a corresponding letter, from a (top) to g (bottom).\n\nWe can create a dictionary that will translate a string of letters into a corresponding number following the image above.\n\n#Actual letter number combos\ncorrect_codes &lt;- 0:9\nnames(correct_codes) &lt;- c(\"abcefg\", \"cf\", \"acdeg\", \"acdfg\", \"bcdf\", \"abdfg\", \"abdefg\", \"acf\", \"abcdefg\", \"abcdfg\")\n\nThe only problem is that the data we are given doesn’t have the correct letters! Each row of the input data we are given includes all 10 possible numbers (0-9) but the letters used are wrong. We have to work out a number of rules to decode this information! See the full explanation for today’s challenge here.\n\n#Load data\nday8_data &lt;- readr::read_delim(file = \"./data/Day8.txt\", delim = \" | \",\n                               col_names = c(\"inputs\", \"outputs\"), show_col_types = FALSE)\n\nhead(day8_data)\n\n# A tibble: 6 × 2\n  inputs                                                     outputs            \n  &lt;chr&gt;                                                      &lt;chr&gt;              \n1 fdceba bafdgc abeg afbdgec gbeacd abced bgc fcdge bg bedgc bafdec cgefd gcebd…\n2 gbfac fegbda fcedagb bea ea abcdef dgbfe gfabe dgea gbdfec gdea bgefdc bea ef…\n3 eg dagef gbcfeda ageb cegbfd gfe dbefa facdg abfged cedbaf befda daefb egf gc…\n4 edgacfb gcfd dgb degfab bcega bdagc cgafbd fbacd gd fceabd fbdac gd gdbcaf dgb\n5 eaf bedgaf dbafc bfceag fedcbg eafdcgb debfa ae adge gdebf abcdfeg febdg ae d…\n6 afbdc aefg ea edbacfg dbefg eab gcbfde abecgd bgefad bfdae gfea bfdea gbdef f…\n\n\nConvert our data into nested lists so that it’s easier to work with.\n\nday8_data &lt;- day8_data %&gt;% \n  mutate(inputs = stringr::str_split(inputs, pattern = \" \"),\n         outputs = stringr::str_split(outputs, pattern = \" \"))"
  },
  {
    "objectID": "posts/2021-12-13-advent-of-code-day-8/index.html#the-challenges",
    "href": "posts/2021-12-13-advent-of-code-day-8/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1 & 2\nIn combination, the two challenges from Day 8 aim to decode the outputs so we’ll combine them together. The first rules we can use are simple. There are 4 numbers that can be easily identified just by the number of characters in the input.\n\n2 characters = 1\n3 characters = 7\n4 characters = 4\n7 characters = 8\n\nSo if we see an input ‘ab’ this must correspond to the number 1 or ‘cf’ when correctly translated to the 7-segment display. But we don’t know if ‘a’ is ‘c’ or ‘f’. We need some more rules to work this out. We’ll apply all these rules below:\n\nnumbers &lt;- NULL\nfor (i in 1:nrow(day8_data)) {\n\n  #Split input into list with individual letters\n  split_input  &lt;- stringr::str_extract_all(day8_data$inputs[[i]], pattern = \"[a-z]{1}\")\n  split_output &lt;- stringr::str_extract_all(day8_data$outputs[[i]], pattern = \"[a-z]{1}\")\n\n  #If there are two values, it must correspond to c and f\n  #Remove from the list afterwards\n  number1 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 2))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 2))] &lt;- NULL\n\n  #If there are three values, it must correspond to a, c and f\n  #Remove from the list afterwards\n  number7 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 3))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 3))] &lt;- NULL\n\n  #If there are four values, it must correspond to b, c, d and f\n  #Remove from the list afterwards\n  number4 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 4))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 4))] &lt;- NULL\n\n  #If there are seven values, it must correspond to a-g\n  #Remove from the list afterwards\n  number8 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 7))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 7))] &lt;- NULL\n\n  #Letter that's not shared between 1 and 7 must be a\n  a &lt;- setdiff(number7, number1)\n\n  #Value of 6 characters that contains the letters in 4 must be 9.\n  #Remove from the list afterwards\n  number9 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number4 %in% letters)))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number4 %in% letters)))] &lt;- NULL\n  \n  #The missing letter in 9 is e\n  e &lt;- setdiff(number8, number9)\n\n  #Remaining 6 letter character that includes the same letters as number1 must be 0.\n  #Remove from the list afterwards\n  number0 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number1 %in% letters)))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6 & all(number1 %in% letters)))] &lt;- NULL\n  \n  #The missing value here is d\n  d &lt;- setdiff(number8, number0)\n\n  #Last remaining length 6 is 6\n  number6 &lt;- split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6))][[1]]\n  split_input[unlist(lapply(split_input, \\(letters) length(letters) == 6))] &lt;- NULL\n  \n  #Missing value is c\n  c &lt;- setdiff(number8, number6)\n\n  #The letter in 7 that wasn't just extracted is f\n  f &lt;- setdiff(number1, c)\n\n  #Letter in 4 that hasn't yet been deciphered in b\n  b &lt;- setdiff(number4, c(c, d, f))\n\n  #Remaining letter in 0 after removing all known ones is g\n  g &lt;- setdiff(number0, c(a, b, c, e, f))\n\n  #Dictionary\n  dict &lt;- letters[1:7]\n  names(dict) &lt;- c(a, b, c, d, e, f, g)\n\n  #Now we can decode our output\n  output &lt;- correct_codes[unlist(lapply(split_output, \\(letters) paste(dict[letters][order(dict[letters])], collapse = \"\")))]\n\n  #Paste all the decoded numbers together\n  numbers &lt;- append(numbers, as.integer(paste(output, collapse = \"\")))\n\n}\n\n\nnumbers\n\n  [1] 6233 4675 5572 5197 8513 4356  307 8507 4088 3756 9295 2569 5165 3061 2862\n [16] 1316 7879 2323 2593 5720 6695 7066 9013 9050 4501 2886 1326  734 9840 8003\n [31] 1752 4944 2038 9216 9250 4237 2145 7182 1101 5063 2136 8496 8523 3326 4598\n [46]  227 6537 3390 5065 9306 8460 6335 2664 6446 5550 4303 6955 3520 9376 2586\n [61] 6520 2121 6685 3109 2838 8257 3082 7739 7132 2770 5996  521 3470  940 2060\n [76] 6396  400 6050 2314 8464  262 9628 4719  706 8307 5355 4289 2422 2634  714\n [91] 2016 2778 6695  474 6970 3689 3305   12 5074 8060 5359 9331 8993 3307 9329\n[106]  458 9872 7089 6058 5030 3634 5702 3756 4506 2957 8995 5657 8375  415 2412\n[121] 3021 7241 8368 9298 6666 8567 2648 9621 3215 5533 4972 2236 8329 2582 2929\n[136] 2089 3436 3594 1383 5555 5461 2268 6377 1440 9494 8314 9483 3267 3037 5547\n[151] 9184 4900 6696 7304 1294 1349 8642 6126 7367 5525 3861 3393 2958 5312 2028\n[166] 5058 8513 4258 1780 3937 5568 6236 5159  518 8059 1014 1080 8538   40 6722\n[181] 3193 2519 5877 2460 3348 7329 8622 4380 3629  771 3734 6563 1900 4502 1472\n[196] 6293 7392 8285 9928 7535\n\n\nThe answer of our final challenge is the sum of all these values.\n\nsum(numbers)\n\n[1] 982158\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6\nDay 7"
  },
  {
    "objectID": "posts/2021-05-20-testing-out-r-4-1-0/index.html",
    "href": "posts/2021-05-20-testing-out-r-4-1-0/index.html",
    "title": "R 4.1.0",
    "section": "",
    "text": "R 4.1.0\nIt’s been talked about for awhile, but the next big changes to R, v.4.1.0, are here! For many R users, new versions don’t make a huge amount of difference to their day to day work. I often find people still happpily running R v3.x! This time around, however, v4.1.0 includes some major (and interesting) changes to the base R syntax, which might affect (improve?) people’s workflow. We’ll cover the most talked about changes below. Put on your party hat and lets get into it.\n\n\n\n\nThe native pipe\n\nEver since the rise of the tidyverse and magrittr I’ve noticed a worrying divergence in how people write R code. New users, or those that jumped on the tidyverse bandwagon, tended to write long piped code using dplyr functions. Older users, who learned R before the rise of the tidyverse, tended to use nested base functions to achieve the same result. Having a variety of tools to approach the same problem isn’t inherently bad, but it seemed like we were heading towards a world where the two groups were almost using different, incomprehensible, programming languages. Enter the native pipe: |&gt;. With v4.1.0 we now have an inbuilt pipe included in base R syntax. With |&gt; you can build piped code without ever needing to load a tidyverse library!\n\n#The tidyverse way\ntidy &lt;- iris %&gt;% \n  filter(Species == \"setosa\") %&gt;% \n  select(Species, Sepal.Length)\n\n#The base R way\nbase &lt;- iris[which(iris$Species == \"setosa\"), c(\"Species\", \"Sepal.Length\")]\n\n#The hybrid way\nhybrid &lt;- iris |&gt;\n  subset(Species == \"setosa\", select = c(\"Species\", \"Sepal.Length\"))\n\n#Same results...different inputs\nidentical(tidy, base)\n\n[1] TRUE\n\nidentical(base, hybrid)\n\n[1] TRUE\n\n\nThis is an awesome change for the base R syntax. I guess we can all forget about magrittr and move to using the native pipe, right? Well, not so fast. While the native pipe is a huge improvement, there are still a few features that might keep you using the more familiar %&gt;% (at least for now).\n\n\nFlexible placement\n\nmagrittr pipe allows the left-hand side to be used at any point in the following function with the placement of the .. In comparison, native pipe requires the left-hand side to always fill the first argument on the right.\n\nnames &lt;- c(\"John_Doe\", \"Jeff_Jones\", \"Rachel_Black\")\n\nnames %&gt;% \n  gsub(pattern = \"_\", x = ., replacement = \" \")\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\nWe can achieve the same thing in base R by writing a nested function, but it’s definitely not as effective and defeats some of the benefits of writing piped code (though see the changes to anonymous functions below.\n\nnames |&gt;\n  {function(x) gsub(pattern = \"_\", x = x, replacement = \" \")}()\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n\n\nShort-hand pipe functions\n\nA less well known, but sometimes useful, feature of the magrittr pipe is the ability to quickly turn a piece of piped code into a function. This functionality isn’t available with the native pipe…although that might not be a deal breaker for many people.\n\n#Create a function to run same pipe\nreplace_underscore &lt;- . %&gt;% \n  gsub(pattern = \"_\", x = ., replacement = \" \")\n\nreplace_underscore(names)\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n#The slightly messier base approach\nreplace_underscore &lt;- function(x){\n  \n  x |&gt;\n    {function(x) gsub(pattern = \"_\", x = x, replacement = \" \")}()\n  \n}\n\nreplace_underscore(names)\n\n[1] \"John Doe\"     \"Jeff Jones\"   \"Rachel Black\"\n\n\n\n\n\nThe rest of the magrittr package\n\nUntil now, we’ve comparing %&gt;% and |&gt;, but if you’re actually loading the magrittr package there are a few other special pipes that are also powerful. One particularly useful one is the tee-pipe, %T&gt;%, which allows you to create an output in the middle of your pipe (e.g. plot or view your data) without needing to stop the pipe.\n\n#Return the sum of two normally distributed variables\ncol_sums &lt;- data.frame(x = rnorm(10), y = rnorm(10)) %T&gt;%\n  #View the data to make sure it all looks fine\n  print() %&gt;%\n  #Return the column sums\n  colSums()\n\n            x          y\n1   0.8896869 -0.2138701\n2  -2.2067168  0.1609721\n3   1.7916267 -1.5109801\n4   0.5172365  0.2260602\n5  -0.6368221  0.3274581\n6   0.3188282 -0.3949109\n7  -0.3346745 -2.1968587\n8   1.1183246 -0.8555464\n9   0.4817347  2.1802146\n10  0.4370767  0.3922608\n\n\n\ncol_sums\n\n        x         y \n 2.376301 -1.885200 \n\n\n\n\n\nDependencies\n\nLet’s not pretend that %&gt;% has things all its own way. One major benefit of the native pipe is that it means you can use pipe with no dependencies. Working with the tidyverse can often mean working with quite a few dependencies, and there’s no guarantee that updates will be backward compatible. If you’re building an R package, even just for personal use, having the option of writing piped code without needing additional package dependencies might be exactly what you’re looking for.\n\n\n\n\n\n\nAnonymous functions\n\nAnother major syntax change that provides base R with capabilities previously available only in the tidyverse are changes to how anonymous functions are written. tidyverse gave the possibility to quickly write anonymous functions using ~. With v4.1.0, we can now easily create anonymous functions with the \\(x) syntax.\n\n#Tidy anon\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarise(across(.fns = ~mean(., na.rm = TRUE)))\n\n#New base anon\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(across(.fns = \\(x) mean(x, na.rm = TRUE)))\n\nUnlike our pipe comparison, the new base anonymous functions have a number of benefits over tidyverse equivalents.\n\n\nUse anywhere in R\n\nThe ~ syntax is useful within tidyverse but can’t easily be used outside of this context. \\(x), on the other hand, can be used to write functions anywhere you want.\n\n#This won't work!\n#tidy_func &lt;- {~mean(., na.rm = TRUE)}\n#tidy_func(c(1, 4, 7))\n\n#Works fine\nbase_func &lt;- \\(x)mean(x, na.rm = TRUE)\nbase_func(c(1, 4, 7))\n\n[1] 4\n\n\n\n\n\nMultiple arguments and argument names\n\nAnonymous functions in tidyverse allow for a single argument, but \\(x) allows for any number of arguments with any name you feel like!\n\nthree_arguments &lt;- \\(a, b, c)a*b/c\n\nthree_arguments(1.5, 4, 3)\n\n[1] 2\n\n\nSo, while I might be holding off on using native pipe, I think the new base R anonymous functions will quickly become part of my coding workflow!\n\n\n\n\nConcatenating factors\n\nOne smaller change that might be useful to regular R users is the new ability to concatenate factors together. Previously, trying to concatenate different factors would coerce all levels into their underlying integers (how the data are actually stored within R). With v4.1.0, factors can be concatenated and stay as factors, combining together all the levels from the two original factors.\n\nfactor1 &lt;- factor(c(\"Apple\", \"Orange\", \"Apple\"), levels = c(\"Apple\", \"Orange\"))\nfactor2 &lt;- factor(c(\"Banana\", \"Strawberry\"), levels = c(\"Banana\", \"Strawberry\"))\n\nc(factor1, factor2)\n\n[1] Apple      Orange     Apple      Banana     Strawberry\nLevels: Apple Orange Banana Strawberry\n\n\nNote that the order of the factor levels in the new factor are dependent on the order in which the two factors are concatenated together.\n\n#Levels start with Apple\nc(factor1, factor2)\n\n[1] Apple      Orange     Apple      Banana     Strawberry\nLevels: Apple Orange Banana Strawberry\n\n#Levels start with Banana\nc(factor2, factor1)\n\n[1] Banana     Strawberry Apple      Orange     Apple     \nLevels: Banana Strawberry Apple Orange\n\n\n\n\n\nWrap up\n\nR v4.1.0 marks an exciting step in R development. The introduction of native pipe and easy anonymous functions will allow users to take advantage of these useful tools even if they’re more comfortable in base R than the tidyverse. While I might not be switching over to using native pipe just yet, I’m really excited to see where this leads in future updates!"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html",
    "title": "Missing data",
    "section": "",
    "text": "A common problem we encounter when analysing data is the presence of missing data, NAs. How do we deal with NAs when we encounter them? In this blog post I’ll focus on different methods for dealing with NAs in R, with a particular focus on climatic time series."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#introduction",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#introduction",
    "title": "Missing data",
    "section": "",
    "text": "A common problem we encounter when analysing data is the presence of missing data, NAs. How do we deal with NAs when we encounter them? In this blog post I’ll focus on different methods for dealing with NAs in R, with a particular focus on climatic time series."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#preparing-our-workspace",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#preparing-our-workspace",
    "title": "Missing data",
    "section": "Preparing our workspace",
    "text": "Preparing our workspace\n\n\nPackages\n\nBelow are all the packages that we’ll use in this example. Many of these are standard data science packages in R, but notice the inclusion of {imputeTS}, which is a tool specifically designed to deal with NAs in a time series.\n\nlibrary(arrow) #To (quickly) read csv files\nlibrary(dplyr) #For data wrangling\nlibrary(ggplot2) #For plotting\nlibrary(imputeTS) #To impute data in data pipelines\n\n\n\nOur example data\n\nWe’ll focus on a time series of temperature data from Melbourne, Australia (Source: Australian Bureau of Meteorology). This is actually a complete time series with no missing data, but we’ll generate 400 NAs in the time series at random.\n\nfull_data &lt;- arrow::read_csv_arrow(file = \"data/melbourne_temp.csv\") %&gt;% \n  mutate(date = lubridate::ymd(paste(Year, Month, Day)),\n         maxT_missing = maxT)\n\n#Generate some NAs in the data\n#We ensure that the last row can't become NA so that linear interpolation is always possible\nset.seed(321)\nfull_data$maxT_missing[sample(1:(nrow(full_data) - 1), size = 400)] &lt;- NA"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#step-1-visualize-the-missing-data",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#step-1-visualize-the-missing-data",
    "title": "Missing data",
    "section": "Step 1: Visualize the missing data",
    "text": "Step 1: Visualize the missing data\n\nThe first step to any analysis should be to inspect and visualize your data. Dealing with NAs is no different. If we know there are NAs in our time series we want to see when they occur and how often. {imputeTS} includes a number of in-built plotting functions to visualize NAs (e.g. ?imputeTS::ggplot_na_distribution) but here I’ve used {ggplot2} to make a set of custom plots.\nFirst, we can look at which year the NAs occur in.\n\nplot_data &lt;- full_data %&gt;% \n  group_by(Year) %&gt;% \n  summarise(perc_NA = sum(is.na(maxT_missing))/n() * 100)\n\n#The limit of the y-axis will be the nearest declie above the data\nyaxis_lim &lt;- (max(plot_data$perc_NA) %/% 10)*10 + 10\n\nggplot(data = plot_data) +\n  geom_col(aes(x = Year, y = perc_NA),\n           fill = \"indianred\", colour = \"black\",\n           size = 0.25) +\n  # scale_fill_manual(values = c(\"steelblue\", \"indianred\"),\n                    # name = \"\", labels = c(\"Not NA\", \"Is NA\")) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(breaks = seq(0, 100, 5),\n                     labels = paste0(seq(0, 100, 5), \"%\")) +\n  coord_cartesian(ylim = c(0, yaxis_lim)) +\n  labs(title = \"Missing Values per Year\",\n       subtitle = \"Percentage of missing data in each year of the time-series\",\n       x = \"Year\", y = \"Percentage of records\") +\n  theme_classic() +\n  theme(panel.grid.major.y = element_line(colour = \"grey75\", size = 0.25))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nPercentage of missing data in each year of the time-series\n\n\n\n\n\nggplot() +\n  geom_line(data = full_data, aes(x = date, y = maxT_missing), colour = \"steelblue2\") +\n  geom_vline(data = filter(full_data, is.na(maxT_missing)), aes(xintercept = date),\n             colour = \"indianred\", alpha = 0.5) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Location of missing values\",\n       subtitle = \"Time series with highlighted missing regions\",\n       y = \"Temperature (C)\",\n       x = \"\") +\n  theme_classic()\n\n\n\n\n\nfull_data %&gt;% \n  filter(Year == 2022) %&gt;% \n  {ggplot() +\n      geom_line(data = ., aes(x = date, y = maxT_missing), colour = \"steelblue2\") +\n      geom_vline(data = filter(., is.na(maxT_missing)), aes(xintercept = date),\n                 colour = \"indianred\", alpha = 0.5) +\n      scale_x_date(expand = c(0, 0)) +\n      scale_y_continuous(expand = c(0, 0)) +\n      labs(title = \"Location of missing values\",\n           subtitle = \"Time series with highlighted missing regions\",\n           y = \"Temperature (C)\",\n           x = \"\") +\n      theme_classic()}\n\n\n\n\n\nggplot_na_gapsize(full_data$maxT_missing)"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#the-basic-method-deletion",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#the-basic-method-deletion",
    "title": "Missing data",
    "section": "The basic method: Deletion",
    "text": "The basic method: Deletion\n\nNAs include no information and cannot be used in statistical models. Therefore, the most common solution when encountering NAs is to simply delete them. This method is perfectly appropraite in many cases, but it can raise 2 key issues in any analysis:\n\nBias: If missing data are biased in some way, removing missing data will create bias in analysis. e.g. if temperature sensors tend to fail more at higher temperatures, removing NA values will bias our analysis towards lower temperatures.\nReduced statistical power: If we have many missing values, we will remove a lot of data and have lower statistical power to answer out questions.\n\nSo, what is the alternative? We can use imputation to fill in sampling periods where data are missing."
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-1-mean-substitution",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-1-mean-substitution",
    "title": "Missing data",
    "section": "Method 1: Mean substitution",
    "text": "Method 1: Mean substitution\n\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_meansub = case_when(is.na(maxT_missing) ~ mean(maxT_missing, na.rm = TRUE),\n                                  TRUE ~ maxT_missing))\n\nIssue here: reduces effect of multi-variate analysis because imputed values will necessarily not be unrelated to other variables used in analysis (because all imputations are the same despite other variables being different).\nAn extreme example:\n\neg_lmdata &lt;- tibble(x = 1:100) %&gt;% \n  mutate(y = x*0.5 + rnorm(n = 100))\n\neg_lmdata$y[sample(x = 1:nrow(eg_lmdata), size = 40)] &lt;- NA\n\neg_lmdata &lt;- eg_lmdata %&gt;% \n  mutate(y_fill = case_when(is.na(y) ~ mean(y, na.rm = TRUE),\n                            TRUE ~ y))\n\nlm(y ~ x, data = eg_lmdata) %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ x, data = eg_lmdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6184 -0.7792  0.2352  0.7237  2.3373 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.049978   0.259576  -0.193    0.848    \nx            0.504576   0.004651 108.486   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.07 on 58 degrees of freedom\n  (40 observations deleted due to missingness)\nMultiple R-squared:  0.9951,    Adjusted R-squared:  0.995 \nF-statistic: 1.177e+04 on 1 and 58 DF,  p-value: &lt; 2.2e-16\n\n\n\nlm(y_fill ~ x, data = eg_lmdata) %&gt;% summary()\n\n\nCall:\nlm(formula = y_fill ~ x, data = eg_lmdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2250  -5.9340   0.0968   5.9225  15.8661 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.6047     1.4368   5.293 7.36e-07 ***\nx             0.3205     0.0247  12.977  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.13 on 98 degrees of freedom\nMultiple R-squared:  0.6321,    Adjusted R-squared:  0.6284 \nF-statistic: 168.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWe will often want to do more complex multi-variate analysis, so this mean imputation is generally not useful!!"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-linear-imputation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-linear-imputation",
    "title": "Missing data",
    "section": "Method 2: Linear imputation",
    "text": "Method 2: Linear imputation\n\nIf we are focussing on multi-variate analysis we want a method that allows imputed values to vary. Linear imputation is one of the simplest methods to do this, especially when using time series data. We impute a missing value using a linear relationship between the points before and after the value itself.\nThis can be done using the ‘approx’ function in {stats}; however, this does not fit nicely within a {tidyverse} pipe coding. As an alternative, we can use the package {imputeTS}, which is a wrapper around basic {stats} functions (e.g. approx, spline) as well as other more complex imputation functions from other packages.\nHow does linear imputation work? [CREATE A GRAPH SHOWING THE IDEA!!]\n\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"linear\")\n\n[1] 1 2 3\n\n\n\n#Notice that it only uses information from the 2 closest values\nimputeTS::na_interpolation(x = c(4, 2, 1, NA, 3, 2, 7), option = \"linear\")\n\n[1] 4 2 1 2 3 2 7\n\n\nIt will use a single linear model to estimate blocks of NAs\n\nimputeTS::na_interpolation(x = c(1, NA, NA, 4), option = \"linear\")\n\n[1] 1 2 3 4\n\n\nInterprets blocks of NAs using the same linear model.\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_linearinterp = imputeTS::na_interpolation(x = maxT_missing, option = \"linear\"))\n\nCompare real and interpolated values\n\nfilter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(diff = maxT - maxT_linearinterp,\n         sign = diff &gt; 0) %&gt;%\n  {ggplot(.) +\n      geom_histogram(aes(x = diff), bins = 15, colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = median(.$diff), lty = 2) +\n      theme_classic() +\n      theme(legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank())}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-weighted-mean",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-2-weighted-mean",
    "title": "Missing data",
    "section": "Method 2: Weighted mean",
    "text": "Method 2: Weighted mean\n\nThe approx function has an alternative where it interpolates gaps using a weighted mean of the left and right values. We define method as ‘constant’ and then weighting using ‘f’; where a value of 1 is fully right weighted (i.e. value will = the value on the right) and value of 0 is fully left weighted. Therefore, f = 0.5 is the same as taking the mean of the two nearest points.\n\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"linear\", method = \"constant\", f = 0.5)\n\n[1] 1 2 3\n\n\nI guess this is a more sophisticated form of mean substitution, because all missing values in a block are given same.\n\nimputeTS::na_interpolation(x = c(1, NA, NA, 4), option = \"linear\", method = \"constant\", f = 0.5)\n\n[1] 1.0 2.5 2.5 4.0\n\n\nThere may be cases where we would want to weight more towards the left or right, but this seems very niche!\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_const = imputeTS::na_interpolation(x = maxT_missing, option = \"linear\", method = \"constant\", f = 0.5))\n\nCompare real and interpolated values\n\nfilter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(diff = maxT - maxT_const,\n         sign = diff &gt; 0) %&gt;%\n  {ggplot(.) +\n      geom_histogram(aes(x = diff), bins = 15, colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = median(.$diff), lty = 2) +\n      theme_classic() +\n      theme(legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank())}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-3-spline-interpolation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-3-spline-interpolation",
    "title": "Missing data",
    "section": "Method 3: Spline interpolation",
    "text": "Method 3: Spline interpolation\n\nAnother from {stats}. We can use different spline functions to fill in gaps. Unlike linear interpolation, spline interpolation uses more data either side of missing value. Makes sense! Would be impossible to fit spline to 2 data points that would be anything but linear! In fact, I guess we can recreate linear by using spline on data with only 1 value either side.\n\n#This is just the same as a linear model because the spline doesn't have any more detail\n#to form a more complex spline\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"spline\")\n\n[1] 1 2 3\n\n\n\n#Once we provide more data, imputation can fit a more complex spline\nimputeTS::na_interpolation(x = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1), option = \"spline\")\n\n[1]  3.0000000  2.5000000  2.0000000  1.0000000  0.8142007  0.7500000  0.0000000\n[8] -0.5000000 -1.0000000\n\n\nThere are 5 types of splines that we can use (need to study them more closely). However, for visual inspection we can also get the spline function back using splinefun() in {stats}.\n\n#Demonstrate how it works\ntest_df &lt;- tibble(x = 1:9,\n                  y = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1))\n\nfmm_func &lt;- splinefun(x = test_df$y, method = \"fmm\")\nperiodic_func &lt;- splinefun(x = test_df$y, method = \"periodic\")\n\nWarning in splinefun(x = test_df$y, method = \"periodic\"): spline: first and\nlast y values differ - using y[1L] for both\n\nnatural_func &lt;- splinefun(x = test_df$y, method = \"natural\")\nmonoH.FC_func &lt;- splinefun(x = test_df$y, method = \"monoH.FC\")\nhyman_func &lt;- splinefun(x = test_df$y, method = \"hyman\")\n\nspline_df &lt;- tibble(x = seq(1, 9, 0.1)) %&gt;% \n  mutate(fmm = fmm_func(x = x),\n         periodic = periodic_func(x = x),\n         natural = natural_func(x = x),\n         monoH.FC = monoH.FC_func(x = x),\n         hyman = hyman_func(x = x)) %&gt;% \n  tidyr::pivot_longer(cols = fmm:hyman, names_to = \"spline\", values_to = \"y\")\n\nggplot()+\n  geom_point(data = test_df, aes(x = x, y = y)) +\n  geom_line(data = spline_df, aes(x = x, y = y, lty = spline)) +\n  theme_classic()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nCompare the different splines\n\nFit all the different splines.\n\nspline_data &lt;- full_data %&gt;% \n  mutate(maxT_fmm = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"fmm\"),\n         maxT_periodic = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"periodic\"),\n         maxT_natural = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"natural\"),\n         #NOTE: monoH and hyman both assume monotonic data (i.e. data should either never increase or decrease)\n         #This is not expected for temp data and (unsurprisingly) these spline methods fail\n         # maxT_monoH = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"monoH.FC\"),\n         # maxT_hyman = imputeTS::na_interpolation(x = maxT_missing, option = \"spline\", method = \"hyman\")\n  ) %&gt;% \n  tidyr::pivot_longer(cols = maxT_fmm:maxT_natural, names_to = \"spline_method\", values_to = \"maxT_impute\")\n\nCompare real and interpolated values\n\nfilter(spline_data, is.na(maxT_missing)) %&gt;% \n  mutate(diff = maxT - maxT_impute,\n         sign = diff &gt; 0) %&gt;%\n  {ggplot(.) +\n      geom_histogram(aes(x = diff), bins = 15, colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = median(.$diff), lty = 2) +\n      facet_wrap(facets = ~spline_method) +\n      theme_classic() +\n      theme(legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank())}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-4-stineman-imputation",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-4-stineman-imputation",
    "title": "Missing data",
    "section": "Method 4: Stineman imputation",
    "text": "Method 4: Stineman imputation\n\nFinal option is na_interpolation. Uses algorithm in Stineman 1980.\n\n#Here essentially still linear\nimputeTS::na_interpolation(x = c(1, NA, 3), option = \"stine\")\n\n[1] 1 2 3\n\n\nI guess this is a more sophisticated form of mean substitution, because all missing values in a block are given same.\n\nimputeTS::na_interpolation(x = c(3, 2.5, 2, 1, NA, 0.75, 0, -0.5, -1), option = \"stine\")\n\n[1]  3.000  2.500  2.000  1.000  0.875  0.750  0.000 -0.500 -1.000\n\n\nThere may be cases where we would want to weight more towards the left or right, but this seems very niche!\n\nfull_data &lt;- full_data %&gt;% \n  mutate(maxT_stine = imputeTS::na_interpolation(x = maxT_missing, option = \"stine\"))\n\nCompare real and interpolated values\n\nfilter(full_data, is.na(maxT_missing)) %&gt;% \n  mutate(diff = maxT - maxT_stine,\n         sign = diff &gt; 0) %&gt;%\n  {ggplot(.) +\n      geom_histogram(aes(x = diff), bins = 15, colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = median(.$diff), lty = 2) +\n      theme_classic() +\n      theme(legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank())}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-weighted-moving-average",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#method-5-weighted-moving-average",
    "title": "Missing data",
    "section": "Method 5: Weighted moving average",
    "text": "Method 5: Weighted moving average\n\nLinear imputation or ‘constant’ option (from {stats} approx()) use just the adjacent values, but we can often benefit from using information from more nearby values. We can do this with a weighted moving average.\nKey arguments:\n\nk: Size of the window to calculate moving average. NOTE: This is size to either side of the NA that will be used, “this means for an NA value at position i of a time series, the observations i-1,i+1 and i+1, i+2 (assuming a window size of k=2) are used to calculate the mean.” (from help).\nweighting: Method to use for weighting values.\nsimple: All values are equally weighted\nlinear: Weight decreases linearly with distance from i. If x is the distance from i, then value has a weight 1/(1 + x), such that the nearest values have weight of 1/2, 1/3, 1/4 etc.\nexponential: Weights decrease exponentially. If x is the distance from i, then value has a weight 1/2^x.\n\n\n#This will be identical to using \"constant\" from `approx()` no matter what method we use!\n#We are only looking at the two nearest neighbours\nimputeTS::na_ma(x = c(3, 1, NA, 3, 10), k = 1)\n\n[1]  3  1  2  3 10\n\nimputeTS::na_interpolation(x = c(3, 1, NA, 3, 10), option = \"linear\", method = \"constant\", f = 0.5)\n\n[1]  3  1  2  3 10\n\n\nAs we expand the window, we can get a more nuanced value. No longer the same as using ‘constant’.\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"simple\")\n\n[1]  4.000000  3.000000  1.000000  4.166667  3.000000  4.000000 10.000000\n\n\nCan use different methods to adjust weighting. Note above that 10 is having a large effect on the returned value even though it is 3 time steps away. We could use linear or exponential weighting to minimize this effect.\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"linear\")\n\n[1]  4.000000  3.000000  1.000000  3.615385  3.000000  4.000000 10.000000\n\nimputeTS::na_ma(x = c(4, 3, 1, NA, 3, 4, 10), k = 3, weighting = \"exponential\")\n\n[1]  4.000000  3.000000  1.000000  3.142857  3.000000  4.000000 10.000000\n\n\nCompare real and interpolated values for each weighting method. For now, we just use default window size k = 4 (i.e. using 8 values in total towards mean)\n\nmovingavg_data &lt;- full_data %&gt;% \n  mutate(maxT_mas = imputeTS::na_ma(x = maxT_missing, weighting = \"simple\"),\n         maxT_mal = imputeTS::na_ma(x = maxT_missing, weighting = \"linear\"),\n         maxT_mae = imputeTS::na_ma(x = maxT_missing, weighting = \"exponential\")) %&gt;%\n  filter(is.na(maxT_missing)) %&gt;% \n  tibble::rowid_to_column() %&gt;% \n  tidyr::pivot_longer(cols = maxT_mas:maxT_mae, names_to = \"weight_method\", values_to = \"maxT_impute\")\n\nfilter(movingavg_data) %&gt;% \n  mutate(diff = maxT - maxT_impute,\n         sign = diff &gt; 0) %&gt;%\n  {ggplot(.) +\n      geom_histogram(aes(x = diff), bins = 15, colour = \"black\", fill = \"grey75\") +\n      geom_vline(xintercept = median(.$diff), lty = 2) +\n      facet_wrap(facets = ~weight_method) +\n      theme_classic() +\n      theme(legend.position = \"none\",\n            axis.text.y = element_blank(),\n            axis.title = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.line.y = element_blank())}"
  },
  {
    "objectID": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#overview-use-loo-cv-to-look-at-effectiveness-of-different-techniques",
    "href": "posts/2022-05-30-missing-data-different-interpolation-methods/index.html#overview-use-loo-cv-to-look-at-effectiveness-of-different-techniques",
    "title": "Missing data",
    "section": "Overview: Use LOO-CV to look at effectiveness of different techniques",
    "text": "Overview: Use LOO-CV to look at effectiveness of different techniques\n\n\nLOO_impute &lt;- function(fn, x, i, ...){\n  \n  if (i &gt; length(x)) {\n    \n    i &lt;- length(x)\n    \n    warning(\"i is larger than number of data points. i reduced to match length(x).\")\n    \n  }\n  \n  missing_index &lt;- sample(1:length(x), size = i, replace = FALSE)\n  \n  results &lt;- purrr::map_df(missing_index, .f = function(missing_index){\n    \n    #Assign to a new vector\n    x_missing &lt;- x\n    #Give an NA\n    x_missing[missing_index] &lt;- NA\n    \n    #Extract known value and imputed value\n    x_true &lt;- x[missing_index]\n    x_imp  &lt;- fn(x_missing, ...)[missing_index]\n    \n    tibble(true = x_true,\n           imputed = x_imp)\n    \n  })\n  \n  results %&gt;% \n    mutate(diff = (imputed - true)^2) %&gt;%\n    summarise(MSE = sum(diff)/n()) %&gt;% \n    mutate(RMSE = sqrt(MSE))\n}\n\nSo how do our different methods compare?\n\nresults &lt;- tibble(method = c(\"linear_interp\",\n                             \"spline_fmm\",\n                             \"stine\",\n                             \"ma_simple\",\n                             \"ma_linear\",\n                             \"ma_expo\")) %&gt;% \n  mutate(do.call(bind_rows,\n                 args = list(LOO_impute(fn = na_interpolation, i = 5000, x = full_data$maxT),\n                             LOO_impute(fn = na_interpolation, i = 5000, x = full_data$maxT, option = \"spline\", method = \"fmm\"),\n                             LOO_impute(fn = na_interpolation, i = 5000, x = full_data$maxT, option = \"stine\"),\n                             LOO_impute(fn = na_ma, i = 5000, x = full_data$maxT, weighting = \"simple\"),\n                             LOO_impute(fn = na_ma, i = 5000, x = full_data$maxT, weighting = \"linear\"),\n                             LOO_impute(fn = na_ma, i = 5000, x = full_data$maxT, weighting = \"exponential\"))))\n\nresults\n\n# A tibble: 6 × 3\n  method          MSE  RMSE\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 linear_interp  12.4  3.52\n2 spline_fmm     14.2  3.77\n3 stine          11.5  3.39\n4 ma_simple      17.2  4.15\n5 ma_linear      15.6  3.95\n6 ma_expo        14.2  3.77"
  },
  {
    "objectID": "posts/2021-12-08-advent-of-code-day-5/index.html#the-data",
    "href": "posts/2021-12-08-advent-of-code-day-5/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nIt’s Day 5 and we’re 20% of the way through the advent challenge! See the explanation for today’s challenge here. Today’s challenges will again involve working with indexing numbers in The Matrix a matrix.\n\nWe’re given two sets of 2-dimensional coordinates (X,Y), which represent the start and end of a line. We then need to count the number of points at which at least two lines overlap. The data includes an unusual separator (-&gt;), so we’ll use read_delim() from the readr package to read in the data. I find the readr functions more powerful than base functions for reading data because they allow for more complex separators. See the example below with base function read.delim that cannot use a separator larger than 1 byte.\n\nread.delim(\"./data/Day5.txt\", sep = \" -&gt; \")\n\nError in scan(file, what = \"\", sep = sep, quote = quote, nlines = 1, quiet = TRUE, : invalid 'sep' value: must be one byte\n\n\n\nlibrary(readr)\n\n#Read in data where each set of coordinates is a character string\nraw_data &lt;- readr::read_delim(\"./data/Day5.txt\",\n                              delim = \" -&gt; \", col_names = c(\"start\", \"end\"),\n                              show_col_types = FALSE,\n                              col_types = list(start = readr::col_character(),\n                                               end = readr::col_character()))\n\nhead(raw_data)\n\n# A tibble: 6 × 2\n  start   end    \n  &lt;chr&gt;   &lt;chr&gt;  \n1 503,977 843,637\n2 437,518 437,225\n3 269,250 625,250\n4 846,751 646,751\n5 18,731  402,731\n6 749,923 749,986\n\n\nWe need to be able to access each of the X and Y values separately, so we’ll separate this data out into 4 columns.\n\nlibrary(stringr)\n\n#Convert the characters into a 4col numeric matrix\nstart_point &lt;- str_split(raw_data$start, pattern = \",\", simplify = TRUE)\nend_point   &lt;- str_split(raw_data$end, pattern = \",\", simplify = TRUE)\nall_points  &lt;- cbind(start_point, end_point)\nall_points  &lt;- as.numeric(all_points)\ndim(all_points) &lt;- c(nrow(raw_data), 4)\n\nhead(all_points)\n\n     [,1] [,2] [,3] [,4]\n[1,]  503  977  843  637\n[2,]  437  518  437  225\n[3,]  269  250  625  250\n[4,]  846  751  646  751\n[5,]   18  731  402  731\n[6,]  749  923  749  986"
  },
  {
    "objectID": "posts/2021-12-08-advent-of-code-day-5/index.html#the-challenges",
    "href": "posts/2021-12-08-advent-of-code-day-5/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor challenge 1 we just focus on horizontal or vertical lines. Just like in our bingo challenge on day 4, we will create an empty matrix (all 0s) on which to map our results. Our matrix is 1000 x 1000 to accomodate all the possible coordinates in our data.\n\nzero_mat &lt;- matrix(0, nrow = 1000, ncol = 1000)\n\n#Look at a section of the matrix\nzero_mat[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    0    0    0    0    0    0    0     0\n [2,]    0    0    0    0    0    0    0    0    0     0\n [3,]    0    0    0    0    0    0    0    0    0     0\n [4,]    0    0    0    0    0    0    0    0    0     0\n [5,]    0    0    0    0    0    0    0    0    0     0\n [6,]    0    0    0    0    0    0    0    0    0     0\n [7,]    0    0    0    0    0    0    0    0    0     0\n [8,]    0    0    0    0    0    0    0    0    0     0\n [9,]    0    0    0    0    0    0    0    0    0     0\n[10,]    0    0    0    0    0    0    0    0    0     0\n\n\nAs we’re just focussing on the horizontal or diagonal lines, we filter only those cases where x or y are constant.\n\n#Identify only horizontal or vertical lines\nnondiag &lt;- all_points[apply(all_points, MARGIN = 1, FUN = function(x){\n\n  x[1] == x[3] | x[2] == x[4]\n\n}), ]\n\nNow, adding lines to our matrix is fairly straight forward but we need to deal with two small issues before we do. First, the coordinates we’re given start at 0,0; however, in R indexing begins at 1 (a shocking fact for people familiar with other programming languages!). This means we’ll have to add 1 to all our coordinates so they can be properly used for indexing.\nThe second hurdle is that our coordinates are provided as X,Y, but to index a matrix in R we need to provide coordinates as Y (i.e. rows), X (i.e. columns). In the for loop below I create a sequence of coordinate to map the horizontal and vertical lines and then reverse their order to be used as our index values.\n\nfor (i in 1:nrow(nondiag)) {\n\n  line &lt;- nondiag[i, ]\n  \n  #Make sequence of first coordinates (X)\n  #Add one to use for R indexing\n  xs   &lt;- (line[1]:line[3]) + 1\n  #Make sequence of second coordinates (Y)\n  ys   &lt;- (line[2]:line[4]) + 1\n\n  #Create a matrix of index values, BUT we need to write this as Y,X instead of X,Y\n  index_values &lt;- cbind(ys, xs)\n\n  zero_mat[index_values] &lt;- zero_mat[index_values] + 1\n\n}\n\nNow we can get our first answer, the number of points at which at least two lines overlap.\n\nsum(zero_mat &gt; 1)\n\n[1] 5124\n\n\n\n\nChallenge 2\n\nChallenge 2 requires us to include the diagonal lines too. This might seem more complex at first, but we are given the helpful caveat that all diagonal lines are 45 degrees, or, in other words, the slope of all lines will be 1. Because of this, our previous approach that creates a sequence of X and Y values increasing by 1 will still be appropriate. Only this time both the X and Y values will change.\n\n#Find all diagonal lines\ndiag &lt;- all_points[apply(all_points, MARGIN = 1, FUN = function(x){\n\n  x[1] != x[3] & x[2] != x[4]\n\n}), ]\n\n\n#Use the same approach to add all diagonal lines to our original matrix\nfor (i in 1:nrow(diag)) {\n\n  line &lt;- diag[i, ]\n\n  #Make sequence of first coordinates (X)\n  #Add one to use for R indexing\n  xs   &lt;- (line[1]:line[3]) + 1\n  #Make sequence of second coordinates (Y)\n  ys   &lt;- (line[2]:line[4]) + 1\n\n  #Create a matrix of index values, BUT we need to write this as Y,X instead of X,Y\n  index_values &lt;- cbind(ys, xs)\n\n  zero_mat[index_values] &lt;- zero_mat[index_values] + 1\n\n}\n\nOnce we have also added diagonal lines, we can get our second result.\n\nsum(zero_mat &gt; 1)\n\n[1] 19771\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-4/index.html#the-data",
    "href": "posts/2021-12-07-advent-of-code-day-4/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here. Most of the data we’ve dealt with so far has been 1 dimensional vectors, but for Day 4 we’re going to start working with 3-dimensional data. We’re playing BINGO!\n\nThe first line of data is a vector of numbers that are called out in the bingo game. We will extract these separately.\n\n#Read first line of data that includes the announced numbers\nnumber_calls_chr &lt;- readLines(\"./data/Day4.txt\", n = 1)\n\n#Covert this to a vector of integers\nnumber_calls_int &lt;- as.integer(stringr::str_split(number_calls_chr, pattern = \",\", simplify = TRUE))\n\nnumber_calls_int\n\n  [1] 13 47 64 52 60 69 80 85 57  1  2  6 30 81 86 40 27 26 97 77 70 92 43 94  8\n [26] 78  3 88 93 17 55 49 32 59 51 28 33 41 83 67 11 91 53 36 96  7 34 79 98 72\n [51] 39 56 31 75 82 62 99 66 29 58  9 50 54 12 45 68  4 46 38 21 24 18 44 48 16\n [76] 61 19  0 90 35 65 37 73 20 22 89 42 23 15 87 74 10 71 25 14 76 84  5 63 95\n\n\nWe then have a set of 100 bingo boards, each of which has 2 dimensions (5 rows and 5 columns). So how do we deal with this? One solution is to build a multi-dimensional array. A multi-dimensional array can be indexed just like a vector (1D) or matrix (2D), so we can easily work with and manipulate the data.\n\n#Read in the bingo boards as integers\nallboards &lt;- as.integer(scan(\"./data/Day4.txt\", what = \"list\", skip = 2))\n\n#Convert into 3D matrix\nboard_array  &lt;- array(allboards, dim = c(5, 5, length(allboards)/25))\n\n#Index the 1st and 2nd boards\nboard_array[,,1:2]\n\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   88   22    7   12   97\n[2,]   67   76   42   68   45\n[3,]   20   86    6   92   13\n[4,]   19   44   69   21   52\n[5,]   15   73   25   75   70\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   75   17   92   44   23\n[2,]   98   93   56    0    6\n[3,]   24   46   97   65   53\n[4,]   18   49   57   54   42\n[5,]   77   13   66   74   20\n\n\nWe can return a 3-dimensional position of a given number using which() and the argument arr.ind = TRUE. This will come in handy for our challenges!\n\n#Find the location of the number 0 on the first two boards\n#It is at position 2,4 on the 2nd board\nwhich(board_array[,,1:2] == 0, arr.ind = TRUE)\n\n     dim1 dim2 dim3\n[1,]    2    4    2"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-4/index.html#the-challenges",
    "href": "posts/2021-12-07-advent-of-code-day-4/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor the first challenge we need to determine which of our 100 boards will win first. To keep track of all the boards I’ll create a corresponding 3D array of logical information (TRUE/FALSE) that records whether a number on a board has been marked.\n\n#Array of logical data for all boards\nresult_array &lt;- array(rep(FALSE, n = length(allboards)), dim = c(5, 5, length(allboards)/25))\n\n#Show array for the same 2 boards\nresult_array[,,1:2]\n\n, , 1\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\n, , 2\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\n\nNow we need to work through each of the called numbers and work out which board gets a full row or column first (we’re ignoring diagonals here). We’ll just use a for loop to run through all these numbers.\n\n#Loop through all numbers called\nresult &lt;- NULL\nfor (number in number_calls_int){\n\n  #Use which to find all locations where this number occurs\n  #Update results on the corresponding array of logicals.\n  result_array[which(board_array == number, arr.ind = TRUE)] &lt;- TRUE\n\n  #Use apply to run through every board (the 3rd dimension, thus MARGIN = 3)\n  #Check if any boards have full rows or columns\n  board_status &lt;- apply(result_array, MARGIN = 3, FUN = function(x){\n\n    any(rowSums(x) == 5) | any(colSums(x) == 5)\n\n  })\n\n  has_winner &lt;- any(board_status)\n\n  #If there is a winner compute our answer\n  if (has_winner) {\n\n    winner &lt;- board_array[,,board_status]\n    unmarked &lt;- winner[!result_array[,,board_status]]\n\n    #Challenge asks for unmarked numbers * current number called\n    result &lt;- sum(unmarked) * number\n\n    break()\n\n  }\n\n}\n\nresult\n\n[1] 49686\n\n\n\n\nChallenge 2\n\nFor the second challenge, we need to find the board that will win last. This time around I’ll practice building a recursive function again.\n\n#Reset our results array\nresult_array &lt;- array(rep(FALSE, n = length(allboards)), dim = c(5, 5, length(allboards)/25))\n\n\n#Create recursive function.\nplay_bingo &lt;- function(bingo_boards, numbers, i, current_results){\n\n  #Find current number being called\n  number &lt;- numbers[i]\n\n  #Update results board with new number called.\n  current_results[which(bingo_boards == number, arr.ind = TRUE)] &lt;- TRUE\n\n  #Check status of all boards\n  board_status &lt;- apply(current_results, MARGIN = 3, FUN = function(x){\n\n    any(rowSums(x) == 5) | any(colSums(x) == 5)\n\n  })\n\n  #If there is only one board left and it is finished...\n  if (length(board_status) == 1 & sum(board_status) == 1) {\n\n    #Then return our answer\n    last_winner &lt;- bingo_boards[,,1]\n    unmarked &lt;- last_winner[!current_results[,,1]]\n\n    return(sum(unmarked) * number)\n\n  #Otherwise, remove all winning boards and call the function again...\n  } else {\n\n    #Filter only losing boards...\n    losing_boards  &lt;- bingo_boards[,,!board_status, drop = FALSE]\n    losing_results &lt;- current_results[,,!board_status, drop = FALSE]\n\n    #Recall function with next number\n    Recall(bingo_board = losing_boards, numbers = numbers, i = i + 1, current_results = losing_results)\n\n  }\n\n}\n\n\n#Let's get our result!\nplay_bingo(bingo_boards = board_array, numbers = number_calls_int, i = 1, current_results = result_array)\n\n[1] 26878\n\n\nBy using a 3D array we were able to easily work with this data. And there’s no reason to stop at 3 dimensions, you can build larger multi-dimensional arrays that can be indexed and searched through in just the same way.\n\n#Go crazy and build a 4D array with 2 sets of boards!\nfourD_array &lt;- array(c(allboards, allboards), dim = c(5, 5, length(allboards)/25, 2))\n\n#Return 1st board being played in the 2nd game\nfourD_array[,,1,2]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   88   22    7   12   97\n[2,]   67   76   42   68   45\n[3,]   20   86    6   92   13\n[4,]   19   44   69   21   52\n[5,]   15   73   25   75   70\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-3/index.html#the-data",
    "href": "posts/2021-12-07-advent-of-code-day-3/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nSee the explanation for today’s challenge here. This time we need to work with binary numbers.\n\nWe’re given a vector of numbers written in binary (e.g. 00100, 11110) and we need to use these to extract two values. It’s not as simple as just converting binary to decimal, we need to first generate new binary numbers that represent the most and least frequent value in each column. Let’s read the data in.\n\nlibrary(readr)\n\n#Read in data where each binary number is a character string\nday3_data &lt;- readr::read_delim(file = \"./data/Day3.txt\", delim = \"/t\",\n                               col_names = \"binary\", show_col_types = FALSE)\n\nhead(day3_data)\n\n# A tibble: 6 × 1\n  binary      \n  &lt;chr&gt;       \n1 000010000011\n2 001010000111\n3 011010000010\n4 000011111110\n5 101101000101\n6 000100010100"
  },
  {
    "objectID": "posts/2021-12-07-advent-of-code-day-3/index.html#the-challenges",
    "href": "posts/2021-12-07-advent-of-code-day-3/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nTo find the most and least common value in each column we first need to separate the character strings so that each binary bit is a separate column. We can do this using separate() in tidyr (and a bit of regex).\nMy first thought was just to use separate() where the separator is an empty string (““), but when we try this we end up with an erroneous empty column at the start.\n\nlibrary(tidyr)\n\nbinary_length &lt;- nchar(day3_data$binary[1])\n\nday3_data %&gt;%\n  separate(col = binary, into = as.character(1:binary_length), sep = \"\") %&gt;% \n  head()\n\n# A tibble: 6 × 12\n  `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`   `10`  `11`  `12` \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 \"\"    0     0     0     0     1     0     0     0     0     0     1    \n2 \"\"    0     0     1     0     1     0     0     0     0     1     1    \n3 \"\"    0     1     1     0     1     0     0     0     0     0     1    \n4 \"\"    0     0     0     0     1     1     1     1     1     1     1    \n5 \"\"    1     0     1     1     0     1     0     0     0     1     0    \n6 \"\"    0     0     0     1     0     0     0     1     0     1     0    \n\n\nInstead, we can use regex to specify that we only want to separate by blank spaces that were preceded by a number. We can do this using the regex lookbehind operation (?&lt;=). In our example, by adding (?&lt;=[0-1]) we are specifying that a separator must have a number 0 or 1 preceding it.\n\nseparated_binary &lt;- day3_data %&gt;%\n  #Use convert = TRUE to automatically coerce to numeric\n  tidyr::separate(col = binary, into = as.character(1:binary_length), sep = \"(?&lt;=[0-1])\", convert = TRUE)\n\nhead(separated_binary)\n\n# A tibble: 6 × 12\n    `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0     0     1     0     0     0     0     0     1     1\n2     0     0     1     0     1     0     0     0     0     1     1     1\n3     0     1     1     0     1     0     0     0     0     0     1     0\n4     0     0     0     0     1     1     1     1     1     1     1     0\n5     1     0     1     1     0     1     0     0     0     1     0     1\n6     0     0     0     1     0     0     0     1     0     1     0     0\n\n\nNow identifying the most or least common value is easy. If the sum of the column is greater than the number of rows, 1 is most common and visa-versa. Below we return TRUE if 1 is most common or FALSE when 0 is most common, which directly corresponds to 1 and 0 respectively when converted to an integer.\n\n#Is 1 most common?\nmost_common &lt;- separated_binary %&gt;%\n  #Return sum of each col\n  #If it's greater than half nrow() then 1 is most common (and the inverse is true)\n  summarise(across(.cols = everything(), .fns = ~sum(.) &gt; (n()/2)))\n\nas.integer(most_common)\n\n [1] 1 0 1 1 1 1 0 0 1 1 1 0\n\n\nAs a final step, we can use the strtoi() function to convert from binary to decimal. This function requires a single string input, so we need to convert our vector of most/least common numbers to a single character string.\n\n(most_common_binary &lt;- paste(as.integer(most_common), collapse = \"\"))\n\n[1] \"101111001110\"\n\n(least_common_binary &lt;- paste(as.integer(!most_common), collapse = \"\"))\n\n[1] \"010000110001\"\n\n\n\n#Convert each number to decimal\n(most_common_decimal    &lt;- strtoi(most_common_binary, base = 2))\n\n[1] 3022\n\n(least_common_decimal  &lt;- strtoi(least_common_binary, base = 2))\n\n[1] 1073\n\n\nOur answer is the product of these two numbers.\n\nmost_common_decimal * least_common_decimal\n\n[1] 3242606\n\n\n\n\nChallenge 2\n\nThe second challenge is just a slightly more complex version of challenge 1, so I’m going to skip the explanation for now. If you’re interested, you can see the code on GitHub.\n\n\nSee previous solutions here:\n\nDay 1\nDay 2"
  },
  {
    "objectID": "posts/2021-12-12-advent-of-code-day-7/index.html#the-data",
    "href": "posts/2021-12-12-advent-of-code-day-7/index.html#the-data",
    "title": "Advent of Code 2021",
    "section": "The Data",
    "text": "The Data\n\nFor Day 7 we move from fish to crabs, that are supposedly using submarines 🤷. I did not come up with the context for this! Either way, we are given a vector of (1-dimensional) positions for each crab and need to identify the position where all crabs can meet that involves the smallest amount of movement. Technically, this challenge isn’t too hard so we can spend a bit of time comparing two approaches.\n\n\n#Load data\nday7_data &lt;- scan(file = \"./data/Day7.txt\", sep = \",\")\n\nday7_data[1:100]\n\n  [1] 1101    1   29   67 1102    0    1   65 1008   65   35   66 1005   66   28\n [16]    1   67   65   20    4    0 1001   65    1   65 1106    0    8   99   35\n [31]   67  101   99  105   32  110   39  101  115  116   32  112   97  115   32\n [46]  117  110  101   32  105  110  116   99  111  100  101   32  112  114  111\n [61]  103  114   97  109   10   14   94  153  512 1778 1219  522  207  112  148\n [76] 1185  380  530  502  957  898   71   10  719   47   51  188  188 1277  446\n [91]  156  188  990  370  549 1086   49  150   42   50"
  },
  {
    "objectID": "posts/2021-12-12-advent-of-code-day-7/index.html#the-challenges",
    "href": "posts/2021-12-12-advent-of-code-day-7/index.html#the-challenges",
    "title": "Advent of Code 2021",
    "section": "The Challenges",
    "text": "The Challenges\n\n\nChallenge 1\n\nFor the first challenge we assume that movement costs increase linearly with distance (i.e. moving 1 positions costs 1 ‘energy’, moving 10 positions costs 10 ‘energy’). We’ll compare the amount of energy expended to reach locations ranging from position 0 until 1991 (the furthest position in our data):\n\n(possible_positions &lt;- range(day7_data))\n\n[1]    0 1991\n\n\nBecause costs of movement are linear, the amount of energy used to move to any position is just the distance covered.\n\nenergy_use &lt;- NULL\nfor (i in possible_positions[1]:possible_positions[2]) {\n  \n  energy_use &lt;- append(energy_use, sum(abs(day7_data - i)))\n  \n}\n\nWhat is the minimum amount of energy used?\n\nmin(energy_use)\n\n[1] 351901\n\n\n\n\nChallenge 2\n\nFor our second puzzle, we assume that energy use increases non-linearly with distance, such that the energy used is the sum of the sequence of distances traveled (i.e. moving 3 positions will use 1 + 2 + 3 = 6 energy). Luckily, there is an easy mathematical solve to determine the sum of any arithmetic sequence (i.e. a sequence that increases at a constant rate):\n\\[S_{n} = n(a_{1} + a_{n})/2\\]\nWhere \\(n\\) is the length of the sequence, \\(a_{1}\\) is the first value of the sequence, and \\(a_{n}\\) is the last term in the sequence. We can make this into a function for easy use.\n\n#Sum of values in a sequence\n#Source: https://www.varsitytutors.com/hotmath/hotmath_help/topics/sum-of-the-first-n-terms-of-an-arithmetic-sequence\nsum_seq &lt;- function(start, end, n){\n  \n  (n*(start + end))/2\n  \n}\n\nNow we can apply the for loop approach from before to find the minimum energy use under this non-linear energy use assumption.\n\nenergy_use2 &lt;- NULL\nfor (i in possible_positions[1]:possible_positions[2]) {\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  energy_use2 &lt;- append(energy_use2, sum(sum_seq(start = 1, end = diffs, n = diffs)))\n  \n}\n\nmin(energy_use2)\n\n[1] 101079875\n\n\n\n\nBONUS ROUND\n\nSo this was all fairly easy it seems, but was this the most efficient method we could use? If we look at the energy use values we’ve calculated we can see that there is a clear ‘trough’ where minimum energy use is found. This is exactly the type of problem that we could solve with an optimization algorithm!\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(aes(x = 0:1991, y = energy_use2)) +\n  #Need to subtract one to index of min energy use because R starts index at 1\n  geom_vline(xintercept = which(energy_use2 == min(energy_use2)) - 1, lty = 2) +\n  labs(x = \"Position\", y = \"Energy use\") +\n  theme_classic()\n\n\nBecause we are only optimizing a single variable, we only need to use a one-dimensional optimizer. To use an optimizer, we first need to create a function that will return the value we want to minimize. This is simply a re-work of the for loop we used above.\n\n#Create function to optimize\noptim_func &lt;- function(i){\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  sum(sum_seq(start = 1, end = diffs, n = diffs))\n  \n}\n\nNow we can feed this function into the optimizer using the optim() function in R. There are many possible optimization algorithms that exist, but in R the ‘Brent’ optimizer is the default algorithm designed for one-dimensional problems.\n\noptimal_energy &lt;- optim(par = 1000, #Specify the value where the optimizer will start searching\n                        fn = optim_func, #Specify the function that will be used\n                        lower = 0,\n                        upper = possible_positions[2], #Specify the upper and lower values of the search space\n                        method = \"Brent\"#Specify the algorithm used\n)\n\nmin(energy_use2) == optimal_energy$value\n\n[1] FALSE\n\noptimal_energy$value\n\n[1] 101079758\n\n\nOur optimizer returns…a smaller minimum energy use than with our previous method?! How is this possible? Well, while our challenge only considered integer distances the optimizer has no such constraint! We can see the difference between the best (continuous) position from the optimizer and the best (categorical) position from our previous method below.\n\noptimal_energy$par\n\n[1] 478.484\n\nwhich(energy_use2 == min(energy_use2)) - 1\n\n[1] 478\n\n\nTo demonstrate how we could get the exact same result we can constrain our distances to be integers using round().\n\n#Try doing it using an optimization algorithm\noptim_func_int &lt;- function(i){\n  \n  diffs &lt;- abs(day7_data - round(i))\n  \n  sum(sum_seq(start = 1, end = diffs, n = diffs))\n  \n}\n\noptimal_energy_int &lt;- optim(par = 1000, #Specify the value where the optimizer will start searching\n                            fn = optim_func_int, #Specify the function that will be used\n                            lower = 0,\n                            upper = possible_positions[2], #Specify the upper and lower values of the search space\n                            method = \"Brent\"#Specify the algorithm used\n)\n\noptimal_energy_int$value\n\n[1] 101079875\n\n\nSo we can now see how an optimizer could be used to solve this problem, but is it actually any better? We can do some bench marking on these methods the same way we did on Day 1. In our original for loop method we necessarily have to calculate the energy use for every possible (integer) distance. In our optimization method, we only need to calculate energy use until the algorithm determines that we have reached a minima (although we cannot be certain this will be the global minimum).\nAfter benchmarking we can see that the optimization function is substantially faster than our for loop method. Orders of magnitude faster even! In our rough bootstrap below we are able to run over 1,000 times more iterations of our optimizer per second than our for loop approach. Although for loops are often useful and powerful, in cases like this an optimizer provides a much more efficient alternative.\n\nlibrary(bench)\n\n#set seed so we achieve the same outcomes\nset.seed(123)\n\ntimes &lt;- mark(old = {for (i in possible_positions[1]:2000) {\n  \n  diffs &lt;- abs(day7_data - i)\n  \n  energy_use2 &lt;- append(energy_use2, sum(sum_seq(start = 1, end = diffs, n = diffs)))\n  \n}},\nnew = optim(par = 1000, #Specify the value where the optimizer will start searching\n            fn = optim_func_int, #Specify the function that will be used\n            lower = 0,\n            upper = 2000, #Specify the upper and lower values of the search space\n            method = \"Brent\"#Specify the algorithm used\n), check = FALSE, iterations = 50)\n\n#How many more iterations per second does our optimizer achieve?\ntimes$`itr/sec`[2]/times$`itr/sec`[1]\n\n[1] 1649.668\n\n\n\n\nSee previous solutions here:\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nBiography\n",
    "section": "",
    "text": "Dr. Liam D. Bailey\n\n\nFreelance Data Scientist\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n \n\n\n\n\nBiography\n\nI am an ecologist and data scientist with an interest in using data science techniques for conservation outcomes.\nMy scientific work has focussed on measuring and understanding the impacts of environmental change on natural systems, particularly the impacts of climate change. I completed my Ph.D. at the Australian National University with Dr. Martijn van de Pol and Dr. Naomi Langmore, focussing on the impacts of increased flooding on the Eurasian oystercatcher (Haematopus ostralegus). Since then my work has included analysis of extreme heat impacts in semi-arid environments and advancing reproductive timing in European passerines.\nMy scientific work has always involved data science, including programming in R, Python and SQL, statistical analyses, and GIS. In 2019 I officially branched out into the world of data science, first as lead developer for the SPI-Birds Network and Database and then in a corporate setting with the language learning app Babbel.\nI am now based in Berlin using my skills in data science and data visualization as a freelancer.\n\n\nInterests\n\n\nData for Good\n\n\nData Visualisation\n\n\nClimate change ecology\n\n\nPhotography\n\n\n\n\nEducation\n\n\n\n\nPhD in Ecology, 2017\n\n\nAustralian National Unveristy, Canberra\n\n\n\n\n\nBEnvSc in Biology, First Class Honours, 2011\n\n\nMonash University, Melbourne"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Viz",
    "section": "",
    "text": "Data Viz\n  Data visualizations portfolio, mostly in R.\n\n\n\n\n\n\nFerris Wheels\n\n\n\n\n\n\nBerlin\n\n\n\n\n\n\nTV Characters\n\n\n\n\n\n\nThe Portal Project\n\n\n\n\n\n\n2 colours\n\n\n\n\n\n\nGreat British Bake-Off\n\n\n\n\n\n\nNYC Pizza\n\n\n\n\n\n\nFood\n\n\n\n\n\n\nEU Energy Generation\n\n\n\n\n\n\nBlue\n\n\n\n\n\n\nHalloween Movies\n\n\n\n\n\n\nPopulation\n\n\n\n\n\n\nMovement\n\n\n\n\n\n\nIsland\n\n\n\n\n\n\nAmazon\n\n\n\n\n\n\nNZ Bird of the Year\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\nUS Drought\n\n\n\n\n\n\nNetwork\n\n\n\n\n\n\nSpace\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Thoughts and ideas...",
    "section": "",
    "text": "Thoughts and ideas...\n \n\n\n\n\n\n    \n        \n          R\n        \n        \n          19 min read\n        \n  \n  Missing data\n  Dealing with NAs in a time-series\n  May 30, 2022\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 8\n  Dec 13, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          7 min read\n        \n  \n  Advent of Code 2021\n  Day 7\n  Dec 12, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          6 min read\n        \n  \n  Advent of Code 2021\n  Day 6\n  Dec 9, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 5\n  Dec 8, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          4 min read\n        \n  \n  Advent of Code 2021\n  Day 4\n  Dec 6, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          3 min read\n        \n  \n  Advent of Code 2021\n  Day 3\n  Dec 5, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          5 min read\n        \n  \n  Advent of Code 2021\n  Day 2\n  Dec 4, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          11 min read\n        \n  \n  Advent of Code 2021\n  Day 1\n  Dec 2, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          6 min read\n        \n  \n  R 4.1.0\n  All the newest features\n  May 20, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          33 min read\n        \n  \n  TidyTuesday - Week 11 (2021)\n  James Bond, the Bechdel test, and adding images to ggplot\n  Mar 24, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          14 min read\n        \n  \n  TidyTuesday - Week 9 (2021)\n  Tracking racial inequality using labour statistics\n  Feb 28, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          9 min read\n        \n  \n  Building maps using OpenStreetMap\n  Up your map game\n  Jan 25, 2021\n\n\n\n\n\n\n\n\n\n    \n        \n          R\n        \n        \n          14 min read\n        \n  \n  Making beautiful tables with the `gt` package\n  OR: How I learnt to stop worrying and love the table\n  Nov 27, 2020\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]